{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Text2Photo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaraShamsher/Text-to-photo-image-synthesis-using-STACKGAN/blob/master/Text2Photo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKhbVHG5ozES",
        "outputId": "e5d5fef1-ade2-48e6-816c-09e803ed05b8"
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import time\n",
        "\n",
        "import PIL\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "from keras import Input, Model\n",
        "from keras import backend as K\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras.layers import Dense, LeakyReLU, BatchNormalization, ReLU, Reshape, UpSampling2D, Conv2D, Activation, \\\n",
        "    concatenate, Flatten, Lambda, Concatenate\n",
        "from keras.optimizers import Adam\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuGiDQmB8H_X",
        "outputId": "b047d2a4-fc78-400b-afe0-40ffd2f3f35f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00jC8mkVDLIO",
        "outputId": "5a32ca62-1566-4621-90c1-67d8d10f8580"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqLD_rToybX8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKwFqlFbFT0-",
        "outputId": "3f1731c6-9c1b-4353-ffdc-e31eae9aa4c5"
      },
      "source": [
        "cd drive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LlfI3NhFTmm",
        "outputId": "2a785673-c578-47d0-c047-969bf3be3f20"
      },
      "source": [
        "cd My Drive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzKIUOHNGEW0",
        "outputId": "52320b19-5ac3-4138-9165-2fdb0127c8dc"
      },
      "source": [
        "cd project"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LkArzG2PETt",
        "outputId": "f9f33289-5a14-4652-ab25-7c4d8a0aecc0"
      },
      "source": [
        "!ls\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "birds\t      logs     results2       stage1_gen.h5\n",
            "CUB_200_2011  results  stage1_dis.h5  Text2Photo.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "gantMOHHVORV",
        "outputId": "428507a5-06c2-442f-a4cb-e8b9b21ff10a"
      },
      "source": [
        "pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/project'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmCnNs7vp9cF"
      },
      "source": [
        "LOAD OUR **DATASET**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3D538Ruo_W8"
      },
      "source": [
        "def load_class_ids(class_info_file_path):\n",
        "    \"\"\"\n",
        "    Load class ids from class_info.pickle file\n",
        "    \"\"\"\n",
        "    with open(class_info_file_path, 'rb') as f:\n",
        "        class_ids = pickle.load(f, encoding='latin1')\n",
        "        return class_ids\n",
        "      \n",
        "def load_embeddings(embeddings_file_path):\n",
        "    \"\"\"\n",
        "    Load embeddings\n",
        "    \"\"\"\n",
        "    with open(embeddings_file_path, 'rb') as f:\n",
        "        embeddings = pickle.load(f, encoding='latin1')\n",
        "        embeddings = np.array(embeddings)\n",
        "        print('embeddings: ', embeddings.shape)\n",
        "    return embeddings\n",
        " \n",
        "def load_filenames(filenames_file_path):\n",
        "    \"\"\"\n",
        "    Load filenames.pickle file and return a list of all file names\n",
        "    \"\"\"\n",
        "    with open(filenames_file_path, 'rb') as f:\n",
        "        filenames = pickle.load(f, encoding='latin1')\n",
        "    return filenames\n",
        " \n",
        "def load_bounding_boxes(dataset_dir):\n",
        "    \"\"\"\n",
        "    Load bounding boxes and return a dictionary of file names and corresponding bounding boxes\n",
        "    \"\"\"\n",
        "    # Paths\n",
        "    bounding_boxes_path = os.path.join(dataset_dir, 'bounding_boxes.txt')\n",
        "    file_paths_path = os.path.join(dataset_dir, 'images.txt')\n",
        "\n",
        "    # Read bounding_boxes.txt and images.txt file\n",
        "    df_bounding_boxes = pd.read_csv(bounding_boxes_path,\n",
        "                                    delim_whitespace=True, header=None).astype(int)\n",
        "    df_file_names = pd.read_csv(file_paths_path, delim_whitespace=True, header=None)\n",
        "\n",
        "    # Create a list of file names\n",
        "    file_names = df_file_names[1].tolist()\n",
        "\n",
        "    # Create a dictionary of file_names and bounding boxes\n",
        "    filename_boundingbox_dict = {img_file[:-4]: [] for img_file in file_names[:2]}\n",
        "\n",
        "    # Assign a bounding box to the corresponding image\n",
        "    for i in range(0, len(file_names)):\n",
        "        # Get the bounding box\n",
        "        bounding_box = df_bounding_boxes.iloc[i][1:].tolist()\n",
        "        key = file_names[i][:-4]\n",
        "        filename_boundingbox_dict[key] = bounding_box\n",
        "\n",
        "    return filename_boundingbox_dict\n",
        "\n",
        "def get_img(img_path, bbox, image_size):\n",
        "    \"\"\"\n",
        "    Load and resize image\n",
        "    \"\"\"\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    width, height = img.size\n",
        "    if bbox is not None:\n",
        "        R = int(np.maximum(bbox[2], bbox[3]) * 0.75)\n",
        "        center_x = int((2 * bbox[0] + bbox[2]) / 2)\n",
        "        center_y = int((2 * bbox[1] + bbox[3]) / 2)\n",
        "        y1 = np.maximum(0, center_y - R)\n",
        "        y2 = np.minimum(height, center_y + R)\n",
        "        x1 = np.maximum(0, center_x - R)\n",
        "        x2 = np.minimum(width, center_x + R)\n",
        "        img = img.crop([x1, y1, x2, y2])\n",
        "    img = img.resize(image_size, PIL.Image.BILINEAR)\n",
        "    return img\n",
        " \n",
        "def load_dataset(filenames_file_path, class_info_file_path, cub_dataset_dir, embeddings_file_path, image_size):\n",
        "    \"\"\"\n",
        "    Load dataset\n",
        "    \"\"\"\n",
        "    filenames = load_filenames(filenames_file_path)\n",
        "    class_ids = load_class_ids(class_info_file_path)\n",
        "    bounding_boxes = load_bounding_boxes(cub_dataset_dir)\n",
        "    all_embeddings = load_embeddings(embeddings_file_path)\n",
        "\n",
        "    X, y, embeddings = [], [], []\n",
        "\n",
        "    print(\"Embeddings shape:\", all_embeddings.shape)\n",
        "\n",
        "    for index, filename in enumerate(filenames):\n",
        "        bounding_box = bounding_boxes[filename]\n",
        "\n",
        "        try:\n",
        "            # Load images\n",
        "            img_name = '{}/images/{}.jpg'.format(cub_dataset_dir, filename)\n",
        "            img = get_img(img_name, bounding_box, image_size)\n",
        "\n",
        "            all_embeddings1 = all_embeddings[index, :, :]\n",
        "\n",
        "            embedding_ix = random.randint(0, all_embeddings1.shape[0] - 1)\n",
        "            embedding = all_embeddings1[embedding_ix, :]\n",
        "\n",
        "            X.append(np.array(img))\n",
        "            y.append(class_ids[index])\n",
        "            embeddings.append(embedding)\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    embeddings = np.array(embeddings)\n",
        "    return X, y, embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CW7aE1BvqFD1"
      },
      "source": [
        "BUILDING STAGE 1 **ARCHITECTURE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygbpUSxvo_4-"
      },
      "source": [
        "def generate_c(x):\n",
        "    mean = x[:, :128]\n",
        "    log_sigma = x[:, 128:]\n",
        "    stddev = K.exp(log_sigma)\n",
        "    epsilon = K.random_normal(shape=K.constant((mean.shape[1],), dtype='int32'))\n",
        "    c = stddev * epsilon + mean\n",
        "    return c\n",
        "  \n",
        "def build_ca_model():\n",
        "    \"\"\"\n",
        "    Get conditioning augmentation model.\n",
        "    Takes an embedding of shape (1024,) and returns a tensor of shape (256,)\n",
        "    \"\"\"\n",
        "    input_layer = Input(shape=(1024,))\n",
        "    x = Dense(256)(input_layer)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    model = Model(inputs=[input_layer], outputs=[x])\n",
        "    return model\n",
        "  \n",
        "def build_embedding_compressor_model():\n",
        "    \"\"\"\n",
        "    Build embedding compressor model\n",
        "    \"\"\"\n",
        "    input_layer = Input(shape=(1024,))\n",
        "    x = Dense(128)(input_layer)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    model = Model(inputs=[input_layer], outputs=[x])\n",
        "    return model\n",
        "  \n",
        "def build_stage1_generator():\n",
        "    \"\"\"\n",
        "    Builds a generator model used in Stage-I\n",
        "    \"\"\"\n",
        "    input_layer = Input(shape=(1024,))\n",
        "    x = Dense(256)(input_layer)\n",
        "    mean_logsigma = LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    c = Lambda(generate_c)(mean_logsigma)\n",
        "\n",
        "    input_layer2 = Input(shape=(100,))\n",
        "\n",
        "    gen_input = Concatenate(axis=1)([c, input_layer2])\n",
        "\n",
        "    x = Dense(128 * 8 * 4 * 4, use_bias=False)(gen_input)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    x = Reshape((4, 4, 128 * 8), input_shape=(128 * 8 * 4 * 4,))(x)\n",
        "\n",
        "    x = UpSampling2D(size=(2, 2))(x)\n",
        "    x = Conv2D(512, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    x = UpSampling2D(size=(2, 2))(x)\n",
        "    x = Conv2D(256, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    x = UpSampling2D(size=(2, 2))(x)\n",
        "    x = Conv2D(128, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    x = UpSampling2D(size=(2, 2))(x)\n",
        "    x = Conv2D(64, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    x = Conv2D(3, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n",
        "    x = Activation(activation='tanh')(x)\n",
        "\n",
        "    stage1_gen = Model(inputs=[input_layer, input_layer2], outputs=[x, mean_logsigma])\n",
        "    return stage1_gen\n",
        "\n",
        "def build_stage1_discriminator():\n",
        "    \"\"\"\n",
        "    Create a model which takes two inputs\n",
        "    1. One from above network\n",
        "    2. One from the embedding layer\n",
        "    3. Concatenate along the axis dimension and feed it to the last module which produces final logits\n",
        "    \"\"\"\n",
        "    input_layer = Input(shape=(64, 64, 3))\n",
        "\n",
        "    x = Conv2D(64, (4, 4),\n",
        "               padding='same', strides=2,\n",
        "               input_shape=(64, 64, 3), use_bias=False)(input_layer)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    x = Conv2D(128, (4, 4), padding='same', strides=2, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    x = Conv2D(256, (4, 4), padding='same', strides=2, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    x = Conv2D(512, (4, 4), padding='same', strides=2, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    input_layer2 = Input(shape=(4, 4, 128))\n",
        "\n",
        "    merged_input = concatenate([x, input_layer2])\n",
        "\n",
        "    x2 = Conv2D(64 * 8, kernel_size=1,\n",
        "                padding=\"same\", strides=1)(merged_input)\n",
        "    x2 = BatchNormalization()(x2)\n",
        "    x2 = LeakyReLU(alpha=0.2)(x2)\n",
        "    x2 = Flatten()(x2)\n",
        "    x2 = Dense(1)(x2)\n",
        "    x2 = Activation('sigmoid')(x2)\n",
        "\n",
        "    stage1_dis = Model(inputs=[input_layer, input_layer2], outputs=[x2])\n",
        "    return stage1_dis\n",
        "  \n",
        "def build_adversarial_model(gen_model, dis_model):\n",
        "    input_layer = Input(shape=(1024,))\n",
        "    input_layer2 = Input(shape=(100,))\n",
        "    input_layer3 = Input(shape=(4, 4, 128))\n",
        "\n",
        "    x, mean_logsigma = gen_model([input_layer, input_layer2])\n",
        "\n",
        "    dis_model.trainable = False\n",
        "    valid = dis_model([x, input_layer3])\n",
        "\n",
        "    model = Model(inputs=[input_layer, input_layer2, input_layer3], outputs=[valid, mean_logsigma])\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tsTO2vZqPzK"
      },
      "source": [
        "CALCULATING **KL_LOSS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnDUfwPYp1iD"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICcMmALzptSq"
      },
      "source": [
        "def KL_loss(y_true, y_pred):\n",
        "    mean = y_pred[:, :128]\n",
        "    logsigma = y_pred[:, :128]\n",
        "    loss = -logsigma + .5 * (-1 + K.exp(2. * logsigma) + K.square(mean))\n",
        "    loss = K.mean(loss)\n",
        "    return loss\n",
        "    \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5zS1mFJp-2f"
      },
      "source": [
        "def custom_generator_loss(y_true, y_pred):\r\n",
        "    # Calculate binary cross entropy loss\r\n",
        "    return K.binary_crossentropy(y_true, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhazB8RFqYiC"
      },
      "source": [
        "saving the **generated image** after every 2 epochs and saving the **logs**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Opf-PokCptfn"
      },
      "source": [
        "def save_rgb_img(img, path):\n",
        "    \"\"\"\n",
        "    Save an rgb image\n",
        "    \"\"\"\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.imshow(img)\n",
        "    ax.axis(\"off\")\n",
        "    ax.set_title(\"Image\")\n",
        "\n",
        "    plt.savefig(path)\n",
        "    plt.close()\n",
        "    \n",
        "def write_log(callback, name, loss, batch_no):\n",
        "    \"\"\"\n",
        "    Write training summary to TensorBoard\n",
        "    \"\"\"\n",
        "    summary = tf.Summary()\n",
        "    summary_value = summary.value.add()\n",
        "    summary_value.simple_value = loss\n",
        "    summary_value.tag = name\n",
        "    callback.writer.add_summary(summary, batch_no)\n",
        "    callback.writer.flush()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtHPaHqvqo6j"
      },
      "source": [
        "initialize the **hyper-parameters** and train our stage I **StackGAN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JIeP6YcptsC",
        "outputId": "c04fe723-ccf1-4d03-dcf9-474c9deae5b0"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    data_dir = \"/content/drive/MyDrive/project/birds\"\n",
        "    train_dir = data_dir + \"/train\"\n",
        "    test_dir = data_dir + \"/test\"\n",
        "    image_size = 64\n",
        "    batch_size = 64\n",
        "    z_dim = 100\n",
        "    stage1_generator_lr = 0.0002\n",
        "    stage1_discriminator_lr = 0.0002\n",
        "    stage1_lr_decay_step = 600\n",
        "    epochs = 5\n",
        "    condition_dim = 128\n",
        "\n",
        "    embeddings_file_path_train = train_dir + \"/char-CNN-RNN-embeddings.pickle\"\n",
        "    embeddings_file_path_test = test_dir + \"/char-CNN-RNN-embeddings.pickle\"\n",
        "\n",
        "    filenames_file_path_train = train_dir + \"/filenames.pickle\"\n",
        "    filenames_file_path_test = test_dir + \"/filenames.pickle\"\n",
        "\n",
        "    class_info_file_path_train = train_dir + \"/class_info.pickle\"\n",
        "    class_info_file_path_test = test_dir + \"/class_info.pickle\"\n",
        "\n",
        "    cub_dataset_dir = \"/content/drive/MyDrive/project/CUB_200_2011\"\n",
        "    \n",
        "    # Define optimizers\n",
        "    dis_optimizer = Adam(lr=stage1_discriminator_lr, beta_1=0.5, beta_2=0.999)\n",
        "    gen_optimizer = Adam(lr=stage1_generator_lr, beta_1=0.5, beta_2=0.999)\n",
        "\n",
        "    \"\"\"\"\n",
        "    Load datasets\n",
        "    \"\"\"\n",
        "    X_train, y_train, embeddings_train = load_dataset(filenames_file_path=filenames_file_path_train,\n",
        "                                                      class_info_file_path=class_info_file_path_train,\n",
        "                                                      cub_dataset_dir=cub_dataset_dir,\n",
        "                                                      embeddings_file_path=embeddings_file_path_train,\n",
        "                                                      image_size=(64, 64))\n",
        "\n",
        "    X_test, y_test, embeddings_test = load_dataset(filenames_file_path=filenames_file_path_test,\n",
        "                                                   class_info_file_path=class_info_file_path_test,\n",
        "                                                   cub_dataset_dir=cub_dataset_dir,\n",
        "                                                   embeddings_file_path=embeddings_file_path_test,\n",
        "                                                   image_size=(64, 64))\n",
        "\n",
        "    \"\"\"\n",
        "    Build and compile networks\n",
        "    \"\"\"\n",
        "    ca_model = build_ca_model()\n",
        "    ca_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
        "\n",
        "    stage1_dis = build_stage1_discriminator()\n",
        "    stage1_dis.compile(loss='binary_crossentropy', optimizer=dis_optimizer)\n",
        "\n",
        "    stage1_gen = build_stage1_generator()\n",
        "    stage1_gen.compile(loss=\"mse\", optimizer=gen_optimizer)\n",
        "\n",
        "    embedding_compressor_model = build_embedding_compressor_model()\n",
        "    embedding_compressor_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
        "\n",
        "    adversarial_model = build_adversarial_model(gen_model=stage1_gen, dis_model=stage1_dis)\n",
        "    adversarial_model.compile(loss=['binary_crossentropy', KL_loss], loss_weights=[1, 2.0],\n",
        "                              optimizer=gen_optimizer, metrics=None)\n",
        "\n",
        "    tensorboard = TensorBoard(log_dir=\"logs/\".format(time.time()))\n",
        "    tensorboard.set_model(stage1_gen)\n",
        "    tensorboard.set_model(stage1_dis)\n",
        "    tensorboard.set_model(ca_model)\n",
        "    tensorboard.set_model(embedding_compressor_model)\n",
        "\n",
        "    # Generate an array containing real and fake values\n",
        "    # Apply label smoothing as well\n",
        "    real_labels = np.ones((batch_size, 1), dtype=float) * 0.9\n",
        "    fake_labels = np.zeros((batch_size, 1), dtype=float) * 0.1\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(\"========================================\")\n",
        "        print(\"Epoch is:\", epoch)\n",
        "        print(\"Number of batches\", int(X_train.shape[0] / batch_size))\n",
        "\n",
        "        gen_losses = []\n",
        "        dis_losses = []\n",
        "\n",
        "        # Load data and train model\n",
        "        number_of_batches = int(X_train.shape[0] / batch_size)\n",
        "        for index in range(number_of_batches):\n",
        "            print(\"Batch:{}\".format(index+1))\n",
        "            \n",
        "            \"\"\"\n",
        "            Train the discriminator network\n",
        "            \"\"\"\n",
        "            # Sample a batch of data\n",
        "            z_noise = np.random.normal(0, 1, size=(batch_size, z_dim))\n",
        "            image_batch = X_train[index * batch_size:(index + 1) * batch_size]\n",
        "            embedding_batch = embeddings_train[index * batch_size:(index + 1) * batch_size]\n",
        "            image_batch = (image_batch - 127.5) / 127.5\n",
        "\n",
        "            # Generate fake images\n",
        "            fake_images, _ = stage1_gen.predict([embedding_batch, z_noise], verbose=3)\n",
        "\n",
        "            # Generate compressed embeddings\n",
        "            compressed_embedding = embedding_compressor_model.predict_on_batch(embedding_batch)\n",
        "            compressed_embedding = np.reshape(compressed_embedding, (-1, 1, 1, condition_dim))\n",
        "            compressed_embedding = np.tile(compressed_embedding, (1, 4, 4, 1))\n",
        "\n",
        "            dis_loss_real = stage1_dis.train_on_batch([image_batch, compressed_embedding],\n",
        "                                                      np.reshape(real_labels, (batch_size, 1)))\n",
        "            dis_loss_fake = stage1_dis.train_on_batch([fake_images, compressed_embedding],\n",
        "                                                      np.reshape(fake_labels, (batch_size, 1)))\n",
        "            dis_loss_wrong = stage1_dis.train_on_batch([image_batch[:(batch_size - 1)], compressed_embedding[1:]],\n",
        "                                                       np.reshape(fake_labels[1:], (batch_size-1, 1)))\n",
        "\n",
        "            d_loss = 0.5 * np.add(dis_loss_real, 0.5 * np.add(dis_loss_wrong, dis_loss_fake))\n",
        "\n",
        "            print(\"d_loss_real:{}\".format(dis_loss_real))\n",
        "            print(\"d_loss_fake:{}\".format(dis_loss_fake))\n",
        "            print(\"d_loss_wrong:{}\".format(dis_loss_wrong))\n",
        "            print(\"d_loss:{}\".format(d_loss))\n",
        "\n",
        "            \"\"\"\n",
        "            Train the generator network \n",
        "            \"\"\"\n",
        "            g_loss = adversarial_model.train_on_batch([embedding_batch, z_noise, compressed_embedding],[K.ones((batch_size, 1)) * 0.9, K.ones((batch_size, 256)) * 0.9])\n",
        "            print(\"g_loss:{}\".format(g_loss))\n",
        "\n",
        "            dis_losses.append(d_loss)\n",
        "            gen_losses.append(g_loss)\n",
        "\n",
        "        \"\"\"\n",
        "        Save losses to Tensorboard after each epoch\n",
        "        \"\"\"\n",
        "        \n",
        "        write_log(tensorboard, 'discriminator_loss', np.mean(dis_losses), epoch)\n",
        "        write_log(tensorboard, 'generator_loss', np.mean(gen_losses[0]), epoch)\n",
        "        \n",
        "        # Generate and save images after every 2nd epoch\n",
        "        if epoch % 2 == 0:\n",
        "            # z_noise2 = np.random.uniform(-1, 1, size=(batch_size, z_dim))\n",
        "            z_noise2 = np.random.normal(0, 1, size=(batch_size, z_dim))\n",
        "            embedding_batch = embeddings_test[0:batch_size]\n",
        "            fake_images, _ = stage1_gen.predict_on_batch([embedding_batch, z_noise2])\n",
        "\n",
        "            # Save images\n",
        "            for i, img in enumerate(fake_images[:10]):\n",
        "                save_rgb_img(img, \"results/gen_{}_{}.png\".format(epoch, i))\n",
        "\n",
        "    # Save models\n",
        "    stage1_gen.save_weights(\"stage1_gen.h5\")\n",
        "    stage1_dis.save_weights(\"stage1_dis.h5\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "embeddings:  (8855, 10, 1024)\n",
            "Embeddings shape: (8855, 10, 1024)\n",
            "embeddings:  (2933, 10, 1024)\n",
            "Embeddings shape: (2933, 10, 1024)\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/keras/callbacks/tensorboard_v1.py:200: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/keras/callbacks/tensorboard_v1.py:203: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "========================================\n",
            "Epoch is: 0\n",
            "Number of batches 138\n",
            "Batch:1\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/tensorflow-1.15.2/python3.6/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n",
            "/tensorflow-1.15.2/python3.6/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "d_loss_real:0.7348410487174988\n",
            "d_loss_fake:4.154080390930176\n",
            "d_loss_wrong:5.4249491691589355\n",
            "d_loss:2.7621777951717377\n",
            "g_loss:[1.431669, 1.3952235, 0.018222772]\n",
            "Batch:2\n",
            "d_loss_real:1.2885637283325195\n",
            "d_loss_fake:0.10898943245410919\n",
            "d_loss_wrong:1.3756340742111206\n",
            "d_loss:1.015437752008438\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/tensorflow-1.15.2/python3.6/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "g_loss:[0.6204621, 0.57125413, 0.024603987]\n",
            "Batch:3\n",
            "d_loss_real:2.6511306762695312\n",
            "d_loss_fake:1.8132270574569702\n",
            "d_loss_wrong:0.6635470986366272\n",
            "d_loss:1.9447588920593262\n",
            "g_loss:[3.8609712, 3.8219776, 0.019496772]\n",
            "Batch:4\n",
            "d_loss_real:3.724064350128174\n",
            "d_loss_fake:0.014394648373126984\n",
            "d_loss_wrong:1.4819421768188477\n",
            "d_loss:2.2361163794994354\n",
            "g_loss:[1.612422, 1.5673375, 0.022542223]\n",
            "Batch:5\n",
            "d_loss_real:2.0510880947113037\n",
            "d_loss_fake:0.6484317779541016\n",
            "d_loss_wrong:1.8449029922485352\n",
            "d_loss:1.648877739906311\n",
            "g_loss:[4.644964, 4.604575, 0.020194441]\n",
            "Batch:6\n",
            "d_loss_real:2.9627232551574707\n",
            "d_loss_fake:0.0017926197033375502\n",
            "d_loss_wrong:1.5466877222061157\n",
            "d_loss:1.8684817254543304\n",
            "g_loss:[3.1898487, 3.149372, 0.02023825]\n",
            "Batch:7\n",
            "d_loss_real:2.3993258476257324\n",
            "d_loss_fake:0.3519850969314575\n",
            "d_loss_wrong:0.9122635722160339\n",
            "d_loss:1.5157250761985779\n",
            "g_loss:[2.8165712, 2.7765307, 0.0200202]\n",
            "Batch:8\n",
            "d_loss_real:2.670504093170166\n",
            "d_loss_fake:0.03076435998082161\n",
            "d_loss_wrong:0.5742144584655762\n",
            "d_loss:1.4864967465400696\n",
            "g_loss:[2.3728712, 2.307155, 0.03285818]\n",
            "Batch:9\n",
            "d_loss_real:1.8242278099060059\n",
            "d_loss_fake:0.033720534294843674\n",
            "d_loss_wrong:1.4688193798065186\n",
            "d_loss:1.2877488732337952\n",
            "g_loss:[1.4738549, 1.3996367, 0.037109092]\n",
            "Batch:10\n",
            "d_loss_real:1.1799163818359375\n",
            "d_loss_fake:0.003649438265711069\n",
            "d_loss_wrong:1.1270209550857544\n",
            "d_loss:0.8726257979869843\n",
            "g_loss:[1.8199946, 1.7597623, 0.03011616]\n",
            "Batch:11\n",
            "d_loss_real:1.5915334224700928\n",
            "d_loss_fake:0.011012720875442028\n",
            "d_loss_wrong:1.00356924533844\n",
            "d_loss:1.049412190914154\n",
            "g_loss:[1.3712229, 1.3326097, 0.019306578]\n",
            "Batch:12\n",
            "d_loss_real:1.6196024417877197\n",
            "d_loss_fake:0.04613571614027023\n",
            "d_loss_wrong:0.9037654995918274\n",
            "d_loss:1.0472765266895294\n",
            "g_loss:[2.622585, 2.5813446, 0.02062017]\n",
            "Batch:13\n",
            "d_loss_real:1.4979723691940308\n",
            "d_loss_fake:0.028767630457878113\n",
            "d_loss_wrong:0.9950281977653503\n",
            "d_loss:1.0049351453781128\n",
            "g_loss:[1.4927276, 1.4490306, 0.021848518]\n",
            "Batch:14\n",
            "d_loss_real:1.4269520044326782\n",
            "d_loss_fake:0.011841494590044022\n",
            "d_loss_wrong:1.0735960006713867\n",
            "d_loss:0.9848353862762451\n",
            "g_loss:[1.3191586, 1.2849972, 0.017080683]\n",
            "Batch:15\n",
            "d_loss_real:1.0961741209030151\n",
            "d_loss_fake:0.08353278040885925\n",
            "d_loss_wrong:1.2977280616760254\n",
            "d_loss:0.8934022784233093\n",
            "g_loss:[1.9470816, 1.8876681, 0.02970669]\n",
            "Batch:16\n",
            "d_loss_real:1.2931426763534546\n",
            "d_loss_fake:0.018678709864616394\n",
            "d_loss_wrong:1.0282155275344849\n",
            "d_loss:0.9082948863506317\n",
            "g_loss:[1.7544854, 1.6885071, 0.032989126]\n",
            "Batch:17\n",
            "d_loss_real:1.4706575870513916\n",
            "d_loss_fake:0.1186988353729248\n",
            "d_loss_wrong:1.4010154008865356\n",
            "d_loss:1.115257352590561\n",
            "g_loss:[3.2164803, 3.1692138, 0.023633217]\n",
            "Batch:18\n",
            "d_loss_real:1.5166555643081665\n",
            "d_loss_fake:0.04029533267021179\n",
            "d_loss_wrong:0.6901307106018066\n",
            "d_loss:0.9409343004226685\n",
            "g_loss:[2.5540683, 2.514597, 0.019735673]\n",
            "Batch:19\n",
            "d_loss_real:1.4590044021606445\n",
            "d_loss_fake:0.045238740742206573\n",
            "d_loss_wrong:0.9545135498046875\n",
            "d_loss:0.9794402718544006\n",
            "g_loss:[2.0210629, 1.9796236, 0.020719634]\n",
            "Batch:20\n",
            "d_loss_real:1.381835699081421\n",
            "d_loss_fake:0.1442812979221344\n",
            "d_loss_wrong:0.8431448340415955\n",
            "d_loss:0.9377743899822235\n",
            "g_loss:[2.2459385, 2.199597, 0.02317087]\n",
            "Batch:21\n",
            "d_loss_real:1.4741809368133545\n",
            "d_loss_fake:0.0790979191660881\n",
            "d_loss_wrong:0.6826006174087524\n",
            "d_loss:0.9275151044130325\n",
            "g_loss:[1.8844119, 1.8473151, 0.018548435]\n",
            "Batch:22\n",
            "d_loss_real:1.2182152271270752\n",
            "d_loss_fake:0.06901325285434723\n",
            "d_loss_wrong:1.0028139352798462\n",
            "d_loss:0.8770644068717957\n",
            "g_loss:[1.3449953, 1.3108352, 0.017080002]\n",
            "Batch:23\n",
            "d_loss_real:1.207322120666504\n",
            "d_loss_fake:0.10116691887378693\n",
            "d_loss_wrong:0.7916746735572815\n",
            "d_loss:0.8268714547157288\n",
            "g_loss:[1.6670961, 1.631304, 0.017896054]\n",
            "Batch:24\n",
            "d_loss_real:1.2683398723602295\n",
            "d_loss_fake:0.08842147141695023\n",
            "d_loss_wrong:0.8184729218482971\n",
            "d_loss:0.8608935326337814\n",
            "g_loss:[1.2028446, 1.1437802, 0.029532172]\n",
            "Batch:25\n",
            "d_loss_real:1.2752220630645752\n",
            "d_loss_fake:0.12165059894323349\n",
            "d_loss_wrong:0.9156789779663086\n",
            "d_loss:0.8969434201717377\n",
            "g_loss:[1.8661811, 1.804992, 0.030594576]\n",
            "Batch:26\n",
            "d_loss_real:1.2702345848083496\n",
            "d_loss_fake:0.18137040734291077\n",
            "d_loss_wrong:0.7685542106628418\n",
            "d_loss:0.8725984394550323\n",
            "g_loss:[2.230567, 2.192697, 0.018935002]\n",
            "Batch:27\n",
            "d_loss_real:1.5510518550872803\n",
            "d_loss_fake:0.08096537739038467\n",
            "d_loss_wrong:1.0349032878875732\n",
            "d_loss:1.054493099451065\n",
            "g_loss:[1.700605, 1.659776, 0.020414555]\n",
            "Batch:28\n",
            "d_loss_real:1.265594482421875\n",
            "d_loss_fake:0.08031643182039261\n",
            "d_loss_wrong:0.8791056871414185\n",
            "d_loss:0.8726527690887451\n",
            "g_loss:[2.5980794, 2.541278, 0.028400812]\n",
            "Batch:29\n",
            "d_loss_real:1.2319388389587402\n",
            "d_loss_fake:0.15590083599090576\n",
            "d_loss_wrong:0.6914597749710083\n",
            "d_loss:0.8278095722198486\n",
            "g_loss:[1.5339091, 1.4780797, 0.027914716]\n",
            "Batch:30\n",
            "d_loss_real:1.3187202215194702\n",
            "d_loss_fake:0.18903949856758118\n",
            "d_loss_wrong:1.0899404287338257\n",
            "d_loss:0.9791050851345062\n",
            "g_loss:[0.97983414, 0.93823045, 0.020801853]\n",
            "Batch:31\n",
            "d_loss_real:1.1170568466186523\n",
            "d_loss_fake:0.13156487047672272\n",
            "d_loss_wrong:0.7333874106407166\n",
            "d_loss:0.7747664898633957\n",
            "g_loss:[3.5085664, 3.460133, 0.024216603]\n",
            "Batch:32\n",
            "d_loss_real:1.4699885845184326\n",
            "d_loss_fake:0.020280811935663223\n",
            "d_loss_wrong:0.5901150703430176\n",
            "d_loss:0.8875932693481445\n",
            "g_loss:[2.4379447, 2.3865733, 0.025685668]\n",
            "Batch:33\n",
            "d_loss_real:1.155951976776123\n",
            "d_loss_fake:0.026135355234146118\n",
            "d_loss_wrong:0.8547820448875427\n",
            "d_loss:0.7982053458690643\n",
            "g_loss:[1.3460716, 1.2884302, 0.028820712]\n",
            "Batch:34\n",
            "d_loss_real:1.1603000164031982\n",
            "d_loss_fake:0.0215546116232872\n",
            "d_loss_wrong:0.721746563911438\n",
            "d_loss:0.765975296497345\n",
            "g_loss:[1.2604543, 1.1973938, 0.031530254]\n",
            "Batch:35\n",
            "d_loss_real:1.1537920236587524\n",
            "d_loss_fake:0.16589096188545227\n",
            "d_loss_wrong:0.7510090470314026\n",
            "d_loss:0.8061210215091705\n",
            "g_loss:[2.3241282, 2.2673817, 0.028373234]\n",
            "Batch:36\n",
            "d_loss_real:1.0532904863357544\n",
            "d_loss_fake:0.04159190505743027\n",
            "d_loss_wrong:0.7854007482528687\n",
            "d_loss:0.7333934009075165\n",
            "g_loss:[1.9071499, 1.8609593, 0.023095295]\n",
            "Batch:37\n",
            "d_loss_real:0.985089898109436\n",
            "d_loss_fake:0.0582248754799366\n",
            "d_loss_wrong:0.9512980580329895\n",
            "d_loss:0.7449256777763367\n",
            "g_loss:[1.7021859, 1.6616417, 0.020272046]\n",
            "Batch:38\n",
            "d_loss_real:1.2390720844268799\n",
            "d_loss_fake:0.023609384894371033\n",
            "d_loss_wrong:0.8056627511978149\n",
            "d_loss:0.8268540799617767\n",
            "g_loss:[1.5084599, 1.467489, 0.02048546]\n",
            "Batch:39\n",
            "d_loss_real:1.1051125526428223\n",
            "d_loss_fake:0.04382847622036934\n",
            "d_loss_wrong:0.7335913777351379\n",
            "d_loss:0.7469112426042557\n",
            "g_loss:[1.207419, 1.1698605, 0.018779274]\n",
            "Batch:40\n",
            "d_loss_real:1.2026793956756592\n",
            "d_loss_fake:0.03623897582292557\n",
            "d_loss_wrong:1.0408334732055664\n",
            "d_loss:0.8706078231334686\n",
            "g_loss:[1.376484, 1.3402908, 0.018096633]\n",
            "Batch:41\n",
            "d_loss_real:1.0319627523422241\n",
            "d_loss_fake:0.04758523032069206\n",
            "d_loss_wrong:0.6965362429618835\n",
            "d_loss:0.7020117491483688\n",
            "g_loss:[1.0241257, 0.9841374, 0.019994136]\n",
            "Batch:42\n",
            "d_loss_real:1.2217543125152588\n",
            "d_loss_fake:0.03430996462702751\n",
            "d_loss_wrong:0.6946489214897156\n",
            "d_loss:0.793116882443428\n",
            "g_loss:[1.2416799, 1.1971644, 0.022257738]\n",
            "Batch:43\n",
            "d_loss_real:1.0325813293457031\n",
            "d_loss_fake:0.0546385794878006\n",
            "d_loss_wrong:0.7025827169418335\n",
            "d_loss:0.7055959850549698\n",
            "g_loss:[0.5683688, 0.5043013, 0.032033756]\n",
            "Batch:44\n",
            "d_loss_real:0.9927055239677429\n",
            "d_loss_fake:0.03382579982280731\n",
            "d_loss_wrong:0.7750158905982971\n",
            "d_loss:0.6985631883144379\n",
            "g_loss:[0.92784196, 0.87391275, 0.026964607]\n",
            "Batch:45\n",
            "d_loss_real:1.0426753759384155\n",
            "d_loss_fake:0.12035344541072845\n",
            "d_loss_wrong:0.6404592990875244\n",
            "d_loss:0.7115408778190613\n",
            "g_loss:[1.7844312, 1.7266872, 0.02887203]\n",
            "Batch:46\n",
            "d_loss_real:0.9953554272651672\n",
            "d_loss_fake:0.015292482450604439\n",
            "d_loss_wrong:0.7576321363449097\n",
            "d_loss:0.6909088641405106\n",
            "g_loss:[1.1427081, 1.0864139, 0.02814712]\n",
            "Batch:47\n",
            "d_loss_real:1.056078314781189\n",
            "d_loss_fake:0.05108077824115753\n",
            "d_loss_wrong:0.7316269874572754\n",
            "d_loss:0.7237160950899124\n",
            "g_loss:[1.2565678, 1.2068299, 0.024868958]\n",
            "Batch:48\n",
            "d_loss_real:1.1060295104980469\n",
            "d_loss_fake:0.14648601412773132\n",
            "d_loss_wrong:0.6452353596687317\n",
            "d_loss:0.7509450912475586\n",
            "g_loss:[2.3172092, 2.2462711, 0.035469048]\n",
            "Batch:49\n",
            "d_loss_real:1.1025127172470093\n",
            "d_loss_fake:0.007088528946042061\n",
            "d_loss_wrong:0.6832064390182495\n",
            "d_loss:0.7238301038742065\n",
            "g_loss:[2.2410731, 2.167119, 0.036977075]\n",
            "Batch:50\n",
            "d_loss_real:1.0415914058685303\n",
            "d_loss_fake:0.008189395070075989\n",
            "d_loss_wrong:0.7077537775039673\n",
            "d_loss:0.6997814923524857\n",
            "g_loss:[1.8651115, 1.814157, 0.025477247]\n",
            "Batch:51\n",
            "d_loss_real:0.9709378480911255\n",
            "d_loss_fake:0.03455192595720291\n",
            "d_loss_wrong:0.7655407190322876\n",
            "d_loss:0.6854920834302902\n",
            "g_loss:[1.9803104, 1.9294788, 0.025415868]\n",
            "Batch:52\n",
            "d_loss_real:0.9512934684753418\n",
            "d_loss_fake:0.01418144442141056\n",
            "d_loss_wrong:0.7765005826950073\n",
            "d_loss:0.6733172386884689\n",
            "g_loss:[1.2226189, 1.1846298, 0.018994574]\n",
            "Batch:53\n",
            "d_loss_real:1.0897400379180908\n",
            "d_loss_fake:0.0332518070936203\n",
            "d_loss_wrong:0.7152165174484253\n",
            "d_loss:0.7319871038198471\n",
            "g_loss:[1.7159283, 1.6490614, 0.033433415]\n",
            "Batch:54\n",
            "d_loss_real:0.9281042814254761\n",
            "d_loss_fake:0.03617426007986069\n",
            "d_loss_wrong:0.76592618227005\n",
            "d_loss:0.6645772457122803\n",
            "g_loss:[2.33311, 2.2704117, 0.03134915]\n",
            "Batch:55\n",
            "d_loss_real:0.9369009733200073\n",
            "d_loss_fake:0.03878632187843323\n",
            "d_loss_wrong:0.7208864688873291\n",
            "d_loss:0.6583686769008636\n",
            "g_loss:[1.4592845, 1.398035, 0.030624773]\n",
            "Batch:56\n",
            "d_loss_real:1.0739113092422485\n",
            "d_loss_fake:0.04246469587087631\n",
            "d_loss_wrong:0.7308520078659058\n",
            "d_loss:0.7302848249673843\n",
            "g_loss:[1.4465983, 1.3948975, 0.025850417]\n",
            "Batch:57\n",
            "d_loss_real:1.030369758605957\n",
            "d_loss_fake:0.16858325898647308\n",
            "d_loss_wrong:0.6933127045631409\n",
            "d_loss:0.7306588739156723\n",
            "g_loss:[2.7140965, 2.649643, 0.032226745]\n",
            "Batch:58\n",
            "d_loss_real:1.0104341506958008\n",
            "d_loss_fake:0.20791713893413544\n",
            "d_loss_wrong:0.6501063704490662\n",
            "d_loss:0.7197229564189911\n",
            "g_loss:[2.6412804, 2.5818262, 0.029727075]\n",
            "Batch:59\n",
            "d_loss_real:1.1825408935546875\n",
            "d_loss_fake:0.1250818371772766\n",
            "d_loss_wrong:0.6820086240768433\n",
            "d_loss:0.7930430620908737\n",
            "g_loss:[2.1855648, 2.139249, 0.023157839]\n",
            "Batch:60\n",
            "d_loss_real:1.2117919921875\n",
            "d_loss_fake:0.12366651743650436\n",
            "d_loss_wrong:0.6930630803108215\n",
            "d_loss:0.8100783973932266\n",
            "g_loss:[1.729584, 1.6871635, 0.021210268]\n",
            "Batch:61\n",
            "d_loss_real:1.1502254009246826\n",
            "d_loss_fake:0.17893536388874054\n",
            "d_loss_wrong:0.6452024579048157\n",
            "d_loss:0.7811471521854401\n",
            "g_loss:[1.2510295, 1.2099164, 0.020556577]\n",
            "Batch:62\n",
            "d_loss_real:1.2401342391967773\n",
            "d_loss_fake:0.25401973724365234\n",
            "d_loss_wrong:0.5578964352607727\n",
            "d_loss:0.8230461627244949\n",
            "g_loss:[2.7083848, 2.6638367, 0.022273961]\n",
            "Batch:63\n",
            "d_loss_real:1.499013066291809\n",
            "d_loss_fake:0.07809926569461823\n",
            "d_loss_wrong:0.560940146446228\n",
            "d_loss:0.9092663824558258\n",
            "g_loss:[2.1400366, 2.0869458, 0.026545446]\n",
            "Batch:64\n",
            "d_loss_real:1.1697312593460083\n",
            "d_loss_fake:0.07308992743492126\n",
            "d_loss_wrong:0.6887799501419067\n",
            "d_loss:0.7753331065177917\n",
            "g_loss:[1.7576481, 1.7129805, 0.02233382]\n",
            "Batch:65\n",
            "d_loss_real:1.1451263427734375\n",
            "d_loss_fake:0.1227736622095108\n",
            "d_loss_wrong:0.7112987637519836\n",
            "d_loss:0.7810812741518021\n",
            "g_loss:[1.6038448, 1.5488572, 0.027493771]\n",
            "Batch:66\n",
            "d_loss_real:1.0286056995391846\n",
            "d_loss_fake:0.09632021188735962\n",
            "d_loss_wrong:0.7025118470191956\n",
            "d_loss:0.7140108644962311\n",
            "g_loss:[1.7736506, 1.7336922, 0.019979209]\n",
            "Batch:67\n",
            "d_loss_real:1.143507957458496\n",
            "d_loss_fake:0.14799147844314575\n",
            "d_loss_wrong:0.5992112755775452\n",
            "d_loss:0.7585546672344208\n",
            "g_loss:[2.946291, 2.916693, 0.014798995]\n",
            "Batch:68\n",
            "d_loss_real:1.0275887250900269\n",
            "d_loss_fake:0.10491743683815002\n",
            "d_loss_wrong:0.7562671899795532\n",
            "d_loss:0.7290905117988586\n",
            "g_loss:[2.3369985, 2.29639, 0.020304235]\n",
            "Batch:69\n",
            "d_loss_real:1.281523585319519\n",
            "d_loss_fake:0.14694401621818542\n",
            "d_loss_wrong:0.9111440777778625\n",
            "d_loss:0.9052838087081909\n",
            "g_loss:[1.6990445, 1.650136, 0.024454217]\n",
            "Batch:70\n",
            "d_loss_real:0.9974865317344666\n",
            "d_loss_fake:0.435086190700531\n",
            "d_loss_wrong:0.5548167824745178\n",
            "d_loss:0.7462190091609955\n",
            "g_loss:[1.6037071, 1.5584633, 0.022621848]\n",
            "Batch:71\n",
            "d_loss_real:1.4045902490615845\n",
            "d_loss_fake:0.2885771095752716\n",
            "d_loss_wrong:0.431058794260025\n",
            "d_loss:0.8822041004896164\n",
            "g_loss:[2.1796782, 2.1355238, 0.02207715]\n",
            "Batch:72\n",
            "d_loss_real:1.4079266786575317\n",
            "d_loss_fake:0.1202664002776146\n",
            "d_loss_wrong:0.5474921464920044\n",
            "d_loss:0.8709029704332352\n",
            "g_loss:[2.639032, 2.6050506, 0.016990684]\n",
            "Batch:73\n",
            "d_loss_real:1.125598430633545\n",
            "d_loss_fake:0.12042377889156342\n",
            "d_loss_wrong:0.655732274055481\n",
            "d_loss:0.7568382322788239\n",
            "g_loss:[2.13706, 2.1087523, 0.014153814]\n",
            "Batch:74\n",
            "d_loss_real:1.198657751083374\n",
            "d_loss_fake:0.18848635256290436\n",
            "d_loss_wrong:0.6501563191413879\n",
            "d_loss:0.8089895397424698\n",
            "g_loss:[2.6149461, 2.591188, 0.011879136]\n",
            "Batch:75\n",
            "d_loss_real:1.1520259380340576\n",
            "d_loss_fake:0.14636752009391785\n",
            "d_loss_wrong:0.6219516396522522\n",
            "d_loss:0.7680927515029907\n",
            "g_loss:[2.2426028, 2.2167578, 0.012922501]\n",
            "Batch:76\n",
            "d_loss_real:1.201512098312378\n",
            "d_loss_fake:0.119264155626297\n",
            "d_loss_wrong:0.5828209519386292\n",
            "d_loss:0.7762773334980011\n",
            "g_loss:[1.8935379, 1.8697133, 0.011912282]\n",
            "Batch:77\n",
            "d_loss_real:1.0612837076187134\n",
            "d_loss_fake:0.12476509809494019\n",
            "d_loss_wrong:0.722822368144989\n",
            "d_loss:0.742538720369339\n",
            "g_loss:[2.6551175, 2.630887, 0.012115227]\n",
            "Batch:78\n",
            "d_loss_real:1.0746934413909912\n",
            "d_loss_fake:0.21092861890792847\n",
            "d_loss_wrong:0.6368747353553772\n",
            "d_loss:0.749297559261322\n",
            "g_loss:[2.3320873, 2.305704, 0.013191627]\n",
            "Batch:79\n",
            "d_loss_real:1.1354680061340332\n",
            "d_loss_fake:0.12739133834838867\n",
            "d_loss_wrong:0.5957291722297668\n",
            "d_loss:0.7485141307115555\n",
            "g_loss:[1.471777, 1.4448862, 0.013445398]\n",
            "Batch:80\n",
            "d_loss_real:1.1199421882629395\n",
            "d_loss_fake:0.10043418407440186\n",
            "d_loss_wrong:0.605201780796051\n",
            "d_loss:0.736380085349083\n",
            "g_loss:[2.0282917, 1.9986844, 0.014803608]\n",
            "Batch:81\n",
            "d_loss_real:1.0966533422470093\n",
            "d_loss_fake:0.039333224296569824\n",
            "d_loss_wrong:0.6018882989883423\n",
            "d_loss:0.7086320519447327\n",
            "g_loss:[1.4977132, 1.4683602, 0.014676489]\n",
            "Batch:82\n",
            "d_loss_real:1.2604848146438599\n",
            "d_loss_fake:0.013003760017454624\n",
            "d_loss_wrong:0.7045095562934875\n",
            "d_loss:0.80962073802948\n",
            "g_loss:[0.86185735, 0.8353431, 0.01325711]\n",
            "Batch:83\n",
            "d_loss_real:0.8743438720703125\n",
            "d_loss_fake:0.05436353385448456\n",
            "d_loss_wrong:0.706960141658783\n",
            "d_loss:0.6275028586387634\n",
            "g_loss:[0.8136402, 0.78230274, 0.015668707]\n",
            "Batch:84\n",
            "d_loss_real:0.9542349576950073\n",
            "d_loss_fake:0.006650532130151987\n",
            "d_loss_wrong:0.7307475209236145\n",
            "d_loss:0.6614669859409332\n",
            "g_loss:[0.92141587, 0.89458764, 0.013414118]\n",
            "Batch:85\n",
            "d_loss_real:0.9557656049728394\n",
            "d_loss_fake:0.02472229301929474\n",
            "d_loss_wrong:0.7152915596961975\n",
            "d_loss:0.6628862619400024\n",
            "g_loss:[0.72019655, 0.69286346, 0.0136665525]\n",
            "Batch:86\n",
            "d_loss_real:0.9474753141403198\n",
            "d_loss_fake:0.06127597391605377\n",
            "d_loss_wrong:0.6745709180831909\n",
            "d_loss:0.6576993763446808\n",
            "g_loss:[1.1077911, 1.0849407, 0.01142519]\n",
            "Batch:87\n",
            "d_loss_real:0.9932762384414673\n",
            "d_loss_fake:0.043318010866642\n",
            "d_loss_wrong:0.6488953232765198\n",
            "d_loss:0.6696914583444595\n",
            "g_loss:[1.2692916, 1.2442042, 0.012543721]\n",
            "Batch:88\n",
            "d_loss_real:1.0257899761199951\n",
            "d_loss_fake:0.010599635541439056\n",
            "d_loss_wrong:0.7070949077606201\n",
            "d_loss:0.6923186182975769\n",
            "g_loss:[0.6293474, 0.6030458, 0.013150778]\n",
            "Batch:89\n",
            "d_loss_real:0.9715819358825684\n",
            "d_loss_fake:0.0323367640376091\n",
            "d_loss_wrong:0.8778905868530273\n",
            "d_loss:0.7133478075265884\n",
            "g_loss:[0.83304596, 0.78596383, 0.023541052]\n",
            "Batch:90\n",
            "d_loss_real:0.9669756889343262\n",
            "d_loss_fake:0.07475192844867706\n",
            "d_loss_wrong:0.7189085483551025\n",
            "d_loss:0.6819029599428177\n",
            "g_loss:[0.984205, 0.94748163, 0.018361676]\n",
            "Batch:91\n",
            "d_loss_real:1.0442644357681274\n",
            "d_loss_fake:0.007645203731954098\n",
            "d_loss_wrong:0.7253062725067139\n",
            "d_loss:0.7053700834512711\n",
            "g_loss:[1.1033484, 1.0691082, 0.017120063]\n",
            "Batch:92\n",
            "d_loss_real:1.1253913640975952\n",
            "d_loss_fake:0.040578048676252365\n",
            "d_loss_wrong:0.9197668433189392\n",
            "d_loss:0.8027819097042084\n",
            "g_loss:[0.82452315, 0.7723067, 0.026108239]\n",
            "Batch:93\n",
            "d_loss_real:1.0242407321929932\n",
            "d_loss_fake:0.010085681453347206\n",
            "d_loss_wrong:0.6783347129821777\n",
            "d_loss:0.684225469827652\n",
            "g_loss:[0.8473691, 0.807472, 0.019948535]\n",
            "Batch:94\n",
            "d_loss_real:0.972442626953125\n",
            "d_loss_fake:0.023032143712043762\n",
            "d_loss_wrong:0.7324427962303162\n",
            "d_loss:0.6750900447368622\n",
            "g_loss:[0.79239655, 0.75662255, 0.017887004]\n",
            "Batch:95\n",
            "d_loss_real:0.9526944160461426\n",
            "d_loss_fake:0.02497394196689129\n",
            "d_loss_wrong:0.6970685720443726\n",
            "d_loss:0.6568578332662582\n",
            "g_loss:[0.74610114, 0.71071064, 0.017695248]\n",
            "Batch:96\n",
            "d_loss_real:0.8559237122535706\n",
            "d_loss_fake:0.007905581034719944\n",
            "d_loss_wrong:0.7104368209838867\n",
            "d_loss:0.6075474619865417\n",
            "g_loss:[0.6614274, 0.6242536, 0.018586893]\n",
            "Batch:97\n",
            "d_loss_real:0.9648327827453613\n",
            "d_loss_fake:0.011923074722290039\n",
            "d_loss_wrong:0.6770893931388855\n",
            "d_loss:0.6546695083379745\n",
            "g_loss:[0.8621825, 0.82973456, 0.016223967]\n",
            "Batch:98\n",
            "d_loss_real:0.9414933919906616\n",
            "d_loss_fake:0.01377798244357109\n",
            "d_loss_wrong:0.7194395661354065\n",
            "d_loss:0.6540510803461075\n",
            "g_loss:[0.56149876, 0.5333693, 0.014064741]\n",
            "Batch:99\n",
            "d_loss_real:1.003293514251709\n",
            "d_loss_fake:0.019957300275564194\n",
            "d_loss_wrong:0.6344247460365295\n",
            "d_loss:0.6652422696352005\n",
            "g_loss:[0.5783905, 0.54464734, 0.016871566]\n",
            "Batch:100\n",
            "d_loss_real:0.9168709516525269\n",
            "d_loss_fake:0.003098498098552227\n",
            "d_loss_wrong:0.6966222524642944\n",
            "d_loss:0.633365660905838\n",
            "g_loss:[0.62342113, 0.5886564, 0.017382346]\n",
            "Batch:101\n",
            "d_loss_real:0.909461498260498\n",
            "d_loss_fake:0.002947029657661915\n",
            "d_loss_wrong:0.7535626888275146\n",
            "d_loss:0.6438581794500351\n",
            "g_loss:[0.5152489, 0.49010402, 0.012572441]\n",
            "Batch:102\n",
            "d_loss_real:0.9726940393447876\n",
            "d_loss_fake:0.004379652440547943\n",
            "d_loss_wrong:0.7141633033752441\n",
            "d_loss:0.6659827530384064\n",
            "g_loss:[0.4921972, 0.46164143, 0.015277874]\n",
            "Batch:103\n",
            "d_loss_real:1.018676996231079\n",
            "d_loss_fake:0.005219834856688976\n",
            "d_loss_wrong:0.6614396572113037\n",
            "d_loss:0.6760033667087555\n",
            "g_loss:[0.47097382, 0.43423107, 0.01837137]\n",
            "Batch:104\n",
            "d_loss_real:1.0272397994995117\n",
            "d_loss_fake:0.005768524017184973\n",
            "d_loss_wrong:0.7247905135154724\n",
            "d_loss:0.6962596625089645\n",
            "g_loss:[0.40261033, 0.37604618, 0.013282079]\n",
            "Batch:105\n",
            "d_loss_real:0.9702713489532471\n",
            "d_loss_fake:0.0038905804976820946\n",
            "d_loss_wrong:0.7408472895622253\n",
            "d_loss:0.6713201403617859\n",
            "g_loss:[0.4517821, 0.42985636, 0.010962867]\n",
            "Batch:106\n",
            "d_loss_real:0.8204565644264221\n",
            "d_loss_fake:0.0017968034371733665\n",
            "d_loss_wrong:0.7356389760971069\n",
            "d_loss:0.5945872217416763\n",
            "g_loss:[0.43275207, 0.4103639, 0.011194076]\n",
            "Batch:107\n",
            "d_loss_real:0.9267326593399048\n",
            "d_loss_fake:0.010506193153560162\n",
            "d_loss_wrong:0.7102208137512207\n",
            "d_loss:0.6435480862855911\n",
            "g_loss:[0.4085971, 0.38130486, 0.013646122]\n",
            "Batch:108\n",
            "d_loss_real:0.9295374155044556\n",
            "d_loss_fake:0.0044266399927437305\n",
            "d_loss_wrong:0.6871495246887207\n",
            "d_loss:0.6376627534627914\n",
            "g_loss:[0.5193427, 0.49317607, 0.013083319]\n",
            "Batch:109\n",
            "d_loss_real:0.9247746467590332\n",
            "d_loss_fake:0.004189218394458294\n",
            "d_loss_wrong:0.6966421604156494\n",
            "d_loss:0.6375951617956161\n",
            "g_loss:[0.47180924, 0.44495073, 0.013429251]\n",
            "Batch:110\n",
            "d_loss_real:0.8732386231422424\n",
            "d_loss_fake:0.00710414769127965\n",
            "d_loss_wrong:0.694709837436676\n",
            "d_loss:0.6120728105306625\n",
            "g_loss:[0.43496755, 0.4030584, 0.015954567]\n",
            "Batch:111\n",
            "d_loss_real:0.8759846687316895\n",
            "d_loss_fake:0.0032499146182090044\n",
            "d_loss_wrong:0.6961767077445984\n",
            "d_loss:0.6128489971160889\n",
            "g_loss:[0.5150617, 0.4886889, 0.0131864045]\n",
            "Batch:112\n",
            "d_loss_real:0.9521435499191284\n",
            "d_loss_fake:0.017802491784095764\n",
            "d_loss_wrong:0.7015358805656433\n",
            "d_loss:0.6559063643217087\n",
            "g_loss:[0.4802347, 0.44381434, 0.01821019]\n",
            "Batch:113\n",
            "d_loss_real:0.8948889970779419\n",
            "d_loss_fake:0.0038347654044628143\n",
            "d_loss_wrong:0.6982550024986267\n",
            "d_loss:0.6229669451713562\n",
            "g_loss:[0.5083766, 0.4767779, 0.015799358]\n",
            "Batch:114\n",
            "d_loss_real:0.8714256882667542\n",
            "d_loss_fake:0.0016169465379789472\n",
            "d_loss_wrong:0.6719480752944946\n",
            "d_loss:0.6041041016578674\n",
            "g_loss:[0.43855634, 0.4093516, 0.014602384]\n",
            "Batch:115\n",
            "d_loss_real:0.8698937296867371\n",
            "d_loss_fake:0.0032459115609526634\n",
            "d_loss_wrong:0.7047476172447205\n",
            "d_loss:0.611945241689682\n",
            "g_loss:[0.5579154, 0.5310599, 0.013427738]\n",
            "Batch:116\n",
            "d_loss_real:0.9929553270339966\n",
            "d_loss_fake:0.006892675533890724\n",
            "d_loss_wrong:0.67048579454422\n",
            "d_loss:0.6658222824335098\n",
            "g_loss:[0.3881989, 0.36171377, 0.013242576]\n",
            "Batch:117\n",
            "d_loss_real:0.8479496836662292\n",
            "d_loss_fake:0.004412129521369934\n",
            "d_loss_wrong:0.7515251636505127\n",
            "d_loss:0.612959161400795\n",
            "g_loss:[0.40639523, 0.38399184, 0.011201691]\n",
            "Batch:118\n",
            "d_loss_real:0.8614047765731812\n",
            "d_loss_fake:0.003289151471108198\n",
            "d_loss_wrong:0.6878043413162231\n",
            "d_loss:0.6034757643938065\n",
            "g_loss:[0.40261912, 0.3815608, 0.01052916]\n",
            "Batch:119\n",
            "d_loss_real:0.8968122601509094\n",
            "d_loss_fake:0.0021167020313441753\n",
            "d_loss_wrong:0.6858479380607605\n",
            "d_loss:0.6203972846269608\n",
            "g_loss:[0.39615357, 0.3780421, 0.009055741]\n",
            "Batch:120\n",
            "d_loss_real:0.8822365403175354\n",
            "d_loss_fake:0.0023542670533061028\n",
            "d_loss_wrong:0.6943323016166687\n",
            "d_loss:0.6152899116277695\n",
            "g_loss:[0.37870413, 0.35716218, 0.010770969]\n",
            "Batch:121\n",
            "d_loss_real:0.9453120231628418\n",
            "d_loss_fake:0.002010112628340721\n",
            "d_loss_wrong:0.697215735912323\n",
            "d_loss:0.647462472319603\n",
            "g_loss:[0.3741194, 0.3536245, 0.010247454]\n",
            "Batch:122\n",
            "d_loss_real:0.8466260433197021\n",
            "d_loss_fake:0.007707349490374327\n",
            "d_loss_wrong:0.7148011922836304\n",
            "d_loss:0.6039401590824127\n",
            "g_loss:[0.41361263, 0.39261347, 0.010499578]\n",
            "Batch:123\n",
            "d_loss_real:1.0481081008911133\n",
            "d_loss_fake:0.0013039548648521304\n",
            "d_loss_wrong:0.6948754787445068\n",
            "d_loss:0.6980989128351212\n",
            "g_loss:[0.40761957, 0.38741538, 0.0101020895]\n",
            "Batch:124\n",
            "d_loss_real:0.9084234833717346\n",
            "d_loss_fake:0.005979540292173624\n",
            "d_loss_wrong:0.6736230850219727\n",
            "d_loss:0.6241123974323273\n",
            "g_loss:[0.36823437, 0.35134327, 0.00844554]\n",
            "Batch:125\n",
            "d_loss_real:0.814927339553833\n",
            "d_loss_fake:0.003685989184305072\n",
            "d_loss_wrong:0.7239137291908264\n",
            "d_loss:0.5893636047840118\n",
            "g_loss:[0.3916279, 0.36993235, 0.010847783]\n",
            "Batch:126\n",
            "d_loss_real:0.8445447683334351\n",
            "d_loss_fake:0.004537475295364857\n",
            "d_loss_wrong:0.7035741806030273\n",
            "d_loss:0.5993002951145172\n",
            "g_loss:[0.35512635, 0.33683586, 0.0091452515]\n",
            "Batch:127\n",
            "d_loss_real:0.98908931016922\n",
            "d_loss_fake:0.005118927452713251\n",
            "d_loss_wrong:0.6405608057975769\n",
            "d_loss:0.655964583158493\n",
            "g_loss:[0.40305084, 0.3843504, 0.009350231]\n",
            "Batch:128\n",
            "d_loss_real:0.8729192018508911\n",
            "d_loss_fake:0.002788806799799204\n",
            "d_loss_wrong:0.756708562374115\n",
            "d_loss:0.626333937048912\n",
            "g_loss:[0.39269248, 0.35418463, 0.019253928]\n",
            "Batch:129\n",
            "d_loss_real:0.8687390089035034\n",
            "d_loss_fake:0.003962001297622919\n",
            "d_loss_wrong:0.7088868618011475\n",
            "d_loss:0.6125817149877548\n",
            "g_loss:[0.38160208, 0.34747893, 0.017061576]\n",
            "Batch:130\n",
            "d_loss_real:0.8751399517059326\n",
            "d_loss_fake:0.002168267033994198\n",
            "d_loss_wrong:0.7032396793365479\n",
            "d_loss:0.6139219552278519\n",
            "g_loss:[0.436588, 0.41278046, 0.011903761]\n",
            "Batch:131\n",
            "d_loss_real:0.919884979724884\n",
            "d_loss_fake:0.0009699609945528209\n",
            "d_loss_wrong:0.7186785340309143\n",
            "d_loss:0.6398546099662781\n",
            "g_loss:[0.3868734, 0.3533946, 0.016739398]\n",
            "Batch:132\n",
            "d_loss_real:0.8647513389587402\n",
            "d_loss_fake:0.0015070249792188406\n",
            "d_loss_wrong:0.6887342929840088\n",
            "d_loss:0.6049360036849976\n",
            "g_loss:[0.37253523, 0.34682357, 0.012855826]\n",
            "Batch:133\n",
            "d_loss_real:0.8681739568710327\n",
            "d_loss_fake:0.002772744046524167\n",
            "d_loss_wrong:0.6990785598754883\n",
            "d_loss:0.609549805521965\n",
            "g_loss:[0.35405478, 0.33556452, 0.009245134]\n",
            "Batch:134\n",
            "d_loss_real:0.8634783029556274\n",
            "d_loss_fake:0.0015015496173873544\n",
            "d_loss_wrong:0.6928215622901917\n",
            "d_loss:0.605319932103157\n",
            "g_loss:[0.37326914, 0.35237527, 0.010446941]\n",
            "Batch:135\n",
            "d_loss_real:0.877082884311676\n",
            "d_loss_fake:0.002270057564601302\n",
            "d_loss_wrong:0.6837548017501831\n",
            "d_loss:0.6100476533174515\n",
            "g_loss:[0.35775414, 0.3411721, 0.008291027]\n",
            "Batch:136\n",
            "d_loss_real:0.8663421869277954\n",
            "d_loss_fake:0.0013992040185257792\n",
            "d_loss_wrong:0.6931408643722534\n",
            "d_loss:0.6068061143159866\n",
            "g_loss:[0.3883956, 0.37252712, 0.007934239]\n",
            "Batch:137\n",
            "d_loss_real:0.9086413383483887\n",
            "d_loss_fake:0.001500951126217842\n",
            "d_loss_wrong:0.6661895513534546\n",
            "d_loss:0.6212432980537415\n",
            "g_loss:[0.3780845, 0.3607743, 0.008655107]\n",
            "Batch:138\n",
            "d_loss_real:0.8507082462310791\n",
            "d_loss_fake:0.0018718214705586433\n",
            "d_loss_wrong:0.6924365162849426\n",
            "d_loss:0.5989312082529068\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "g_loss:[0.37593406, 0.35903326, 0.008450404]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "========================================\n",
            "Epoch is: 1\n",
            "Number of batches 138\n",
            "Batch:1\n",
            "d_loss_real:0.8891799449920654\n",
            "d_loss_fake:0.0017146257450804114\n",
            "d_loss_wrong:0.7229893803596497\n",
            "d_loss:0.6257659792900085\n",
            "g_loss:[0.3839865, 0.3667846, 0.008600948]\n",
            "Batch:2\n",
            "d_loss_real:0.8489013314247131\n",
            "d_loss_fake:0.0023936214856803417\n",
            "d_loss_wrong:0.715271532535553\n",
            "d_loss:0.6038669496774673\n",
            "g_loss:[0.3564856, 0.33116955, 0.012658022]\n",
            "Batch:3\n",
            "d_loss_real:0.8819365501403809\n",
            "d_loss_fake:0.0012931202072650194\n",
            "d_loss_wrong:0.6991437673568726\n",
            "d_loss:0.6160774976015091\n",
            "g_loss:[0.35283354, 0.3313926, 0.01072048]\n",
            "Batch:4\n",
            "d_loss_real:0.9389616847038269\n",
            "d_loss_fake:0.0022730394266545773\n",
            "d_loss_wrong:0.7766427993774414\n",
            "d_loss:0.6642097979784012\n",
            "g_loss:[0.40251634, 0.37762922, 0.012443564]\n",
            "Batch:5\n",
            "d_loss_real:0.8441025614738464\n",
            "d_loss_fake:0.0012163880746811628\n",
            "d_loss_wrong:0.8596121072769165\n",
            "d_loss:0.6372584104537964\n",
            "g_loss:[0.37576413, 0.3530957, 0.011334211]\n",
            "Batch:6\n",
            "d_loss_real:0.9044116139411926\n",
            "d_loss_fake:0.0007486624526791275\n",
            "d_loss_wrong:0.8522236943244934\n",
            "d_loss:0.6654488891363144\n",
            "g_loss:[0.35811555, 0.33751535, 0.010300092]\n",
            "Batch:7\n",
            "d_loss_real:0.9263324737548828\n",
            "d_loss_fake:0.0011451022000983357\n",
            "d_loss_wrong:0.68638014793396\n",
            "d_loss:0.6350475549697876\n",
            "g_loss:[0.3730017, 0.35385165, 0.009575026]\n",
            "Batch:8\n",
            "d_loss_real:0.9296810030937195\n",
            "d_loss_fake:0.0014736020239070058\n",
            "d_loss_wrong:0.6064178347587585\n",
            "d_loss:0.6168133616447449\n",
            "g_loss:[0.3586062, 0.32996702, 0.014319582]\n",
            "Batch:9\n",
            "d_loss_real:0.9277263283729553\n",
            "d_loss_fake:0.004222742281854153\n",
            "d_loss_wrong:0.6463795304298401\n",
            "d_loss:0.626513734459877\n",
            "g_loss:[0.3673217, 0.33369505, 0.016813327]\n",
            "Batch:10\n",
            "d_loss_real:0.8714829683303833\n",
            "d_loss_fake:0.0025639482773840427\n",
            "d_loss_wrong:0.659578800201416\n",
            "d_loss:0.6012771725654602\n",
            "g_loss:[0.38025302, 0.35212964, 0.014061695]\n",
            "Batch:11\n",
            "d_loss_real:0.9658100605010986\n",
            "d_loss_fake:0.0015309675363823771\n",
            "d_loss_wrong:0.6395991444587708\n",
            "d_loss:0.643187552690506\n",
            "g_loss:[0.34696892, 0.3309985, 0.007985211]\n",
            "Batch:12\n",
            "d_loss_real:0.9183520078659058\n",
            "d_loss_fake:0.001809495035558939\n",
            "d_loss_wrong:0.6724474430084229\n",
            "d_loss:0.6277402341365814\n",
            "g_loss:[0.34487873, 0.3294117, 0.0077335294]\n",
            "Batch:13\n",
            "d_loss_real:0.8582351207733154\n",
            "d_loss_fake:0.0011386058758944273\n",
            "d_loss_wrong:0.7021712064743042\n",
            "d_loss:0.6049450188875198\n",
            "g_loss:[0.35704693, 0.3399257, 0.008560616]\n",
            "Batch:14\n",
            "d_loss_real:0.9280509352684021\n",
            "d_loss_fake:0.001979175955057144\n",
            "d_loss_wrong:0.6854423880577087\n",
            "d_loss:0.63588085770607\n",
            "g_loss:[0.3473881, 0.33275053, 0.007318782]\n",
            "Batch:15\n",
            "d_loss_real:0.8854990005493164\n",
            "d_loss_fake:0.0015976952854543924\n",
            "d_loss_wrong:0.7616165280342102\n",
            "d_loss:0.6335530579090118\n",
            "g_loss:[0.3561217, 0.32994044, 0.01309062]\n",
            "Batch:16\n",
            "d_loss_real:0.8325037360191345\n",
            "d_loss_fake:0.001451779156923294\n",
            "d_loss_wrong:0.7776554822921753\n",
            "d_loss:0.6110286861658096\n",
            "g_loss:[0.36719373, 0.33788094, 0.014656392]\n",
            "Batch:17\n",
            "d_loss_real:0.9348185658454895\n",
            "d_loss_fake:0.0018133261473849416\n",
            "d_loss_wrong:0.8573899865150452\n",
            "d_loss:0.6822101175785065\n",
            "g_loss:[0.35173476, 0.33173883, 0.009997969]\n",
            "Batch:18\n",
            "d_loss_real:0.8964754343032837\n",
            "d_loss_fake:0.0007511620642617345\n",
            "d_loss_wrong:0.6685216426849365\n",
            "d_loss:0.6155559122562408\n",
            "g_loss:[0.35198492, 0.33684623, 0.007569337]\n",
            "Batch:19\n",
            "d_loss_real:0.9728881120681763\n",
            "d_loss_fake:0.0013367317151278257\n",
            "d_loss_wrong:0.6827454566955566\n",
            "d_loss:0.6574646085500717\n",
            "g_loss:[0.3458488, 0.33117914, 0.007334823]\n",
            "Batch:20\n",
            "d_loss_real:0.960904598236084\n",
            "d_loss_fake:0.0009433794766664505\n",
            "d_loss_wrong:0.7388705015182495\n",
            "d_loss:0.6654057651758194\n",
            "g_loss:[0.36099523, 0.3443155, 0.008339867]\n",
            "Batch:21\n",
            "d_loss_real:0.907834529876709\n",
            "d_loss_fake:0.0009395767701789737\n",
            "d_loss_wrong:0.639299750328064\n",
            "d_loss:0.6139770895242691\n",
            "g_loss:[0.34217566, 0.33013225, 0.0060217157]\n",
            "Batch:22\n",
            "d_loss_real:0.8522790670394897\n",
            "d_loss_fake:0.00132565142121166\n",
            "d_loss_wrong:0.7350910305976868\n",
            "d_loss:0.6102437078952789\n",
            "g_loss:[0.34677488, 0.33519012, 0.0057923766]\n",
            "Batch:23\n",
            "d_loss_real:0.8458924293518066\n",
            "d_loss_fake:0.0016993253957480192\n",
            "d_loss_wrong:0.7037287354469299\n",
            "d_loss:0.5993032306432724\n",
            "g_loss:[0.34436676, 0.3317873, 0.00628973]\n",
            "Batch:24\n",
            "d_loss_real:0.8498514890670776\n",
            "d_loss_fake:0.0025141104124486446\n",
            "d_loss_wrong:0.6828038096427917\n",
            "d_loss:0.5962552279233932\n",
            "g_loss:[0.37232652, 0.35226798, 0.010029271]\n",
            "Batch:25\n",
            "d_loss_real:0.9137436151504517\n",
            "d_loss_fake:0.000605240638833493\n",
            "d_loss_wrong:0.731090247631073\n",
            "d_loss:0.6397956758737564\n",
            "g_loss:[0.3649729, 0.34397173, 0.010500584]\n",
            "Batch:26\n",
            "d_loss_real:0.8947709798812866\n",
            "d_loss_fake:0.000477912079077214\n",
            "d_loss_wrong:0.6911634206771851\n",
            "d_loss:0.6202958226203918\n",
            "g_loss:[0.36385554, 0.3500591, 0.006898221]\n",
            "Batch:27\n",
            "d_loss_real:0.9178675413131714\n",
            "d_loss_fake:0.0005815171753056347\n",
            "d_loss_wrong:0.7691115736961365\n",
            "d_loss:0.651357039809227\n",
            "g_loss:[0.35295567, 0.33824426, 0.007355703]\n",
            "Batch:28\n",
            "d_loss_real:0.8483065962791443\n",
            "d_loss_fake:0.0005567434709519148\n",
            "d_loss_wrong:0.6613194346427917\n",
            "d_loss:0.5896223485469818\n",
            "g_loss:[0.3475907, 0.33240077, 0.0075949766]\n",
            "Batch:29\n",
            "d_loss_real:0.8891280889511108\n",
            "d_loss_fake:0.0013845504727214575\n",
            "d_loss_wrong:0.6706825494766235\n",
            "d_loss:0.6125808209180832\n",
            "g_loss:[0.36081806, 0.3447508, 0.008033631]\n",
            "Batch:30\n",
            "d_loss_real:0.9210551977157593\n",
            "d_loss_fake:0.001511523500084877\n",
            "d_loss_wrong:0.7733220458030701\n",
            "d_loss:0.6542359888553619\n",
            "g_loss:[0.3463075, 0.33278513, 0.0067611774]\n",
            "Batch:31\n",
            "d_loss_real:0.8471184968948364\n",
            "d_loss_fake:0.0006091067334637046\n",
            "d_loss_wrong:0.6994146108627319\n",
            "d_loss:0.5985651761293411\n",
            "g_loss:[0.35295492, 0.3371343, 0.007910309]\n",
            "Batch:32\n",
            "d_loss_real:0.8877443671226501\n",
            "d_loss_fake:0.001112615573219955\n",
            "d_loss_wrong:0.6365535855293274\n",
            "d_loss:0.6032887399196625\n",
            "g_loss:[0.35634118, 0.34150714, 0.007417028]\n",
            "Batch:33\n",
            "d_loss_real:0.8741400241851807\n",
            "d_loss_fake:0.001047275960445404\n",
            "d_loss_wrong:0.6804461479187012\n",
            "d_loss:0.6074433624744415\n",
            "g_loss:[0.34918773, 0.3337709, 0.0077084117]\n",
            "Batch:34\n",
            "d_loss_real:0.9079412817955017\n",
            "d_loss_fake:0.0012586764059960842\n",
            "d_loss_wrong:0.6456537246704102\n",
            "d_loss:0.6156987398862839\n",
            "g_loss:[0.370428, 0.35127482, 0.009576582]\n",
            "Batch:35\n",
            "d_loss_real:0.901543140411377\n",
            "d_loss_fake:0.0013455264270305634\n",
            "d_loss_wrong:0.6994521021842957\n",
            "d_loss:0.6259709745645523\n",
            "g_loss:[0.35646197, 0.33857813, 0.008941925]\n",
            "Batch:36\n",
            "d_loss_real:0.8472549319267273\n",
            "d_loss_fake:0.002250256948173046\n",
            "d_loss_wrong:0.6922749280929565\n",
            "d_loss:0.5972587615251541\n",
            "g_loss:[0.34673363, 0.33183625, 0.007448687]\n",
            "Batch:37\n",
            "d_loss_real:0.8767386674880981\n",
            "d_loss_fake:0.0006914890254847705\n",
            "d_loss_wrong:0.7213499546051025\n",
            "d_loss:0.6188796907663345\n",
            "g_loss:[0.37110093, 0.3579567, 0.0065721124]\n",
            "Batch:38\n",
            "d_loss_real:0.9072229266166687\n",
            "d_loss_fake:0.002024927409365773\n",
            "d_loss_wrong:0.7102041840553284\n",
            "d_loss:0.631668746471405\n",
            "g_loss:[0.35254273, 0.3394571, 0.0065428144]\n",
            "Batch:39\n",
            "d_loss_real:0.8641465306282043\n",
            "d_loss_fake:0.0016245623119175434\n",
            "d_loss_wrong:0.6966844797134399\n",
            "d_loss:0.6066505312919617\n",
            "g_loss:[0.37056345, 0.35863572, 0.0059638666]\n",
            "Batch:40\n",
            "d_loss_real:0.9270158410072327\n",
            "d_loss_fake:0.002374292816966772\n",
            "d_loss_wrong:0.8036996126174927\n",
            "d_loss:0.6650263965129852\n",
            "g_loss:[0.36363629, 0.35186958, 0.005883353]\n",
            "Batch:41\n",
            "d_loss_real:0.8665327429771423\n",
            "d_loss_fake:0.00024799982202239335\n",
            "d_loss_wrong:0.6720023155212402\n",
            "d_loss:0.6013289541006088\n",
            "g_loss:[0.35056382, 0.33781517, 0.0063743303]\n",
            "Batch:42\n",
            "d_loss_real:0.9416438341140747\n",
            "d_loss_fake:0.0011235821293666959\n",
            "d_loss_wrong:0.6625972986221313\n",
            "d_loss:0.6367521435022354\n",
            "g_loss:[0.37049857, 0.35750192, 0.0064983238]\n",
            "Batch:43\n",
            "d_loss_real:0.8263460397720337\n",
            "d_loss_fake:0.0010083337547257543\n",
            "d_loss_wrong:0.6902113556861877\n",
            "d_loss:0.5859779417514801\n",
            "g_loss:[0.35308623, 0.33870363, 0.007191296]\n",
            "Batch:44\n",
            "d_loss_real:0.8364203572273254\n",
            "d_loss_fake:0.0013208293821662664\n",
            "d_loss_wrong:0.707157552242279\n",
            "d_loss:0.5953297764062881\n",
            "g_loss:[0.353015, 0.34004015, 0.0064874324]\n",
            "Batch:45\n",
            "d_loss_real:0.8231979012489319\n",
            "d_loss_fake:0.0007730366778559983\n",
            "d_loss_wrong:0.6624970436096191\n",
            "d_loss:0.5774164646863937\n",
            "g_loss:[0.34316367, 0.33038312, 0.006390279]\n",
            "Batch:46\n",
            "d_loss_real:0.8655776977539062\n",
            "d_loss_fake:0.0030493016820400953\n",
            "d_loss_wrong:0.6762953400611877\n",
            "d_loss:0.6026250123977661\n",
            "g_loss:[0.3727819, 0.35958385, 0.006599026]\n",
            "Batch:47\n",
            "d_loss_real:0.8918753862380981\n",
            "d_loss_fake:0.0014570788480341434\n",
            "d_loss_wrong:0.6746131181716919\n",
            "d_loss:0.6149552464485168\n",
            "g_loss:[0.37359598, 0.36112988, 0.0062330533]\n",
            "Batch:48\n",
            "d_loss_real:0.8846668004989624\n",
            "d_loss_fake:0.0004922645166516304\n",
            "d_loss_wrong:0.6497083306312561\n",
            "d_loss:0.6048835515975952\n",
            "g_loss:[0.37336457, 0.35654977, 0.008407392]\n",
            "Batch:49\n",
            "d_loss_real:0.8429537415504456\n",
            "d_loss_fake:0.00045033456990495324\n",
            "d_loss_wrong:0.6667196154594421\n",
            "d_loss:0.5882693529129028\n",
            "g_loss:[0.36069903, 0.34628412, 0.0072074477]\n",
            "Batch:50\n",
            "d_loss_real:0.8533470034599304\n",
            "d_loss_fake:0.0027010466437786818\n",
            "d_loss_wrong:0.6660423874855042\n",
            "d_loss:0.5938593596220016\n",
            "g_loss:[0.34502572, 0.3332508, 0.005887457]\n",
            "Batch:51\n",
            "d_loss_real:0.8581788539886475\n",
            "d_loss_fake:0.0006627410184592009\n",
            "d_loss_wrong:0.680009663105011\n",
            "d_loss:0.5992575287818909\n",
            "g_loss:[0.3563908, 0.34523097, 0.005579924]\n",
            "Batch:52\n",
            "d_loss_real:0.8565901517868042\n",
            "d_loss_fake:0.001779385725967586\n",
            "d_loss_wrong:0.6904423236846924\n",
            "d_loss:0.6013505011796951\n",
            "g_loss:[0.34358466, 0.33537555, 0.004104554]\n",
            "Batch:53\n",
            "d_loss_real:0.8978493809700012\n",
            "d_loss_fake:0.0005087442114017904\n",
            "d_loss_wrong:0.659694492816925\n",
            "d_loss:0.6139754951000214\n",
            "g_loss:[0.3481326, 0.33360192, 0.007265351]\n",
            "Batch:54\n",
            "d_loss_real:0.8377178311347961\n",
            "d_loss_fake:0.002502807416021824\n",
            "d_loss_wrong:0.6984915137290955\n",
            "d_loss:0.5941074937582016\n",
            "g_loss:[0.36255124, 0.34910792, 0.006721661]\n",
            "Batch:55\n",
            "d_loss_real:0.8468356132507324\n",
            "d_loss_fake:0.0007416621665470302\n",
            "d_loss_wrong:0.6733030080795288\n",
            "d_loss:0.5919289737939835\n",
            "g_loss:[0.38311222, 0.36992356, 0.0065943347]\n",
            "Batch:56\n",
            "d_loss_real:0.882289707660675\n",
            "d_loss_fake:0.0004322243039496243\n",
            "d_loss_wrong:0.7108430862426758\n",
            "d_loss:0.6189636886119843\n",
            "g_loss:[0.37044147, 0.3598723, 0.005284582]\n",
            "Batch:57\n",
            "d_loss_real:0.8855734467506409\n",
            "d_loss_fake:0.00039420108078047633\n",
            "d_loss_wrong:0.6598718762397766\n",
            "d_loss:0.6078532487154007\n",
            "g_loss:[0.3544946, 0.34037513, 0.0070597385]\n",
            "Batch:58\n",
            "d_loss_real:0.8506063222885132\n",
            "d_loss_fake:0.0012072413228452206\n",
            "d_loss_wrong:0.6842691898345947\n",
            "d_loss:0.5966722667217255\n",
            "g_loss:[0.34369954, 0.33042124, 0.0066391528]\n",
            "Batch:59\n",
            "d_loss_real:0.8556756377220154\n",
            "d_loss_fake:0.0005376370390877128\n",
            "d_loss_wrong:0.7015734314918518\n",
            "d_loss:0.6033655852079391\n",
            "g_loss:[0.3428669, 0.33175796, 0.0055544684]\n",
            "Batch:60\n",
            "d_loss_real:0.8778462409973145\n",
            "d_loss_fake:0.001243398990482092\n",
            "d_loss_wrong:0.7042531371116638\n",
            "d_loss:0.615297257900238\n",
            "g_loss:[0.36345842, 0.35254908, 0.005454669]\n",
            "Batch:61\n",
            "d_loss_real:0.8876652717590332\n",
            "d_loss_fake:0.0012937580468133092\n",
            "d_loss_wrong:0.6850557923316956\n",
            "d_loss:0.6154200285673141\n",
            "g_loss:[0.33854473, 0.32852787, 0.0050084307]\n",
            "Batch:62\n",
            "d_loss_real:0.8972256183624268\n",
            "d_loss_fake:0.000938170007430017\n",
            "d_loss_wrong:0.6638430953025818\n",
            "d_loss:0.61480812728405\n",
            "g_loss:[0.38668427, 0.377837, 0.004423634]\n",
            "Batch:63\n",
            "d_loss_real:0.869733452796936\n",
            "d_loss_fake:0.0006251023150980473\n",
            "d_loss_wrong:0.6905624270439148\n",
            "d_loss:0.6076636016368866\n",
            "g_loss:[0.34595305, 0.3349349, 0.0055090752]\n",
            "Batch:64\n",
            "d_loss_real:0.8577226400375366\n",
            "d_loss_fake:0.0005893281195312738\n",
            "d_loss_wrong:0.6665608286857605\n",
            "d_loss:0.595648854970932\n",
            "g_loss:[0.35694313, 0.34701777, 0.0049626753]\n",
            "Batch:65\n",
            "d_loss_real:0.9010538458824158\n",
            "d_loss_fake:0.0004231795610394329\n",
            "d_loss_wrong:0.7208123207092285\n",
            "d_loss:0.6308358013629913\n",
            "g_loss:[0.35078517, 0.33823413, 0.006275513]\n",
            "Batch:66\n",
            "d_loss_real:0.8261767029762268\n",
            "d_loss_fake:0.0008950032060965896\n",
            "d_loss_wrong:0.6917053461074829\n",
            "d_loss:0.586238443851471\n",
            "g_loss:[0.3395057, 0.3307665, 0.0043696007]\n",
            "Batch:67\n",
            "d_loss_real:0.8964715003967285\n",
            "d_loss_fake:0.0009816940873861313\n",
            "d_loss_wrong:0.6508967280387878\n",
            "d_loss:0.6112053543329239\n",
            "g_loss:[0.3703018, 0.36269653, 0.003802637]\n",
            "Batch:68\n",
            "d_loss_real:0.8855836391448975\n",
            "d_loss_fake:0.0010491759749129415\n",
            "d_loss_wrong:0.7039345502853394\n",
            "d_loss:0.6190377473831177\n",
            "g_loss:[0.4262074, 0.41615903, 0.00502418]\n",
            "Batch:69\n",
            "d_loss_real:0.8978582620620728\n",
            "d_loss_fake:0.001056384528055787\n",
            "d_loss_wrong:0.7996367812156677\n",
            "d_loss:0.6491024196147919\n",
            "g_loss:[0.3728621, 0.3609109, 0.005975605]\n",
            "Batch:70\n",
            "d_loss_real:0.8183756470680237\n",
            "d_loss_fake:0.0013132841559126973\n",
            "d_loss_wrong:0.718524158000946\n",
            "d_loss:0.5891471803188324\n",
            "g_loss:[0.36591557, 0.3545673, 0.005674134]\n",
            "Batch:71\n",
            "d_loss_real:0.8705376386642456\n",
            "d_loss_fake:0.0010113146854564548\n",
            "d_loss_wrong:0.6449729800224304\n",
            "d_loss:0.5967648923397064\n",
            "g_loss:[0.39562538, 0.3850346, 0.005295389]\n",
            "Batch:72\n",
            "d_loss_real:0.8940868377685547\n",
            "d_loss_fake:0.0017566384049132466\n",
            "d_loss_wrong:0.6615942120552063\n",
            "d_loss:0.612881138920784\n",
            "g_loss:[0.3504795, 0.34168622, 0.004396649]\n",
            "Batch:73\n",
            "d_loss_real:0.8597347736358643\n",
            "d_loss_fake:0.0011184969916939735\n",
            "d_loss_wrong:0.6752573847770691\n",
            "d_loss:0.598961353302002\n",
            "g_loss:[0.3377074, 0.3302002, 0.0037535988]\n",
            "Batch:74\n",
            "d_loss_real:0.9324402809143066\n",
            "d_loss_fake:0.0017675539711490273\n",
            "d_loss_wrong:0.6822055578231812\n",
            "d_loss:0.6372134238481522\n",
            "g_loss:[0.3482501, 0.34136736, 0.0034413636]\n",
            "Batch:75\n",
            "d_loss_real:0.8845794200897217\n",
            "d_loss_fake:0.006543770432472229\n",
            "d_loss_wrong:0.6839753985404968\n",
            "d_loss:0.6149194985628128\n",
            "g_loss:[0.37246016, 0.3654334, 0.0035133855]\n",
            "Batch:76\n",
            "d_loss_real:0.8789912462234497\n",
            "d_loss_fake:0.004913263488560915\n",
            "d_loss_wrong:0.6534953713417053\n",
            "d_loss:0.6040977835655212\n",
            "g_loss:[0.4713049, 0.4641394, 0.0035827435]\n",
            "Batch:77\n",
            "d_loss_real:0.8565572500228882\n",
            "d_loss_fake:0.0028520768973976374\n",
            "d_loss_wrong:0.6956340074539185\n",
            "d_loss:0.6029001474380493\n",
            "g_loss:[0.44378433, 0.43700922, 0.0033875513]\n",
            "Batch:78\n",
            "d_loss_real:0.8508333563804626\n",
            "d_loss_fake:0.004693831782788038\n",
            "d_loss_wrong:0.6816685199737549\n",
            "d_loss:0.5970072597265244\n",
            "g_loss:[0.4561132, 0.44824976, 0.003931717]\n",
            "Batch:79\n",
            "d_loss_real:0.8602912425994873\n",
            "d_loss_fake:0.0012167789973318577\n",
            "d_loss_wrong:0.6637802124023438\n",
            "d_loss:0.5963948667049408\n",
            "g_loss:[0.4519407, 0.44440338, 0.0037686462]\n",
            "Batch:80\n",
            "d_loss_real:0.8551211953163147\n",
            "d_loss_fake:0.0007686630706302822\n",
            "d_loss_wrong:0.6678910255432129\n",
            "d_loss:0.5947255194187164\n",
            "g_loss:[0.4202556, 0.4121496, 0.0040530013]\n",
            "Batch:81\n",
            "d_loss_real:0.8565310835838318\n",
            "d_loss_fake:0.0009078243747353554\n",
            "d_loss_wrong:0.6424039602279663\n",
            "d_loss:0.589093491435051\n",
            "g_loss:[0.46390492, 0.45618108, 0.0038619125]\n",
            "Batch:82\n",
            "d_loss_real:0.9572270512580872\n",
            "d_loss_fake:0.0026625264436006546\n",
            "d_loss_wrong:0.6554430723190308\n",
            "d_loss:0.6431399285793304\n",
            "g_loss:[0.4029076, 0.39595965, 0.0034739785]\n",
            "Batch:83\n",
            "d_loss_real:0.7997922897338867\n",
            "d_loss_fake:0.021780844777822495\n",
            "d_loss_wrong:0.6685000061988831\n",
            "d_loss:0.5724663585424423\n",
            "g_loss:[1.0254153, 1.0168092, 0.0043030353]\n",
            "Batch:84\n",
            "d_loss_real:0.8670246601104736\n",
            "d_loss_fake:0.002386833541095257\n",
            "d_loss_wrong:0.6860551238059998\n",
            "d_loss:0.6056228131055832\n",
            "g_loss:[0.81030077, 0.8031849, 0.0035579144]\n",
            "Batch:85\n",
            "d_loss_real:0.8531004190444946\n",
            "d_loss_fake:0.03568361699581146\n",
            "d_loss_wrong:0.6641804575920105\n",
            "d_loss:0.6015162318944931\n",
            "g_loss:[1.8492312, 1.8412554, 0.003987916]\n",
            "Batch:86\n",
            "d_loss_real:0.8789749145507812\n",
            "d_loss_fake:0.08617639541625977\n",
            "d_loss_wrong:0.5904178023338318\n",
            "d_loss:0.6086360067129135\n",
            "g_loss:[4.6177645, 4.611045, 0.0033597695]\n",
            "Batch:87\n",
            "d_loss_real:1.067285180091858\n",
            "d_loss_fake:0.14698731899261475\n",
            "d_loss_wrong:0.636194109916687\n",
            "d_loss:0.7294379472732544\n",
            "g_loss:[6.2233267, 6.215454, 0.003936244]\n",
            "Batch:88\n",
            "d_loss_real:0.974695086479187\n",
            "d_loss_fake:0.004456905648112297\n",
            "d_loss_wrong:0.6321982741355896\n",
            "d_loss:0.6465113312005997\n",
            "g_loss:[4.365, 4.356863, 0.0040684766]\n",
            "Batch:89\n",
            "d_loss_real:0.8653258085250854\n",
            "d_loss_fake:0.08622008562088013\n",
            "d_loss_wrong:0.7127318978309631\n",
            "d_loss:0.6324009001255035\n",
            "g_loss:[3.1611407, 3.1487021, 0.0062193116]\n",
            "Batch:90\n",
            "d_loss_real:0.9886743426322937\n",
            "d_loss_fake:0.0714075118303299\n",
            "d_loss_wrong:0.6022443175315857\n",
            "d_loss:0.6627501249313354\n",
            "g_loss:[4.1255364, 4.1155396, 0.00499838]\n",
            "Batch:91\n",
            "d_loss_real:0.9874697923660278\n",
            "d_loss_fake:0.04192424938082695\n",
            "d_loss_wrong:0.6234660148620605\n",
            "d_loss:0.6600824594497681\n",
            "g_loss:[3.7793782, 3.7693381, 0.00502003]\n",
            "Batch:92\n",
            "d_loss_real:1.0339618921279907\n",
            "d_loss_fake:0.05843338742852211\n",
            "d_loss_wrong:0.725718080997467\n",
            "d_loss:0.7130188196897507\n",
            "g_loss:[3.8055725, 3.7903743, 0.0075990874]\n",
            "Batch:93\n",
            "d_loss_real:0.9376877546310425\n",
            "d_loss_fake:0.12180107831954956\n",
            "d_loss_wrong:0.6188681125640869\n",
            "d_loss:0.6540111750364304\n",
            "g_loss:[3.7848122, 3.771503, 0.0066545787]\n",
            "Batch:94\n",
            "d_loss_real:0.9011298418045044\n",
            "d_loss_fake:0.16336794197559357\n",
            "d_loss_wrong:0.6271636486053467\n",
            "d_loss:0.648197814822197\n",
            "g_loss:[2.3081515, 2.2964406, 0.0058553824]\n",
            "Batch:95\n",
            "d_loss_real:1.0655620098114014\n",
            "d_loss_fake:0.019837331026792526\n",
            "d_loss_wrong:0.5747189521789551\n",
            "d_loss:0.6814200729131699\n",
            "g_loss:[2.188725, 2.1779761, 0.005374476]\n",
            "Batch:96\n",
            "d_loss_real:0.8876731395721436\n",
            "d_loss_fake:0.01317810732871294\n",
            "d_loss_wrong:0.6412490606307983\n",
            "d_loss:0.6074433624744415\n",
            "g_loss:[2.3112419, 2.297171, 0.0070354077]\n",
            "Batch:97\n",
            "d_loss_real:1.009542465209961\n",
            "d_loss_fake:0.01622159592807293\n",
            "d_loss_wrong:0.5968480110168457\n",
            "d_loss:0.6580386310815811\n",
            "g_loss:[1.9248236, 1.9107897, 0.007016932]\n",
            "Batch:98\n",
            "d_loss_real:0.9586843252182007\n",
            "d_loss_fake:0.01304292306303978\n",
            "d_loss_wrong:0.6782224774360657\n",
            "d_loss:0.6521585136651993\n",
            "g_loss:[1.3725958, 1.3592645, 0.0066656475]\n",
            "Batch:99\n",
            "d_loss_real:0.9322797060012817\n",
            "d_loss_fake:0.02485598996281624\n",
            "d_loss_wrong:0.6246818900108337\n",
            "d_loss:0.6285243183374405\n",
            "g_loss:[1.5339092, 1.5189662, 0.0074715046]\n",
            "Batch:100\n",
            "d_loss_real:0.8574305772781372\n",
            "d_loss_fake:0.011538202874362469\n",
            "d_loss_wrong:0.6578313708305359\n",
            "d_loss:0.5960576832294464\n",
            "g_loss:[1.3768489, 1.3621578, 0.0073455386]\n",
            "Batch:101\n",
            "d_loss_real:0.9130826592445374\n",
            "d_loss_fake:0.010534480214118958\n",
            "d_loss_wrong:0.6829797625541687\n",
            "d_loss:0.6299198865890503\n",
            "g_loss:[1.726687, 1.7165799, 0.005053521]\n",
            "Batch:102\n",
            "d_loss_real:0.9600685238838196\n",
            "d_loss_fake:0.014196345582604408\n",
            "d_loss_wrong:0.6561024188995361\n",
            "d_loss:0.6476089507341385\n",
            "g_loss:[1.3844888, 1.3736484, 0.0054201907]\n",
            "Batch:103\n",
            "d_loss_real:0.8780732154846191\n",
            "d_loss_fake:0.08308908343315125\n",
            "d_loss_wrong:0.6340351700782776\n",
            "d_loss:0.6183176636695862\n",
            "g_loss:[1.5174376, 1.5066918, 0.005372879]\n",
            "Batch:104\n",
            "d_loss_real:1.0089137554168701\n",
            "d_loss_fake:0.008154407143592834\n",
            "d_loss_wrong:0.6351076364517212\n",
            "d_loss:0.6652723848819733\n",
            "g_loss:[1.5365, 1.5281765, 0.0041617085]\n",
            "Batch:105\n",
            "d_loss_real:1.0377355813980103\n",
            "d_loss_fake:0.028039157390594482\n",
            "d_loss_wrong:0.6542759537696838\n",
            "d_loss:0.6894465684890747\n",
            "g_loss:[1.3619947, 1.3539021, 0.0040463274]\n",
            "Batch:106\n",
            "d_loss_real:0.8309363126754761\n",
            "d_loss_fake:0.09229852259159088\n",
            "d_loss_wrong:0.6616937518119812\n",
            "d_loss:0.6039662212133408\n",
            "g_loss:[1.277739, 1.2695029, 0.0041180705]\n",
            "Batch:107\n",
            "d_loss_real:0.9452386498451233\n",
            "d_loss_fake:0.1026311069726944\n",
            "d_loss_wrong:0.6250991225242615\n",
            "d_loss:0.6545518785715103\n",
            "g_loss:[1.338918, 1.3286941, 0.005111933]\n",
            "Batch:108\n",
            "d_loss_real:0.9738636016845703\n",
            "d_loss_fake:0.10462185740470886\n",
            "d_loss_wrong:0.6023958921432495\n",
            "d_loss:0.6636862456798553\n",
            "g_loss:[1.6304718, 1.6186688, 0.0059015397]\n",
            "Batch:109\n",
            "d_loss_real:1.0442531108856201\n",
            "d_loss_fake:0.18149520456790924\n",
            "d_loss_wrong:0.5349498987197876\n",
            "d_loss:0.701237827539444\n",
            "g_loss:[1.6735871, 1.6635568, 0.005015107]\n",
            "Batch:110\n",
            "d_loss_real:1.103925347328186\n",
            "d_loss_fake:0.20564617216587067\n",
            "d_loss_wrong:0.5115931630134583\n",
            "d_loss:0.731272503733635\n",
            "g_loss:[1.8966283, 1.8845341, 0.006047098]\n",
            "Batch:111\n",
            "d_loss_real:1.0899473428726196\n",
            "d_loss_fake:0.13469994068145752\n",
            "d_loss_wrong:0.5474992394447327\n",
            "d_loss:0.7155234664678574\n",
            "g_loss:[2.4712965, 2.4618726, 0.004712005]\n",
            "Batch:112\n",
            "d_loss_real:1.0254778861999512\n",
            "d_loss_fake:0.1765928864479065\n",
            "d_loss_wrong:0.6055859923362732\n",
            "d_loss:0.7082836627960205\n",
            "g_loss:[2.6113548, 2.5997562, 0.005799303]\n",
            "Batch:113\n",
            "d_loss_real:1.0852727890014648\n",
            "d_loss_fake:0.15154114365577698\n",
            "d_loss_wrong:0.5459264516830444\n",
            "d_loss:0.7170032858848572\n",
            "g_loss:[2.5641644, 2.5541933, 0.004985564]\n",
            "Batch:114\n",
            "d_loss_real:1.0616530179977417\n",
            "d_loss_fake:0.14437565207481384\n",
            "d_loss_wrong:0.5302433967590332\n",
            "d_loss:0.6994812786579132\n",
            "g_loss:[2.578, 2.5689206, 0.0045397338]\n",
            "Batch:115\n",
            "d_loss_real:1.0619730949401855\n",
            "d_loss_fake:0.10271452367305756\n",
            "d_loss_wrong:0.5610010623931885\n",
            "d_loss:0.6969154477119446\n",
            "g_loss:[2.9850352, 2.9762292, 0.004402969]\n",
            "Batch:116\n",
            "d_loss_real:1.1033995151519775\n",
            "d_loss_fake:0.076362743973732\n",
            "d_loss_wrong:0.6036147475242615\n",
            "d_loss:0.7216941267251968\n",
            "g_loss:[2.667819, 2.6596587, 0.0040801982]\n",
            "Batch:117\n",
            "d_loss_real:1.0127642154693604\n",
            "d_loss_fake:0.15657854080200195\n",
            "d_loss_wrong:0.6193022131919861\n",
            "d_loss:0.7003522962331772\n",
            "g_loss:[2.645423, 2.6372662, 0.0040783687]\n",
            "Batch:118\n",
            "d_loss_real:0.9814351201057434\n",
            "d_loss_fake:0.09770862758159637\n",
            "d_loss_wrong:0.6061802506446838\n",
            "d_loss:0.6666897833347321\n",
            "g_loss:[2.7677753, 2.761547, 0.0031140782]\n",
            "Batch:119\n",
            "d_loss_real:0.9946935772895813\n",
            "d_loss_fake:0.15072475373744965\n",
            "d_loss_wrong:0.5898110270500183\n",
            "d_loss:0.6824807375669479\n",
            "g_loss:[2.490037, 2.4840565, 0.0029902572]\n",
            "Batch:120\n",
            "d_loss_real:1.1198129653930664\n",
            "d_loss_fake:0.20277145504951477\n",
            "d_loss_wrong:0.5362330675125122\n",
            "d_loss:0.7446576058864594\n",
            "g_loss:[3.1524372, 3.1451514, 0.0036428894]\n",
            "Batch:121\n",
            "d_loss_real:1.2524528503417969\n",
            "d_loss_fake:0.27265292406082153\n",
            "d_loss_wrong:0.6148988008499146\n",
            "d_loss:0.8481143563985825\n",
            "g_loss:[2.5060892, 2.4994714, 0.0033089225]\n",
            "Batch:122\n",
            "d_loss_real:1.035710334777832\n",
            "d_loss_fake:0.25003474950790405\n",
            "d_loss_wrong:0.5396742820739746\n",
            "d_loss:0.7152824252843857\n",
            "g_loss:[3.0122032, 3.0045304, 0.0038363698]\n",
            "Batch:123\n",
            "d_loss_real:1.239192008972168\n",
            "d_loss_fake:0.05630561709403992\n",
            "d_loss_wrong:0.6399257779121399\n",
            "d_loss:0.7936538457870483\n",
            "g_loss:[3.4938889, 3.486478, 0.0037053837]\n",
            "Batch:124\n",
            "d_loss_real:1.1850794553756714\n",
            "d_loss_fake:0.0746549516916275\n",
            "d_loss_wrong:0.533499002456665\n",
            "d_loss:0.7445782124996185\n",
            "g_loss:[2.682955, 2.6760347, 0.0034601348]\n",
            "Batch:125\n",
            "d_loss_real:0.9081132411956787\n",
            "d_loss_fake:0.059923797845840454\n",
            "d_loss_wrong:0.6620920896530151\n",
            "d_loss:0.6345605850219727\n",
            "g_loss:[2.7674146, 2.7582529, 0.0045809075]\n",
            "Batch:126\n",
            "d_loss_real:0.9558882713317871\n",
            "d_loss_fake:0.07185690104961395\n",
            "d_loss_wrong:0.6427116394042969\n",
            "d_loss:0.6565862745046616\n",
            "g_loss:[2.449158, 2.4407032, 0.0042274245]\n",
            "Batch:127\n",
            "d_loss_real:1.1028468608856201\n",
            "d_loss_fake:0.05178554728627205\n",
            "d_loss_wrong:0.598786473274231\n",
            "d_loss:0.7140664309263229\n",
            "g_loss:[2.197544, 2.1892922, 0.004125949]\n",
            "Batch:128\n",
            "d_loss_real:1.1089093685150146\n",
            "d_loss_fake:0.10663434863090515\n",
            "d_loss_wrong:0.6227110028266907\n",
            "d_loss:0.7367910146713257\n",
            "g_loss:[2.0652544, 2.0482914, 0.008481512]\n",
            "Batch:129\n",
            "d_loss_real:1.0449005365371704\n",
            "d_loss_fake:0.16762274503707886\n",
            "d_loss_wrong:0.555249035358429\n",
            "d_loss:0.7031682133674622\n",
            "g_loss:[1.9876759, 1.9736936, 0.0069911773]\n",
            "Batch:130\n",
            "d_loss_real:1.267950177192688\n",
            "d_loss_fake:0.2345874011516571\n",
            "d_loss_wrong:0.543197751045227\n",
            "d_loss:0.8284213840961456\n",
            "g_loss:[1.9548154, 1.9449146, 0.0049504316]\n",
            "Batch:131\n",
            "d_loss_real:1.174318552017212\n",
            "d_loss_fake:0.1312912106513977\n",
            "d_loss_wrong:0.5671055316925049\n",
            "d_loss:0.7617584615945816\n",
            "g_loss:[2.0313778, 2.0168533, 0.007262177]\n",
            "Batch:132\n",
            "d_loss_real:1.0360395908355713\n",
            "d_loss_fake:0.16397599875926971\n",
            "d_loss_wrong:0.5891309976577759\n",
            "d_loss:0.7062965482473373\n",
            "g_loss:[2.767041, 2.7550635, 0.005988736]\n",
            "Batch:133\n",
            "d_loss_real:1.1917128562927246\n",
            "d_loss_fake:0.10121375322341919\n",
            "d_loss_wrong:0.5626847743988037\n",
            "d_loss:0.761831060051918\n",
            "g_loss:[2.0069106, 1.9985363, 0.0041871113]\n",
            "Batch:134\n",
            "d_loss_real:0.9992018938064575\n",
            "d_loss_fake:0.26907065510749817\n",
            "d_loss_wrong:0.5459514856338501\n",
            "d_loss:0.7033564746379852\n",
            "g_loss:[2.1906333, 2.180945, 0.0048441933]\n",
            "Batch:135\n",
            "d_loss_real:1.1564369201660156\n",
            "d_loss_fake:0.29571184515953064\n",
            "d_loss_wrong:0.5384798645973206\n",
            "d_loss:0.78676638007164\n",
            "g_loss:[1.6751806, 1.6676252, 0.0037776812]\n",
            "Batch:136\n",
            "d_loss_real:1.1810307502746582\n",
            "d_loss_fake:0.284528523683548\n",
            "d_loss_wrong:0.5069018006324768\n",
            "d_loss:0.7883729636669159\n",
            "g_loss:[2.0554593, 2.0480082, 0.0037254752]\n",
            "Batch:137\n",
            "d_loss_real:1.2070177793502808\n",
            "d_loss_fake:0.3411625325679779\n",
            "d_loss_wrong:0.5377720594406128\n",
            "d_loss:0.8232425451278687\n",
            "g_loss:[1.5721626, 1.56443, 0.0038662876]\n",
            "Batch:138\n",
            "d_loss_real:1.1051318645477295\n",
            "d_loss_fake:0.27787959575653076\n",
            "d_loss_wrong:0.5185439586639404\n",
            "d_loss:0.7516718208789825\n",
            "g_loss:[1.9866396, 1.9774382, 0.0046006776]\n",
            "========================================\n",
            "Epoch is: 2\n",
            "Number of batches 138\n",
            "Batch:1\n",
            "d_loss_real:1.249502420425415\n",
            "d_loss_fake:0.17589274048805237\n",
            "d_loss_wrong:0.5345368385314941\n",
            "d_loss:0.8023585975170135\n",
            "g_loss:[1.9968373, 1.989005, 0.0039161383]\n",
            "Batch:2\n",
            "d_loss_real:1.1408203840255737\n",
            "d_loss_fake:0.14615210890769958\n",
            "d_loss_wrong:0.5655240416526794\n",
            "d_loss:0.748329222202301\n",
            "g_loss:[2.3574624, 2.3470254, 0.0052185436]\n",
            "Batch:3\n",
            "d_loss_real:1.0640299320220947\n",
            "d_loss_fake:0.17651790380477905\n",
            "d_loss_wrong:0.5863681435585022\n",
            "d_loss:0.7227364778518677\n",
            "g_loss:[3.3061104, 3.2950654, 0.0055225417]\n",
            "Batch:4\n",
            "d_loss_real:1.2224557399749756\n",
            "d_loss_fake:0.14017146825790405\n",
            "d_loss_wrong:0.7247921228408813\n",
            "d_loss:0.8274687677621841\n",
            "g_loss:[1.9437522, 1.931123, 0.0063145794]\n",
            "Batch:5\n",
            "d_loss_real:0.9950060844421387\n",
            "d_loss_fake:0.22441036999225616\n",
            "d_loss_wrong:0.6908510327339172\n",
            "d_loss:0.7263183891773224\n",
            "g_loss:[2.4981925, 2.4866014, 0.00579565]\n",
            "Batch:6\n",
            "d_loss_real:1.118518352508545\n",
            "d_loss_fake:0.2534767985343933\n",
            "d_loss_wrong:0.6784607172012329\n",
            "d_loss:0.792243555188179\n",
            "g_loss:[2.1831956, 2.1726956, 0.0052499315]\n",
            "Batch:7\n",
            "d_loss_real:1.129638671875\n",
            "d_loss_fake:0.14844352006912231\n",
            "d_loss_wrong:0.566388726234436\n",
            "d_loss:0.7435273975133896\n",
            "g_loss:[2.1728017, 2.1621704, 0.005315628]\n",
            "Batch:8\n",
            "d_loss_real:1.2375764846801758\n",
            "d_loss_fake:0.12427428364753723\n",
            "d_loss_wrong:0.45340418815612793\n",
            "d_loss:0.7632078528404236\n",
            "g_loss:[2.328878, 2.317179, 0.005849512]\n",
            "Batch:9\n",
            "d_loss_real:1.2316125631332397\n",
            "d_loss_fake:0.09841033071279526\n",
            "d_loss_wrong:0.5265325903892517\n",
            "d_loss:0.7720420062541962\n",
            "g_loss:[2.0270529, 2.0084422, 0.009305312]\n",
            "Batch:10\n",
            "d_loss_real:1.1425988674163818\n",
            "d_loss_fake:0.13458582758903503\n",
            "d_loss_wrong:0.5052019357681274\n",
            "d_loss:0.7312463819980621\n",
            "g_loss:[2.043761, 2.0198526, 0.011954134]\n",
            "Batch:11\n",
            "d_loss_real:1.1911925077438354\n",
            "d_loss_fake:0.14973649382591248\n",
            "d_loss_wrong:0.5396711826324463\n",
            "d_loss:0.767948180437088\n",
            "g_loss:[2.1525166, 2.1403904, 0.0060630664]\n",
            "Batch:12\n",
            "d_loss_real:1.1938073635101318\n",
            "d_loss_fake:0.12176450341939926\n",
            "d_loss_wrong:0.5298469066619873\n",
            "d_loss:0.7598065286874771\n",
            "g_loss:[1.4022236, 1.3907262, 0.005748678]\n",
            "Batch:13\n",
            "d_loss_real:1.122572898864746\n",
            "d_loss_fake:0.23256132006645203\n",
            "d_loss_wrong:0.5306950211524963\n",
            "d_loss:0.7521005272865295\n",
            "g_loss:[1.4074913, 1.39608, 0.005705669]\n",
            "Batch:14\n",
            "d_loss_real:1.2757773399353027\n",
            "d_loss_fake:0.2921386957168579\n",
            "d_loss_wrong:0.5096065998077393\n",
            "d_loss:0.8383249938488007\n",
            "g_loss:[1.6496937, 1.6410427, 0.0043255175]\n",
            "Batch:15\n",
            "d_loss_real:1.2470755577087402\n",
            "d_loss_fake:0.33349916338920593\n",
            "d_loss_wrong:0.5099131464958191\n",
            "d_loss:0.8343908488750458\n",
            "g_loss:[1.4305412, 1.4166191, 0.0069610653]\n",
            "Batch:16\n",
            "d_loss_real:1.1451807022094727\n",
            "d_loss_fake:0.16678783297538757\n",
            "d_loss_wrong:0.6132417917251587\n",
            "d_loss:0.7675977647304535\n",
            "g_loss:[2.0619917, 2.0465367, 0.0077274456]\n",
            "Batch:17\n",
            "d_loss_real:1.1118299961090088\n",
            "d_loss_fake:0.20263181626796722\n",
            "d_loss_wrong:0.9272202849388123\n",
            "d_loss:0.8383780121803284\n",
            "g_loss:[1.9531888, 1.9405735, 0.0063076857]\n",
            "Batch:18\n",
            "d_loss_real:1.14826500415802\n",
            "d_loss_fake:0.19906701147556305\n",
            "d_loss_wrong:0.46923384070396423\n",
            "d_loss:0.7412077188491821\n",
            "g_loss:[1.8045406, 1.7952884, 0.0046261107]\n",
            "Batch:19\n",
            "d_loss_real:1.298689842224121\n",
            "d_loss_fake:0.10297398269176483\n",
            "d_loss_wrong:0.5626498460769653\n",
            "d_loss:0.8157508820295334\n",
            "g_loss:[1.7524303, 1.7431371, 0.0046466286]\n",
            "Batch:20\n",
            "d_loss_real:1.2054104804992676\n",
            "d_loss_fake:0.15916834771633148\n",
            "d_loss_wrong:0.626689612865448\n",
            "d_loss:0.799169734120369\n",
            "g_loss:[1.8553278, 1.8452008, 0.005063558]\n",
            "Batch:21\n",
            "d_loss_real:1.2455593347549438\n",
            "d_loss_fake:0.12477559596300125\n",
            "d_loss_wrong:0.6196959018707275\n",
            "d_loss:0.808897539973259\n",
            "g_loss:[1.4737083, 1.4651921, 0.004258112]\n",
            "Batch:22\n",
            "d_loss_real:1.0169540643692017\n",
            "d_loss_fake:0.18573354184627533\n",
            "d_loss_wrong:0.6131757497787476\n",
            "d_loss:0.7082043588161469\n",
            "g_loss:[1.6521579, 1.6447452, 0.0037063407]\n",
            "Batch:23\n",
            "d_loss_real:1.0829081535339355\n",
            "d_loss_fake:0.1834820955991745\n",
            "d_loss_wrong:0.5573009848594666\n",
            "d_loss:0.7266498506069183\n",
            "g_loss:[1.9944899, 1.9865338, 0.0039780745]\n",
            "Batch:24\n",
            "d_loss_real:1.1013730764389038\n",
            "d_loss_fake:0.12266653031110764\n",
            "d_loss_wrong:0.7333308458328247\n",
            "d_loss:0.7646858841180801\n",
            "g_loss:[1.3799496, 1.3667072, 0.0066212113]\n",
            "Batch:25\n",
            "d_loss_real:1.1576206684112549\n",
            "d_loss_fake:0.2952101528644562\n",
            "d_loss_wrong:0.5853740572929382\n",
            "d_loss:0.7989563941955566\n",
            "g_loss:[1.5392761, 1.5266578, 0.006309158]\n",
            "Batch:26\n",
            "d_loss_real:1.1283247470855713\n",
            "d_loss_fake:0.146962970495224\n",
            "d_loss_wrong:0.5600668787956238\n",
            "d_loss:0.740919828414917\n",
            "g_loss:[1.4859874, 1.4780021, 0.00399269]\n",
            "Batch:27\n",
            "d_loss_real:1.193025827407837\n",
            "d_loss_fake:0.14905793964862823\n",
            "d_loss_wrong:0.6803430318832397\n",
            "d_loss:0.8038631528615952\n",
            "g_loss:[1.7957453, 1.7869086, 0.0044183224]\n",
            "Batch:28\n",
            "d_loss_real:1.025443434715271\n",
            "d_loss_fake:0.13964106142520905\n",
            "d_loss_wrong:0.59765625\n",
            "d_loss:0.6970460414886475\n",
            "g_loss:[1.740508, 1.7288845, 0.005811753]\n",
            "Batch:29\n",
            "d_loss_real:1.0741395950317383\n",
            "d_loss_fake:0.17536817491054535\n",
            "d_loss_wrong:0.5564414262771606\n",
            "d_loss:0.7200222015380859\n",
            "g_loss:[1.5802236, 1.5702853, 0.0049690944]\n",
            "Batch:30\n",
            "d_loss_real:1.1844422817230225\n",
            "d_loss_fake:0.08198091387748718\n",
            "d_loss_wrong:0.6776617169380188\n",
            "d_loss:0.7821317911148071\n",
            "g_loss:[1.673875, 1.6656805, 0.00409723]\n",
            "Batch:31\n",
            "d_loss_real:0.9347777366638184\n",
            "d_loss_fake:0.09886211156845093\n",
            "d_loss_wrong:0.635752260684967\n",
            "d_loss:0.6510424613952637\n",
            "g_loss:[1.4289954, 1.4201897, 0.004402821]\n",
            "Batch:32\n",
            "d_loss_real:1.0861132144927979\n",
            "d_loss_fake:0.0628151223063469\n",
            "d_loss_wrong:0.571414053440094\n",
            "d_loss:0.7016139030456543\n",
            "g_loss:[1.2699347, 1.2603326, 0.004801025]\n",
            "Batch:33\n",
            "d_loss_real:0.985369861125946\n",
            "d_loss_fake:0.1964327096939087\n",
            "d_loss_wrong:0.5699257850646973\n",
            "d_loss:0.6842745542526245\n",
            "g_loss:[1.6414558, 1.6304536, 0.005501073]\n",
            "Batch:34\n",
            "d_loss_real:1.3240389823913574\n",
            "d_loss_fake:0.0930156335234642\n",
            "d_loss_wrong:0.5548779368400574\n",
            "d_loss:0.8239928781986237\n",
            "g_loss:[1.2508045, 1.237524, 0.0066402527]\n",
            "Batch:35\n",
            "d_loss_real:1.110805630683899\n",
            "d_loss_fake:0.11536258459091187\n",
            "d_loss_wrong:0.6361397504806519\n",
            "d_loss:0.7432783991098404\n",
            "g_loss:[1.001087, 0.9882405, 0.0064232517]\n",
            "Batch:36\n",
            "d_loss_real:0.9461056590080261\n",
            "d_loss_fake:0.18984851241111755\n",
            "d_loss_wrong:0.5828988552093506\n",
            "d_loss:0.6662396788597107\n",
            "g_loss:[1.373591, 1.363428, 0.0050814883]\n",
            "Batch:37\n",
            "d_loss_real:1.093911051750183\n",
            "d_loss_fake:0.0661337673664093\n",
            "d_loss_wrong:0.6098209023475647\n",
            "d_loss:0.7159442007541656\n",
            "g_loss:[1.187889, 1.1793478, 0.004270593]\n",
            "Batch:38\n",
            "d_loss_real:0.9605535268783569\n",
            "d_loss_fake:0.08071836084127426\n",
            "d_loss_wrong:0.670163094997406\n",
            "d_loss:0.6679971218109131\n",
            "g_loss:[1.2283801, 1.2196476, 0.004366191]\n",
            "Batch:39\n",
            "d_loss_real:1.0370149612426758\n",
            "d_loss_fake:0.07451517879962921\n",
            "d_loss_wrong:0.6182702779769897\n",
            "d_loss:0.6917038410902023\n",
            "g_loss:[1.1631184, 1.1551142, 0.0040020924]\n",
            "Batch:40\n",
            "d_loss_real:1.1208670139312744\n",
            "d_loss_fake:0.15330109000205994\n",
            "d_loss_wrong:0.7041475176811218\n",
            "d_loss:0.774795651435852\n",
            "g_loss:[1.2711934, 1.2638489, 0.003672223]\n",
            "Batch:41\n",
            "d_loss_real:1.0255603790283203\n",
            "d_loss_fake:0.07037993520498276\n",
            "d_loss_wrong:0.6009042263031006\n",
            "d_loss:0.6806012243032455\n",
            "g_loss:[1.4762187, 1.4676766, 0.00427106]\n",
            "Batch:42\n",
            "d_loss_real:1.0424516201019287\n",
            "d_loss_fake:0.09823474287986755\n",
            "d_loss_wrong:0.6450880169868469\n",
            "d_loss:0.7070564925670624\n",
            "g_loss:[1.4711242, 1.4621427, 0.0044907555]\n",
            "Batch:43\n",
            "d_loss_real:1.2378807067871094\n",
            "d_loss_fake:0.07767783105373383\n",
            "d_loss_wrong:0.6060185432434082\n",
            "d_loss:0.7898644506931305\n",
            "g_loss:[1.1597018, 1.149026, 0.005337921]\n",
            "Batch:44\n",
            "d_loss_real:0.9512549638748169\n",
            "d_loss_fake:0.14122934639453888\n",
            "d_loss_wrong:0.6366222500801086\n",
            "d_loss:0.67009037733078\n",
            "g_loss:[1.2356572, 1.2269281, 0.004364538]\n",
            "Batch:45\n",
            "d_loss_real:1.1526496410369873\n",
            "d_loss_fake:0.12477805465459824\n",
            "d_loss_wrong:0.5201510190963745\n",
            "d_loss:0.7375570833683014\n",
            "g_loss:[1.2763966, 1.2674874, 0.0044546286]\n",
            "Batch:46\n",
            "d_loss_real:0.8942644596099854\n",
            "d_loss_fake:0.11418614536523819\n",
            "d_loss_wrong:0.6271564364433289\n",
            "d_loss:0.6324678808450699\n",
            "g_loss:[1.5899928, 1.5810442, 0.0044742906]\n",
            "Batch:47\n",
            "d_loss_real:0.9626541137695312\n",
            "d_loss_fake:0.0457112081348896\n",
            "d_loss_wrong:0.6516611576080322\n",
            "d_loss:0.6556701511144638\n",
            "g_loss:[1.569837, 1.5620404, 0.0038982565]\n",
            "Batch:48\n",
            "d_loss_real:1.1792248487472534\n",
            "d_loss_fake:0.14112228155136108\n",
            "d_loss_wrong:0.6760836243629456\n",
            "d_loss:0.7939139008522034\n",
            "g_loss:[1.2702265, 1.2603133, 0.0049565816]\n",
            "Batch:49\n",
            "d_loss_real:1.0645638704299927\n",
            "d_loss_fake:0.2026176154613495\n",
            "d_loss_wrong:0.5739722847938538\n",
            "d_loss:0.7264294028282166\n",
            "g_loss:[1.2732745, 1.2633629, 0.0049558347]\n",
            "Batch:50\n",
            "d_loss_real:1.0514532327651978\n",
            "d_loss_fake:0.16858574748039246\n",
            "d_loss_wrong:0.5675788521766663\n",
            "d_loss:0.709767758846283\n",
            "g_loss:[1.4174207, 1.4094973, 0.003961741]\n",
            "Batch:51\n",
            "d_loss_real:0.9507274627685547\n",
            "d_loss_fake:0.11302751302719116\n",
            "d_loss_wrong:0.6067819595336914\n",
            "d_loss:0.655316099524498\n",
            "g_loss:[2.044248, 2.0368452, 0.003701437]\n",
            "Batch:52\n",
            "d_loss_real:0.9995992183685303\n",
            "d_loss_fake:0.132513165473938\n",
            "d_loss_wrong:0.6403095126152039\n",
            "d_loss:0.6930052787065506\n",
            "g_loss:[1.9958113, 1.990787, 0.002512132]\n",
            "Batch:53\n",
            "d_loss_real:1.279285192489624\n",
            "d_loss_fake:0.1372799426317215\n",
            "d_loss_wrong:0.6492855548858643\n",
            "d_loss:0.8362839668989182\n",
            "g_loss:[1.4798193, 1.4713197, 0.0042498265]\n",
            "Batch:54\n",
            "d_loss_real:0.8770588636398315\n",
            "d_loss_fake:0.28937122225761414\n",
            "d_loss_wrong:0.6503482460975647\n",
            "d_loss:0.6734592914581299\n",
            "g_loss:[1.9234232, 1.9155964, 0.003913412]\n",
            "Batch:55\n",
            "d_loss_real:1.090670108795166\n",
            "d_loss_fake:0.15143315494060516\n",
            "d_loss_wrong:0.565845787525177\n",
            "d_loss:0.7246547937393188\n",
            "g_loss:[1.690694, 1.68342, 0.0036370426]\n",
            "Batch:56\n",
            "d_loss_real:1.0857478380203247\n",
            "d_loss_fake:0.19437313079833984\n",
            "d_loss_wrong:0.7021505236625671\n",
            "d_loss:0.7670048326253891\n",
            "g_loss:[1.5142746, 1.5076724, 0.003301059]\n",
            "Batch:57\n",
            "d_loss_real:1.118013858795166\n",
            "d_loss_fake:0.1858290433883667\n",
            "d_loss_wrong:0.5625558495521545\n",
            "d_loss:0.7461031526327133\n",
            "g_loss:[1.9106946, 1.9018046, 0.004445035]\n",
            "Batch:58\n",
            "d_loss_real:1.088447093963623\n",
            "d_loss_fake:0.12308626621961594\n",
            "d_loss_wrong:0.5460399985313416\n",
            "d_loss:0.711505115032196\n",
            "g_loss:[1.657886, 1.6497312, 0.004077429]\n",
            "Batch:59\n",
            "d_loss_real:1.3506046533584595\n",
            "d_loss_fake:0.2293103188276291\n",
            "d_loss_wrong:0.5964864492416382\n",
            "d_loss:0.8817515224218369\n",
            "g_loss:[1.3298657, 1.3222562, 0.003804767]\n",
            "Batch:60\n",
            "d_loss_real:1.0803487300872803\n",
            "d_loss_fake:0.27245551347732544\n",
            "d_loss_wrong:0.5249510407447815\n",
            "d_loss:0.7395260035991669\n",
            "g_loss:[1.6663418, 1.6588874, 0.0037272053]\n",
            "Batch:61\n",
            "d_loss_real:1.1766639947891235\n",
            "d_loss_fake:0.10632123053073883\n",
            "d_loss_wrong:0.5190284252166748\n",
            "d_loss:0.7446694076061249\n",
            "g_loss:[1.8990226, 1.8925514, 0.003235584]\n",
            "Batch:62\n",
            "d_loss_real:1.1624984741210938\n",
            "d_loss_fake:0.15977367758750916\n",
            "d_loss_wrong:0.5573750138282776\n",
            "d_loss:0.760536402463913\n",
            "g_loss:[2.1119633, 2.1062317, 0.0028657864]\n",
            "Batch:63\n",
            "d_loss_real:1.1930317878723145\n",
            "d_loss_fake:0.2894172668457031\n",
            "d_loss_wrong:0.4954901337623596\n",
            "d_loss:0.7927427440881729\n",
            "g_loss:[2.1916304, 2.1839757, 0.0038273137]\n",
            "Batch:64\n",
            "d_loss_real:1.1776918172836304\n",
            "d_loss_fake:0.22360259294509888\n",
            "d_loss_wrong:0.5206915736198425\n",
            "d_loss:0.7749194502830505\n",
            "g_loss:[2.5388224, 2.5319388, 0.0034417803]\n",
            "Batch:65\n",
            "d_loss_real:1.2716625928878784\n",
            "d_loss_fake:0.20761847496032715\n",
            "d_loss_wrong:0.6304301619529724\n",
            "d_loss:0.8453434556722641\n",
            "g_loss:[2.4264257, 2.4188797, 0.003772946]\n",
            "Batch:66\n",
            "d_loss_real:1.0593600273132324\n",
            "d_loss_fake:0.08585622161626816\n",
            "d_loss_wrong:0.610209047794342\n",
            "d_loss:0.7036963254213333\n",
            "g_loss:[2.6458232, 2.6399455, 0.0029389085]\n",
            "Batch:67\n",
            "d_loss_real:1.1369508504867554\n",
            "d_loss_fake:0.045166295021772385\n",
            "d_loss_wrong:0.6485015749931335\n",
            "d_loss:0.741892397403717\n",
            "g_loss:[2.255597, 2.2504396, 0.0025787186]\n",
            "Batch:68\n",
            "d_loss_real:0.9492594003677368\n",
            "d_loss_fake:0.13835641741752625\n",
            "d_loss_wrong:0.6747735738754272\n",
            "d_loss:0.6779122054576874\n",
            "g_loss:[2.3069758, 2.2995508, 0.0037125102]\n",
            "Batch:69\n",
            "d_loss_real:1.1034269332885742\n",
            "d_loss_fake:0.09393002837896347\n",
            "d_loss_wrong:0.8421074748039246\n",
            "d_loss:0.7857228368520737\n",
            "g_loss:[2.471266, 2.4638436, 0.0037111752]\n",
            "Batch:70\n",
            "d_loss_real:0.9878489375114441\n",
            "d_loss_fake:0.07048051059246063\n",
            "d_loss_wrong:0.6046789288520813\n",
            "d_loss:0.6627143323421478\n",
            "g_loss:[1.5720557, 1.5648079, 0.0036239275]\n",
            "Batch:71\n",
            "d_loss_real:1.125655174255371\n",
            "d_loss_fake:0.11592890322208405\n",
            "d_loss_wrong:0.5893397331237793\n",
            "d_loss:0.7391447424888611\n",
            "g_loss:[1.5707492, 1.5634651, 0.003642018]\n",
            "Batch:72\n",
            "d_loss_real:1.0945578813552856\n",
            "d_loss_fake:0.25839072465896606\n",
            "d_loss_wrong:0.5793167352676392\n",
            "d_loss:0.7567058056592941\n",
            "g_loss:[1.5089334, 1.5029101, 0.0030116495]\n",
            "Batch:73\n",
            "d_loss_real:1.0122852325439453\n",
            "d_loss_fake:0.0637347549200058\n",
            "d_loss_wrong:0.5923089981079102\n",
            "d_loss:0.6701535582542419\n",
            "g_loss:[1.7621216, 1.7568574, 0.0026320752]\n",
            "Batch:74\n",
            "d_loss_real:1.0948100090026855\n",
            "d_loss_fake:0.18218040466308594\n",
            "d_loss_wrong:0.5644155144691467\n",
            "d_loss:0.7340539842844009\n",
            "g_loss:[1.8670725, 1.8621161, 0.0024781714]\n",
            "Batch:75\n",
            "d_loss_real:1.1692302227020264\n",
            "d_loss_fake:0.15049149096012115\n",
            "d_loss_wrong:0.5608019232749939\n",
            "d_loss:0.7624384611845016\n",
            "g_loss:[1.6704458, 1.6653366, 0.002554588]\n",
            "Batch:76\n",
            "d_loss_real:1.2873587608337402\n",
            "d_loss_fake:0.1404077261686325\n",
            "d_loss_wrong:0.5444468855857849\n",
            "d_loss:0.8148930370807648\n",
            "g_loss:[1.8104441, 1.8051677, 0.0026381966]\n",
            "Batch:77\n",
            "d_loss_real:1.05268394947052\n",
            "d_loss_fake:0.16754725575447083\n",
            "d_loss_wrong:0.6037264466285706\n",
            "d_loss:0.719160407781601\n",
            "g_loss:[1.8291758, 1.8246362, 0.0022698338]\n",
            "Batch:78\n",
            "d_loss_real:1.1212468147277832\n",
            "d_loss_fake:0.07633501291275024\n",
            "d_loss_wrong:0.6369727849960327\n",
            "d_loss:0.7389503568410873\n",
            "g_loss:[1.5136814, 1.508585, 0.0025482457]\n",
            "Batch:79\n",
            "d_loss_real:1.0531225204467773\n",
            "d_loss_fake:0.06268341839313507\n",
            "d_loss_wrong:0.5818026661872864\n",
            "d_loss:0.6876827776432037\n",
            "g_loss:[1.2379026, 1.2326186, 0.002642023]\n",
            "Batch:80\n",
            "d_loss_real:1.003373384475708\n",
            "d_loss_fake:0.14154711365699768\n",
            "d_loss_wrong:0.6277163028717041\n",
            "d_loss:0.6940025389194489\n",
            "g_loss:[1.6904361, 1.6842561, 0.003090038]\n",
            "Batch:81\n",
            "d_loss_real:1.0014355182647705\n",
            "d_loss_fake:0.09597708284854889\n",
            "d_loss_wrong:0.5563925504684448\n",
            "d_loss:0.6638101637363434\n",
            "g_loss:[1.8405198, 1.8347759, 0.0028719206]\n",
            "Batch:82\n",
            "d_loss_real:1.2646520137786865\n",
            "d_loss_fake:0.11119277030229568\n",
            "d_loss_wrong:0.6444501280784607\n",
            "d_loss:0.8212367296218872\n",
            "g_loss:[1.8027649, 1.797544, 0.002610452]\n",
            "Batch:83\n",
            "d_loss_real:0.8951926231384277\n",
            "d_loss_fake:0.23502090573310852\n",
            "d_loss_wrong:0.5959821343421936\n",
            "d_loss:0.65534707903862\n",
            "g_loss:[1.7345042, 1.7280974, 0.0032033715]\n",
            "Batch:84\n",
            "d_loss_real:1.1801551580429077\n",
            "d_loss_fake:0.23459067940711975\n",
            "d_loss_wrong:0.5600890517234802\n",
            "d_loss:0.7887475192546844\n",
            "g_loss:[1.693743, 1.6885015, 0.0026207792]\n",
            "Batch:85\n",
            "d_loss_real:1.1153128147125244\n",
            "d_loss_fake:0.15356892347335815\n",
            "d_loss_wrong:0.5484692454338074\n",
            "d_loss:0.7331659495830536\n",
            "g_loss:[1.9036195, 1.897911, 0.0028542816]\n",
            "Batch:86\n",
            "d_loss_real:1.0879887342453003\n",
            "d_loss_fake:0.09012573957443237\n",
            "d_loss_wrong:0.5946106314659119\n",
            "d_loss:0.7151784598827362\n",
            "g_loss:[2.0169623, 2.0116131, 0.002674637]\n",
            "Batch:87\n",
            "d_loss_real:1.010357141494751\n",
            "d_loss_fake:0.17367830872535706\n",
            "d_loss_wrong:0.5699276924133301\n",
            "d_loss:0.6910800635814667\n",
            "g_loss:[1.9666595, 1.9608517, 0.0029039397]\n",
            "Batch:88\n",
            "d_loss_real:1.443268895149231\n",
            "d_loss_fake:0.21120545268058777\n",
            "d_loss_wrong:0.6107851266860962\n",
            "d_loss:0.9271320998668671\n",
            "g_loss:[1.6559771, 1.6502007, 0.0028881903]\n",
            "Batch:89\n",
            "d_loss_real:1.0285816192626953\n",
            "d_loss_fake:0.20968669652938843\n",
            "d_loss_wrong:0.6189630627632141\n",
            "d_loss:0.7214532494544983\n",
            "g_loss:[1.6632786, 1.6548777, 0.004200484]\n",
            "Batch:90\n",
            "d_loss_real:1.030297040939331\n",
            "d_loss_fake:0.15607419610023499\n",
            "d_loss_wrong:0.6181966662406921\n",
            "d_loss:0.7087162435054779\n",
            "g_loss:[1.7784148, 1.7713667, 0.0035240722]\n",
            "Batch:91\n",
            "d_loss_real:1.0726932287216187\n",
            "d_loss_fake:0.16550903022289276\n",
            "d_loss_wrong:0.667395293712616\n",
            "d_loss:0.7445726990699768\n",
            "g_loss:[1.320086, 1.3130584, 0.003513805]\n",
            "Batch:92\n",
            "d_loss_real:1.1110811233520508\n",
            "d_loss_fake:0.19777071475982666\n",
            "d_loss_wrong:0.7504900097846985\n",
            "d_loss:0.7926057428121567\n",
            "g_loss:[1.2992442, 1.2887268, 0.005258694]\n",
            "Batch:93\n",
            "d_loss_real:1.0400139093399048\n",
            "d_loss_fake:0.2533809244632721\n",
            "d_loss_wrong:0.6111082434654236\n",
            "d_loss:0.7361292541027069\n",
            "g_loss:[1.6880944, 1.6792324, 0.004431025]\n",
            "Batch:94\n",
            "d_loss_real:1.270745038986206\n",
            "d_loss_fake:0.3020016551017761\n",
            "d_loss_wrong:0.5107181072235107\n",
            "d_loss:0.8385524600744247\n",
            "g_loss:[1.248198, 1.2396353, 0.0042813555]\n",
            "Batch:95\n",
            "d_loss_real:1.1586978435516357\n",
            "d_loss_fake:0.15379345417022705\n",
            "d_loss_wrong:0.5275732278823853\n",
            "d_loss:0.749690592288971\n",
            "g_loss:[1.3239284, 1.3167908, 0.0035687808]\n",
            "Batch:96\n",
            "d_loss_real:1.045331358909607\n",
            "d_loss_fake:0.2008737027645111\n",
            "d_loss_wrong:0.5487140417098999\n",
            "d_loss:0.7100626230239868\n",
            "g_loss:[1.4090189, 1.3992922, 0.004863339]\n",
            "Batch:97\n",
            "d_loss_real:1.0534639358520508\n",
            "d_loss_fake:0.09280985593795776\n",
            "d_loss_wrong:0.5664688944816589\n",
            "d_loss:0.6915516555309296\n",
            "g_loss:[1.311886, 1.3033955, 0.0042451965]\n",
            "Batch:98\n",
            "d_loss_real:1.0324511528015137\n",
            "d_loss_fake:0.3622463643550873\n",
            "d_loss_wrong:0.6080605983734131\n",
            "d_loss:0.7588023245334625\n",
            "g_loss:[2.4667058, 2.4596357, 0.0035350155]\n",
            "Batch:99\n",
            "d_loss_real:1.1108417510986328\n",
            "d_loss_fake:0.08722291886806488\n",
            "d_loss_wrong:0.5399682521820068\n",
            "d_loss:0.7122186720371246\n",
            "g_loss:[2.289916, 2.2827847, 0.0035656837]\n",
            "Batch:100\n",
            "d_loss_real:1.1751577854156494\n",
            "d_loss_fake:0.14193813502788544\n",
            "d_loss_wrong:0.5210431814193726\n",
            "d_loss:0.7533242255449295\n",
            "g_loss:[1.9962137, 1.9887727, 0.0037204819]\n",
            "Batch:101\n",
            "d_loss_real:1.0391844511032104\n",
            "d_loss_fake:0.15621933341026306\n",
            "d_loss_wrong:0.6111090183258057\n",
            "d_loss:0.711424320936203\n",
            "g_loss:[2.2437832, 2.2383232, 0.00273003]\n",
            "Batch:102\n",
            "d_loss_real:1.0252330303192139\n",
            "d_loss_fake:0.12339699268341064\n",
            "d_loss_wrong:0.5769316554069519\n",
            "d_loss:0.6876986771821976\n",
            "g_loss:[2.1468108, 2.1406784, 0.0030661677]\n",
            "Batch:103\n",
            "d_loss_real:1.1172325611114502\n",
            "d_loss_fake:0.10083704441785812\n",
            "d_loss_wrong:0.5560101270675659\n",
            "d_loss:0.7228280752897263\n",
            "g_loss:[1.8428261, 1.836384, 0.0032210182]\n",
            "Batch:104\n",
            "d_loss_real:1.2331706285476685\n",
            "d_loss_fake:0.14626002311706543\n",
            "d_loss_wrong:0.6240004301071167\n",
            "d_loss:0.8091504275798798\n",
            "g_loss:[1.3572738, 1.3521086, 0.002582615]\n",
            "Batch:105\n",
            "d_loss_real:0.9521145820617676\n",
            "d_loss_fake:0.1204233467578888\n",
            "d_loss_wrong:0.6521404981613159\n",
            "d_loss:0.6691982448101044\n",
            "g_loss:[1.4500084, 1.4456497, 0.0021793102]\n",
            "Batch:106\n",
            "d_loss_real:0.9322332143783569\n",
            "d_loss_fake:0.03685307502746582\n",
            "d_loss_wrong:0.6277351379394531\n",
            "d_loss:0.6322636604309082\n",
            "g_loss:[1.3175471, 1.3130422, 0.0022524386]\n",
            "Batch:107\n",
            "d_loss_real:1.0481672286987305\n",
            "d_loss_fake:0.02494639717042446\n",
            "d_loss_wrong:0.6641020178794861\n",
            "d_loss:0.696345716714859\n",
            "g_loss:[1.2932531, 1.2875137, 0.0028696735]\n",
            "Batch:108\n",
            "d_loss_real:0.9434914588928223\n",
            "d_loss_fake:0.04081300273537636\n",
            "d_loss_wrong:0.6706244945526123\n",
            "d_loss:0.6496051102876663\n",
            "g_loss:[1.0055133, 0.9992302, 0.0031415771]\n",
            "Batch:109\n",
            "d_loss_real:0.9657390713691711\n",
            "d_loss_fake:0.02182803303003311\n",
            "d_loss_wrong:0.6998773217201233\n",
            "d_loss:0.6632958799600601\n",
            "g_loss:[0.9594026, 0.9532161, 0.0030932804]\n",
            "Batch:110\n",
            "d_loss_real:0.8645555973052979\n",
            "d_loss_fake:0.032276902347803116\n",
            "d_loss_wrong:0.6609309315681458\n",
            "d_loss:0.6055797636508942\n",
            "g_loss:[1.0759835, 1.0689065, 0.0035384633]\n",
            "Batch:111\n",
            "d_loss_real:0.9316524267196655\n",
            "d_loss_fake:0.019030271098017693\n",
            "d_loss_wrong:0.6941310167312622\n",
            "d_loss:0.644116535782814\n",
            "g_loss:[0.95291126, 0.94716394, 0.0028736477]\n",
            "Batch:112\n",
            "d_loss_real:1.0022609233856201\n",
            "d_loss_fake:0.025268679484725\n",
            "d_loss_wrong:0.774833619594574\n",
            "d_loss:0.7011560350656509\n",
            "g_loss:[0.93120164, 0.9222714, 0.0044651437]\n",
            "Batch:113\n",
            "d_loss_real:0.8776511549949646\n",
            "d_loss_fake:0.05490107461810112\n",
            "d_loss_wrong:0.6587361097335815\n",
            "d_loss:0.6172348707914352\n",
            "g_loss:[0.7945399, 0.78703254, 0.003753678]\n",
            "Batch:114\n",
            "d_loss_real:0.9675294160842896\n",
            "d_loss_fake:0.014637794345617294\n",
            "d_loss_wrong:0.6118723154067993\n",
            "d_loss:0.6403922289609909\n",
            "g_loss:[1.0174663, 1.0102627, 0.003601775]\n",
            "Batch:115\n",
            "d_loss_real:0.8635608553886414\n",
            "d_loss_fake:0.05804964154958725\n",
            "d_loss_wrong:0.6896488070487976\n",
            "d_loss:0.6187050342559814\n",
            "g_loss:[0.91995424, 0.91333544, 0.0033094017]\n",
            "Batch:116\n",
            "d_loss_real:1.1301798820495605\n",
            "d_loss_fake:0.034043438732624054\n",
            "d_loss_wrong:0.7130727171897888\n",
            "d_loss:0.7518689781427383\n",
            "g_loss:[1.0586338, 1.0519452, 0.0033443205]\n",
            "Batch:117\n",
            "d_loss_real:0.8180259466171265\n",
            "d_loss_fake:0.0447770394384861\n",
            "d_loss_wrong:0.685721755027771\n",
            "d_loss:0.5916376709938049\n",
            "g_loss:[1.041503, 1.0350553, 0.0032238532]\n",
            "Batch:118\n",
            "d_loss_real:0.9453071355819702\n",
            "d_loss_fake:0.0636715516448021\n",
            "d_loss_wrong:0.6208443641662598\n",
            "d_loss:0.6437825411558151\n",
            "g_loss:[1.0761038, 1.0703535, 0.002875151]\n",
            "Batch:119\n",
            "d_loss_real:0.9223748445510864\n",
            "d_loss_fake:0.05098259449005127\n",
            "d_loss_wrong:0.6067468523979187\n",
            "d_loss:0.6256197839975357\n",
            "g_loss:[1.175354, 1.1693969, 0.0029785838]\n",
            "Batch:120\n",
            "d_loss_real:0.9024075269699097\n",
            "d_loss_fake:0.07530558109283447\n",
            "d_loss_wrong:0.6513025164604187\n",
            "d_loss:0.6328557878732681\n",
            "g_loss:[1.1252544, 1.1179233, 0.003665571]\n",
            "Batch:121\n",
            "d_loss_real:0.8996163606643677\n",
            "d_loss_fake:0.11078839004039764\n",
            "d_loss_wrong:0.6470997929573059\n",
            "d_loss:0.6392802298069\n",
            "g_loss:[1.607872, 1.599543, 0.004164487]\n",
            "Batch:122\n",
            "d_loss_real:1.0418490171432495\n",
            "d_loss_fake:0.04481945186853409\n",
            "d_loss_wrong:0.5997399091720581\n",
            "d_loss:0.6820643544197083\n",
            "g_loss:[1.6016128, 1.592966, 0.0043234276]\n",
            "Batch:123\n",
            "d_loss_real:1.2931368350982666\n",
            "d_loss_fake:0.10901430249214172\n",
            "d_loss_wrong:0.6767148375511169\n",
            "d_loss:0.8430007100105286\n",
            "g_loss:[1.6763649, 1.6682107, 0.00407709]\n",
            "Batch:124\n",
            "d_loss_real:0.8864035606384277\n",
            "d_loss_fake:0.06939851492643356\n",
            "d_loss_wrong:0.6362863183021545\n",
            "d_loss:0.619622990489006\n",
            "g_loss:[1.3624425, 1.3555804, 0.003431012]\n",
            "Batch:125\n",
            "d_loss_real:0.9141985177993774\n",
            "d_loss_fake:0.05422656238079071\n",
            "d_loss_wrong:0.6066095232963562\n",
            "d_loss:0.6223082840442657\n",
            "g_loss:[1.2701102, 1.2610183, 0.0045459685]\n",
            "Batch:126\n",
            "d_loss_real:0.8889451026916504\n",
            "d_loss_fake:0.04676283895969391\n",
            "d_loss_wrong:0.6383687853813171\n",
            "d_loss:0.6157554537057877\n",
            "g_loss:[1.078393, 1.0699127, 0.004240174]\n",
            "Batch:127\n",
            "d_loss_real:1.0378451347351074\n",
            "d_loss_fake:0.07413339614868164\n",
            "d_loss_wrong:0.5803307890892029\n",
            "d_loss:0.6825386136770248\n",
            "g_loss:[1.1438428, 1.1360284, 0.0039072144]\n",
            "Batch:128\n",
            "d_loss_real:0.9637338519096375\n",
            "d_loss_fake:0.09287138283252716\n",
            "d_loss_wrong:0.612673282623291\n",
            "d_loss:0.658253088593483\n",
            "g_loss:[0.94607097, 0.9329193, 0.006575829]\n",
            "Batch:129\n",
            "d_loss_real:0.9155515432357788\n",
            "d_loss_fake:0.04464452341198921\n",
            "d_loss_wrong:0.6148766279220581\n",
            "d_loss:0.622656062245369\n",
            "g_loss:[1.0679871, 1.0565758, 0.005705658]\n",
            "Batch:130\n",
            "d_loss_real:0.9829097390174866\n",
            "d_loss_fake:0.15406978130340576\n",
            "d_loss_wrong:0.6516354084014893\n",
            "d_loss:0.692881166934967\n",
            "g_loss:[1.1224718, 1.1148226, 0.003824618]\n",
            "Batch:131\n",
            "d_loss_real:0.9165098071098328\n",
            "d_loss_fake:0.1207779049873352\n",
            "d_loss_wrong:0.6436604261398315\n",
            "d_loss:0.6493644863367081\n",
            "g_loss:[1.3898945, 1.377813, 0.006040751]\n",
            "Batch:132\n",
            "d_loss_real:1.1093130111694336\n",
            "d_loss_fake:0.06294840574264526\n",
            "d_loss_wrong:0.603122889995575\n",
            "d_loss:0.7211743295192719\n",
            "g_loss:[1.1991942, 1.1904399, 0.0043771034]\n",
            "Batch:133\n",
            "d_loss_real:0.9446536302566528\n",
            "d_loss_fake:0.051801230758428574\n",
            "d_loss_wrong:0.6306949853897095\n",
            "d_loss:0.6429508626461029\n",
            "g_loss:[1.5583282, 1.5511165, 0.0036058233]\n",
            "Batch:134\n",
            "d_loss_real:0.939310610294342\n",
            "d_loss_fake:0.019239123910665512\n",
            "d_loss_wrong:0.6437755823135376\n",
            "d_loss:0.6354089826345444\n",
            "g_loss:[1.1277372, 1.1197648, 0.0039861854]\n",
            "Batch:135\n",
            "d_loss_real:1.2641477584838867\n",
            "d_loss_fake:0.1125819981098175\n",
            "d_loss_wrong:0.6274309754371643\n",
            "d_loss:0.8170771300792694\n",
            "g_loss:[1.1347394, 1.1279123, 0.0034135287]\n",
            "Batch:136\n",
            "d_loss_real:0.8596463203430176\n",
            "d_loss_fake:0.028242578729987144\n",
            "d_loss_wrong:0.6881564855575562\n",
            "d_loss:0.608922928571701\n",
            "g_loss:[1.0997818, 1.0928413, 0.0034702164]\n",
            "Batch:137\n",
            "d_loss_real:0.929337739944458\n",
            "d_loss_fake:0.030708475038409233\n",
            "d_loss_wrong:0.6711052060127258\n",
            "d_loss:0.6401222944259644\n",
            "g_loss:[1.0053056, 0.9975455, 0.0038801115]\n",
            "Batch:138\n",
            "d_loss_real:0.9183875918388367\n",
            "d_loss_fake:0.03223104402422905\n",
            "d_loss_wrong:0.6387798190116882\n",
            "d_loss:0.6269465088844299\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "g_loss:[1.0577971, 1.0496109, 0.0040931394]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "========================================\n",
            "Epoch is: 3\n",
            "Number of batches 138\n",
            "Batch:1\n",
            "d_loss_real:0.8636237978935242\n",
            "d_loss_fake:0.02807287871837616\n",
            "d_loss_wrong:0.7049607038497925\n",
            "d_loss:0.6150702983140945\n",
            "g_loss:[1.1611452, 1.1550578, 0.0030437214]\n",
            "Batch:2\n",
            "d_loss_real:0.8420836329460144\n",
            "d_loss_fake:0.025188889354467392\n",
            "d_loss_wrong:0.6937149167060852\n",
            "d_loss:0.6007677614688873\n",
            "g_loss:[1.2390679, 1.230937, 0.0040654503]\n",
            "Batch:3\n",
            "d_loss_real:0.9865588545799255\n",
            "d_loss_fake:0.020274881273508072\n",
            "d_loss_wrong:0.6938419938087463\n",
            "d_loss:0.6718086451292038\n",
            "g_loss:[0.9679099, 0.9580122, 0.0049488246]\n",
            "Batch:4\n",
            "d_loss_real:0.906847357749939\n",
            "d_loss_fake:0.012362409383058548\n",
            "d_loss_wrong:0.7043409943580627\n",
            "d_loss:0.6325995326042175\n",
            "g_loss:[0.91890633, 0.9085686, 0.0051688626]\n",
            "Batch:5\n",
            "d_loss_real:0.8521538972854614\n",
            "d_loss_fake:0.01668606884777546\n",
            "d_loss_wrong:0.6765202283859253\n",
            "d_loss:0.5993785262107849\n",
            "g_loss:[0.99601525, 0.98651004, 0.004752604]\n",
            "Batch:6\n",
            "d_loss_real:0.9208271503448486\n",
            "d_loss_fake:0.0353487953543663\n",
            "d_loss_wrong:0.6964132189750671\n",
            "d_loss:0.6433540731668472\n",
            "g_loss:[0.89253914, 0.8832811, 0.0046290206]\n",
            "Batch:7\n",
            "d_loss_real:0.8637866973876953\n",
            "d_loss_fake:0.030542919412255287\n",
            "d_loss_wrong:0.6837222576141357\n",
            "d_loss:0.610459640622139\n",
            "g_loss:[0.99046516, 0.9820224, 0.004221379]\n",
            "Batch:8\n",
            "d_loss_real:0.9072204828262329\n",
            "d_loss_fake:0.022686008363962173\n",
            "d_loss_wrong:0.6069179177284241\n",
            "d_loss:0.6110112220048904\n",
            "g_loss:[0.9775258, 0.96593726, 0.005794251]\n",
            "Batch:9\n",
            "d_loss_real:1.276521921157837\n",
            "d_loss_fake:0.09288935363292694\n",
            "d_loss_wrong:0.6534485220909119\n",
            "d_loss:0.8248454332351685\n",
            "g_loss:[0.8000138, 0.7850409, 0.0074864216]\n",
            "Batch:10\n",
            "d_loss_real:0.9051834344863892\n",
            "d_loss_fake:0.04437869414687157\n",
            "d_loss_wrong:0.6503821611404419\n",
            "d_loss:0.6262819319963455\n",
            "g_loss:[0.79262584, 0.77911025, 0.0067577884]\n",
            "Batch:11\n",
            "d_loss_real:1.106212854385376\n",
            "d_loss_fake:0.06281423568725586\n",
            "d_loss_wrong:0.6340219378471375\n",
            "d_loss:0.7273154705762863\n",
            "g_loss:[1.0087259, 1.0001371, 0.0042944234]\n",
            "Batch:12\n",
            "d_loss_real:0.8952069282531738\n",
            "d_loss_fake:0.10878239572048187\n",
            "d_loss_wrong:0.6523047685623169\n",
            "d_loss:0.6378752589225769\n",
            "g_loss:[0.89582056, 0.8875754, 0.0041225776]\n",
            "Batch:13\n",
            "d_loss_real:0.849020779132843\n",
            "d_loss_fake:0.06643559783697128\n",
            "d_loss_wrong:0.6359931826591492\n",
            "d_loss:0.6001175791025162\n",
            "g_loss:[0.9041808, 0.8955326, 0.004324101]\n",
            "Batch:14\n",
            "d_loss_real:1.0429476499557495\n",
            "d_loss_fake:0.01463501900434494\n",
            "d_loss_wrong:0.6178984045982361\n",
            "d_loss:0.6796071827411652\n",
            "g_loss:[1.0358126, 1.0283076, 0.0037525515]\n",
            "Batch:15\n",
            "d_loss_real:0.8500580787658691\n",
            "d_loss_fake:0.036538269370794296\n",
            "d_loss_wrong:0.7032888531684875\n",
            "d_loss:0.609985813498497\n",
            "g_loss:[0.89928436, 0.8877579, 0.005763232]\n",
            "Batch:16\n",
            "d_loss_real:0.8376635313034058\n",
            "d_loss_fake:0.053163282573223114\n",
            "d_loss_wrong:0.6829472184181213\n",
            "d_loss:0.6028593927621841\n",
            "g_loss:[0.9216578, 0.9092239, 0.006216939]\n",
            "Batch:17\n",
            "d_loss_real:0.8991208076477051\n",
            "d_loss_fake:0.057748228311538696\n",
            "d_loss_wrong:0.6732048392295837\n",
            "d_loss:0.6322986781597137\n",
            "g_loss:[0.9893901, 0.97791815, 0.005735949]\n",
            "Batch:18\n",
            "d_loss_real:0.9467653036117554\n",
            "d_loss_fake:0.014634786173701286\n",
            "d_loss_wrong:0.6277641654014587\n",
            "d_loss:0.633982390165329\n",
            "g_loss:[0.9723539, 0.96379775, 0.0042780647]\n",
            "Batch:19\n",
            "d_loss_real:0.9317083358764648\n",
            "d_loss_fake:0.07553760707378387\n",
            "d_loss_wrong:0.6427632570266724\n",
            "d_loss:0.6454293876886368\n",
            "g_loss:[1.0375037, 1.029097, 0.0042033615]\n",
            "Batch:20\n",
            "d_loss_real:0.987709105014801\n",
            "d_loss_fake:0.02507733926177025\n",
            "d_loss_wrong:0.6907119750976562\n",
            "d_loss:0.6728018820285797\n",
            "g_loss:[0.81377405, 0.8047738, 0.004500112]\n",
            "Batch:21\n",
            "d_loss_real:0.9547059535980225\n",
            "d_loss_fake:0.01689767837524414\n",
            "d_loss_wrong:0.6066149473190308\n",
            "d_loss:0.63323113322258\n",
            "g_loss:[0.8627469, 0.85439324, 0.004176826]\n",
            "Batch:22\n",
            "d_loss_real:0.832486629486084\n",
            "d_loss_fake:0.026512200012803078\n",
            "d_loss_wrong:0.7347968816757202\n",
            "d_loss:0.6065705865621567\n",
            "g_loss:[0.9290323, 0.92142165, 0.0038053538]\n",
            "Batch:23\n",
            "d_loss_real:0.9003698825836182\n",
            "d_loss_fake:0.019544899463653564\n",
            "d_loss_wrong:0.6501689553260803\n",
            "d_loss:0.6176134049892426\n",
            "g_loss:[0.8217018, 0.8138277, 0.003937061]\n",
            "Batch:24\n",
            "d_loss_real:0.8916375041007996\n",
            "d_loss_fake:0.02417900785803795\n",
            "d_loss_wrong:0.6395446062088013\n",
            "d_loss:0.6117496490478516\n",
            "g_loss:[0.7928511, 0.7804603, 0.006195383]\n",
            "Batch:25\n",
            "d_loss_real:0.9051177501678467\n",
            "d_loss_fake:0.03984992578625679\n",
            "d_loss_wrong:0.6916269063949585\n",
            "d_loss:0.6354280859231949\n",
            "g_loss:[0.86553067, 0.8538496, 0.0058405343]\n",
            "Batch:26\n",
            "d_loss_real:0.8299147486686707\n",
            "d_loss_fake:0.0253991037607193\n",
            "d_loss_wrong:0.684678316116333\n",
            "d_loss:0.5924767255783081\n",
            "g_loss:[0.86369085, 0.85633534, 0.003677745]\n",
            "Batch:27\n",
            "d_loss_real:0.9742212295532227\n",
            "d_loss_fake:0.011826245114207268\n",
            "d_loss_wrong:0.7974491715431213\n",
            "d_loss:0.6894294619560242\n",
            "g_loss:[0.7556764, 0.7467847, 0.0044458606]\n",
            "Batch:28\n",
            "d_loss_real:1.0191409587860107\n",
            "d_loss_fake:0.06144246086478233\n",
            "d_loss_wrong:0.6002811193466187\n",
            "d_loss:0.6750013679265976\n",
            "g_loss:[0.8953923, 0.8827817, 0.0063053146]\n",
            "Batch:29\n",
            "d_loss_real:0.870009183883667\n",
            "d_loss_fake:0.009940113872289658\n",
            "d_loss_wrong:0.6681278944015503\n",
            "d_loss:0.6045215874910355\n",
            "g_loss:[0.79333276, 0.7818967, 0.0057180263]\n",
            "Batch:30\n",
            "d_loss_real:0.8815330266952515\n",
            "d_loss_fake:0.005276720970869064\n",
            "d_loss_wrong:0.763273298740387\n",
            "d_loss:0.6329040229320526\n",
            "g_loss:[0.844333, 0.8348723, 0.004730337]\n",
            "Batch:31\n",
            "d_loss_real:0.8944676518440247\n",
            "d_loss_fake:0.09672170877456665\n",
            "d_loss_wrong:0.6146684885025024\n",
            "d_loss:0.6250813752412796\n",
            "g_loss:[1.0386552, 1.0278158, 0.0054196725]\n",
            "Batch:32\n",
            "d_loss_real:1.0624103546142578\n",
            "d_loss_fake:0.030534422025084496\n",
            "d_loss_wrong:0.5725507736206055\n",
            "d_loss:0.6819764822721481\n",
            "g_loss:[0.8605846, 0.8491597, 0.0057124454]\n",
            "Batch:33\n",
            "d_loss_real:0.8940703272819519\n",
            "d_loss_fake:0.0364392064511776\n",
            "d_loss_wrong:0.6416330337524414\n",
            "d_loss:0.6165532171726227\n",
            "g_loss:[0.9108326, 0.8991653, 0.0058336593]\n",
            "Batch:34\n",
            "d_loss_real:0.9313309788703918\n",
            "d_loss_fake:0.13855232298374176\n",
            "d_loss_wrong:0.5964561700820923\n",
            "d_loss:0.6494176089763641\n",
            "g_loss:[0.8625085, 0.84987915, 0.006314669]\n",
            "Batch:35\n",
            "d_loss_real:0.977972686290741\n",
            "d_loss_fake:0.06607984006404877\n",
            "d_loss_wrong:0.6455748081207275\n",
            "d_loss:0.6669000089168549\n",
            "g_loss:[0.9907807, 0.9794298, 0.005675451]\n",
            "Batch:36\n",
            "d_loss_real:0.9783850908279419\n",
            "d_loss_fake:0.03303651511669159\n",
            "d_loss_wrong:0.593915581703186\n",
            "d_loss:0.6459305733442307\n",
            "g_loss:[1.045024, 1.0358292, 0.0045974003]\n",
            "Batch:37\n",
            "d_loss_real:0.9727880954742432\n",
            "d_loss_fake:0.014668375253677368\n",
            "d_loss_wrong:0.6334795951843262\n",
            "d_loss:0.6484310328960419\n",
            "g_loss:[1.0155367, 1.0076194, 0.003958633]\n",
            "Batch:38\n",
            "d_loss_real:0.8648726940155029\n",
            "d_loss_fake:0.083824023604393\n",
            "d_loss_wrong:0.6676141619682312\n",
            "d_loss:0.6202958971261978\n",
            "g_loss:[1.1016523, 1.0927138, 0.004469202]\n",
            "Batch:39\n",
            "d_loss_real:0.8790754675865173\n",
            "d_loss_fake:0.023472726345062256\n",
            "d_loss_wrong:0.6549973487854004\n",
            "d_loss:0.6091552525758743\n",
            "g_loss:[1.2002746, 1.1924151, 0.0039297184]\n",
            "Batch:40\n",
            "d_loss_real:1.0302276611328125\n",
            "d_loss_fake:0.024028455838561058\n",
            "d_loss_wrong:0.6860243678092957\n",
            "d_loss:0.6926270425319672\n",
            "g_loss:[0.92867947, 0.92143685, 0.0036213035]\n",
            "Batch:41\n",
            "d_loss_real:0.9119923114776611\n",
            "d_loss_fake:0.03305976092815399\n",
            "d_loss_wrong:0.6424586772918701\n",
            "d_loss:0.6248757690191269\n",
            "g_loss:[0.88817936, 0.8797368, 0.0042212945]\n",
            "Batch:42\n",
            "d_loss_real:0.9912198185920715\n",
            "d_loss_fake:0.027156906202435493\n",
            "d_loss_wrong:0.6718003749847412\n",
            "d_loss:0.6703492254018784\n",
            "g_loss:[0.9586687, 0.9487304, 0.0049691433]\n",
            "Batch:43\n",
            "d_loss_real:1.2483763694763184\n",
            "d_loss_fake:0.14040660858154297\n",
            "d_loss_wrong:0.6464965343475342\n",
            "d_loss:0.8209139704704285\n",
            "g_loss:[0.9206123, 0.9071051, 0.0067535858]\n",
            "Batch:44\n",
            "d_loss_real:0.8387296795845032\n",
            "d_loss_fake:0.1120021790266037\n",
            "d_loss_wrong:0.651849627494812\n",
            "d_loss:0.6103277951478958\n",
            "g_loss:[1.1569823, 1.1460676, 0.00545735]\n",
            "Batch:45\n",
            "d_loss_real:1.0044426918029785\n",
            "d_loss_fake:0.05864489823579788\n",
            "d_loss_wrong:0.5634751319885254\n",
            "d_loss:0.6577513515949249\n",
            "g_loss:[1.0562922, 1.0445439, 0.005874159]\n",
            "Batch:46\n",
            "d_loss_real:0.8410563468933105\n",
            "d_loss_fake:0.08587871491909027\n",
            "d_loss_wrong:0.6376734375953674\n",
            "d_loss:0.60141621530056\n",
            "g_loss:[1.1973269, 1.1863599, 0.0054835253]\n",
            "Batch:47\n",
            "d_loss_real:0.8543041944503784\n",
            "d_loss_fake:0.16086801886558533\n",
            "d_loss_wrong:0.603249192237854\n",
            "d_loss:0.6181814074516296\n",
            "g_loss:[2.4294062, 2.4202123, 0.0045969617]\n",
            "Batch:48\n",
            "d_loss_real:1.4841806888580322\n",
            "d_loss_fake:0.07388179004192352\n",
            "d_loss_wrong:0.7052119374275208\n",
            "d_loss:0.9368637800216675\n",
            "g_loss:[1.3608862, 1.3493991, 0.005743543]\n",
            "Batch:49\n",
            "d_loss_real:0.8585111498832703\n",
            "d_loss_fake:0.16104412078857422\n",
            "d_loss_wrong:0.5957323312759399\n",
            "d_loss:0.6184496879577637\n",
            "g_loss:[1.4062009, 1.3940635, 0.0060686986]\n",
            "Batch:50\n",
            "d_loss_real:1.1791290044784546\n",
            "d_loss_fake:0.21600854396820068\n",
            "d_loss_wrong:0.522213876247406\n",
            "d_loss:0.774120107293129\n",
            "g_loss:[1.4775612, 1.4677355, 0.0049128486]\n",
            "Batch:51\n",
            "d_loss_real:0.9653027653694153\n",
            "d_loss_fake:0.1555289924144745\n",
            "d_loss_wrong:0.6031143069267273\n",
            "d_loss:0.6723122000694275\n",
            "g_loss:[1.6198252, 1.6095808, 0.005122248]\n",
            "Batch:52\n",
            "d_loss_real:0.9149198532104492\n",
            "d_loss_fake:0.16544753313064575\n",
            "d_loss_wrong:0.6472122073173523\n",
            "d_loss:0.6606248617172241\n",
            "g_loss:[1.5900543, 1.5824399, 0.0038071615]\n",
            "Batch:53\n",
            "d_loss_real:1.0397617816925049\n",
            "d_loss_fake:0.18817880749702454\n",
            "d_loss_wrong:0.5705005526542664\n",
            "d_loss:0.7095507383346558\n",
            "g_loss:[1.7833643, 1.7723535, 0.0055053625]\n",
            "Batch:54\n",
            "d_loss_real:1.0082106590270996\n",
            "d_loss_fake:0.10518519580364227\n",
            "d_loss_wrong:0.6433771252632141\n",
            "d_loss:0.6912459135055542\n",
            "g_loss:[1.7801186, 1.7697222, 0.005198171]\n",
            "Batch:55\n",
            "d_loss_real:1.0582504272460938\n",
            "d_loss_fake:0.08188766986131668\n",
            "d_loss_wrong:0.5751974582672119\n",
            "d_loss:0.6933964937925339\n",
            "g_loss:[1.4079233, 1.39767, 0.005126656]\n",
            "Batch:56\n",
            "d_loss_real:1.0751135349273682\n",
            "d_loss_fake:0.18827085196971893\n",
            "d_loss_wrong:0.6492058634757996\n",
            "d_loss:0.746925950050354\n",
            "g_loss:[1.5100356, 1.5000862, 0.004974695]\n",
            "Batch:57\n",
            "d_loss_real:0.9709386825561523\n",
            "d_loss_fake:0.07433225214481354\n",
            "d_loss_wrong:0.6280280351638794\n",
            "d_loss:0.6610594093799591\n",
            "g_loss:[1.6371665, 1.6249955, 0.0060854997]\n",
            "Batch:58\n",
            "d_loss_real:0.9768555164337158\n",
            "d_loss_fake:0.11207352578639984\n",
            "d_loss_wrong:0.5870429277420044\n",
            "d_loss:0.6632068753242493\n",
            "g_loss:[1.7155174, 1.7037189, 0.0058992594]\n",
            "Batch:59\n",
            "d_loss_real:1.0504260063171387\n",
            "d_loss_fake:0.06702247262001038\n",
            "d_loss_wrong:0.6753345727920532\n",
            "d_loss:0.7108022570610046\n",
            "g_loss:[1.5791665, 1.5697095, 0.0047285]\n",
            "Batch:60\n",
            "d_loss_real:0.9884850978851318\n",
            "d_loss_fake:0.09801499545574188\n",
            "d_loss_wrong:0.6749412417411804\n",
            "d_loss:0.6874816119670868\n",
            "g_loss:[1.3740249, 1.3648305, 0.004597194]\n",
            "Batch:61\n",
            "d_loss_real:1.0282502174377441\n",
            "d_loss_fake:0.0881313607096672\n",
            "d_loss_wrong:0.5828315019607544\n",
            "d_loss:0.6818658262491226\n",
            "g_loss:[1.4770516, 1.4683645, 0.004343597]\n",
            "Batch:62\n",
            "d_loss_real:0.9541504383087158\n",
            "d_loss_fake:0.09721928834915161\n",
            "d_loss_wrong:0.6337377429008484\n",
            "d_loss:0.6598144769668579\n",
            "g_loss:[1.5382006, 1.5298917, 0.004154447]\n",
            "Batch:63\n",
            "d_loss_real:1.182115077972412\n",
            "d_loss_fake:0.10680381953716278\n",
            "d_loss_wrong:0.5725085735321045\n",
            "d_loss:0.7608856409788132\n",
            "g_loss:[1.26095, 1.2501941, 0.0053779315]\n",
            "Batch:64\n",
            "d_loss_real:1.0006775856018066\n",
            "d_loss_fake:0.13483688235282898\n",
            "d_loss_wrong:0.5623853206634521\n",
            "d_loss:0.6746443510055542\n",
            "g_loss:[1.2839193, 1.2733635, 0.005277923]\n",
            "Batch:65\n",
            "d_loss_real:1.0308032035827637\n",
            "d_loss_fake:0.13601401448249817\n",
            "d_loss_wrong:0.6594886183738708\n",
            "d_loss:0.7142772674560547\n",
            "g_loss:[1.7364411, 1.7255594, 0.0054408675]\n",
            "Batch:66\n",
            "d_loss_real:1.0091334581375122\n",
            "d_loss_fake:0.11873535811901093\n",
            "d_loss_wrong:0.5643554925918579\n",
            "d_loss:0.6753394454717636\n",
            "g_loss:[1.531729, 1.5218674, 0.004930796]\n",
            "Batch:67\n",
            "d_loss_real:1.1818242073059082\n",
            "d_loss_fake:0.18456149101257324\n",
            "d_loss_wrong:0.622887134552002\n",
            "d_loss:0.7927742600440979\n",
            "g_loss:[1.1407126, 1.132313, 0.0041998215]\n",
            "Batch:68\n",
            "d_loss_real:0.9347404837608337\n",
            "d_loss_fake:0.10505806654691696\n",
            "d_loss_wrong:0.6220991611480713\n",
            "d_loss:0.6491595506668091\n",
            "g_loss:[1.5055532, 1.4952254, 0.0051638847]\n",
            "Batch:69\n",
            "d_loss_real:1.3140568733215332\n",
            "d_loss_fake:0.1719265878200531\n",
            "d_loss_wrong:0.9198890924453735\n",
            "d_loss:0.9299823641777039\n",
            "g_loss:[0.87872463, 0.86853296, 0.0050958423]\n",
            "Batch:70\n",
            "d_loss_real:0.958389401435852\n",
            "d_loss_fake:0.3053942322731018\n",
            "d_loss_wrong:0.5257173776626587\n",
            "d_loss:0.6869726032018661\n",
            "g_loss:[1.3077163, 1.2971002, 0.0053080344]\n",
            "Batch:71\n",
            "d_loss_real:1.3068703413009644\n",
            "d_loss_fake:0.1749219447374344\n",
            "d_loss_wrong:0.4558880031108856\n",
            "d_loss:0.8111376613378525\n",
            "g_loss:[1.6279746, 1.6162171, 0.0058787432]\n",
            "Batch:72\n",
            "d_loss_real:1.3068127632141113\n",
            "d_loss_fake:0.1350541114807129\n",
            "d_loss_wrong:0.5659616589546204\n",
            "d_loss:0.828660324215889\n",
            "g_loss:[1.6229159, 1.6139007, 0.004507603]\n",
            "Batch:73\n",
            "d_loss_real:1.0933237075805664\n",
            "d_loss_fake:0.14589528739452362\n",
            "d_loss_wrong:0.6031597256660461\n",
            "d_loss:0.7339256107807159\n",
            "g_loss:[1.505202, 1.4966594, 0.004271335]\n",
            "Batch:74\n",
            "d_loss_real:1.0676584243774414\n",
            "d_loss_fake:0.06846952438354492\n",
            "d_loss_wrong:0.654107391834259\n",
            "d_loss:0.7144734412431717\n",
            "g_loss:[1.2566217, 1.2487048, 0.003958475]\n",
            "Batch:75\n",
            "d_loss_real:1.0325567722320557\n",
            "d_loss_fake:0.06952385604381561\n",
            "d_loss_wrong:0.611362636089325\n",
            "d_loss:0.6865000128746033\n",
            "g_loss:[1.306568, 1.299096, 0.0037360457]\n",
            "Batch:76\n",
            "d_loss_real:1.1195101737976074\n",
            "d_loss_fake:0.11396858096122742\n",
            "d_loss_wrong:0.5737348198890686\n",
            "d_loss:0.7316809296607971\n",
            "g_loss:[1.221171, 1.2133354, 0.003917829]\n",
            "Batch:77\n",
            "d_loss_real:0.9356518983840942\n",
            "d_loss_fake:0.07750604301691055\n",
            "d_loss_wrong:0.6468786597251892\n",
            "d_loss:0.6489221304655075\n",
            "g_loss:[1.1541115, 1.1470625, 0.003524477]\n",
            "Batch:78\n",
            "d_loss_real:1.1368343830108643\n",
            "d_loss_fake:0.10206515341997147\n",
            "d_loss_wrong:0.623232901096344\n",
            "d_loss:0.7497417032718658\n",
            "g_loss:[1.0008922, 0.9934441, 0.0037240665]\n",
            "Batch:79\n",
            "d_loss_real:0.9020498991012573\n",
            "d_loss_fake:0.06245698407292366\n",
            "d_loss_wrong:0.6310117244720459\n",
            "d_loss:0.6243921220302582\n",
            "g_loss:[0.9141202, 0.90692234, 0.003598929]\n",
            "Batch:80\n",
            "d_loss_real:0.9256044030189514\n",
            "d_loss_fake:0.02724520117044449\n",
            "d_loss_wrong:0.6839984655380249\n",
            "d_loss:0.6406131237745285\n",
            "g_loss:[0.8542035, 0.84643847, 0.0038825218]\n",
            "Batch:81\n",
            "d_loss_real:0.9733266830444336\n",
            "d_loss_fake:0.026424590498209\n",
            "d_loss_wrong:0.6059007048606873\n",
            "d_loss:0.6447446644306183\n",
            "g_loss:[0.72186875, 0.7139715, 0.00394863]\n",
            "Batch:82\n",
            "d_loss_real:0.9777088165283203\n",
            "d_loss_fake:0.08537568151950836\n",
            "d_loss_wrong:0.6390272974967957\n",
            "d_loss:0.6699551492929459\n",
            "g_loss:[0.7154153, 0.7085482, 0.0034335582]\n",
            "Batch:83\n",
            "d_loss_real:0.8607227802276611\n",
            "d_loss_fake:0.05911124870181084\n",
            "d_loss_wrong:0.6130474805831909\n",
            "d_loss:0.5984010696411133\n",
            "g_loss:[0.77664095, 0.7686461, 0.003997409]\n",
            "Batch:84\n",
            "d_loss_real:0.9586266279220581\n",
            "d_loss_fake:0.0420270636677742\n",
            "d_loss_wrong:0.6537882089614868\n",
            "d_loss:0.6532671302556992\n",
            "g_loss:[0.8468434, 0.8401359, 0.00335376]\n",
            "Batch:85\n",
            "d_loss_real:0.9083948731422424\n",
            "d_loss_fake:0.04225866496562958\n",
            "d_loss_wrong:0.6550414562225342\n",
            "d_loss:0.6285224705934525\n",
            "g_loss:[0.77727723, 0.7706332, 0.0033220134]\n",
            "Batch:86\n",
            "d_loss_real:0.9220011234283447\n",
            "d_loss_fake:0.018398193642497063\n",
            "d_loss_wrong:0.6632168292999268\n",
            "d_loss:0.631404310464859\n",
            "g_loss:[0.8301974, 0.82431173, 0.0029428434]\n",
            "Batch:87\n",
            "d_loss_real:0.9022848606109619\n",
            "d_loss_fake:0.03298879414796829\n",
            "d_loss_wrong:0.6621333956718445\n",
            "d_loss:0.624922975897789\n",
            "g_loss:[0.85937977, 0.8535211, 0.002929339]\n",
            "Batch:88\n",
            "d_loss_real:0.9645951986312866\n",
            "d_loss_fake:0.04662757366895676\n",
            "d_loss_wrong:0.6698463559150696\n",
            "d_loss:0.661416083574295\n",
            "g_loss:[0.80917615, 0.8032151, 0.0029805389]\n",
            "Batch:89\n",
            "d_loss_real:1.053452730178833\n",
            "d_loss_fake:0.051455896347761154\n",
            "d_loss_wrong:0.6800189018249512\n",
            "d_loss:0.7095950692892075\n",
            "g_loss:[0.752209, 0.74146646, 0.0053712693]\n",
            "Batch:90\n",
            "d_loss_real:0.9321984648704529\n",
            "d_loss_fake:0.02454639971256256\n",
            "d_loss_wrong:0.679601788520813\n",
            "d_loss:0.64213627576828\n",
            "g_loss:[0.79503113, 0.7863368, 0.0043471735]\n",
            "Batch:91\n",
            "d_loss_real:0.9324036836624146\n",
            "d_loss_fake:0.046312861144542694\n",
            "d_loss_wrong:0.698798418045044\n",
            "d_loss:0.6524796634912491\n",
            "g_loss:[0.7820437, 0.7745936, 0.003725042]\n",
            "Batch:92\n",
            "d_loss_real:1.0814192295074463\n",
            "d_loss_fake:0.13942036032676697\n",
            "d_loss_wrong:0.7233243584632874\n",
            "d_loss:0.7563957870006561\n",
            "g_loss:[1.0781718, 1.0675879, 0.0052920273]\n",
            "Batch:93\n",
            "d_loss_real:0.981501579284668\n",
            "d_loss_fake:0.03259740024805069\n",
            "d_loss_wrong:0.6277653574943542\n",
            "d_loss:0.6558414846658707\n",
            "g_loss:[1.0363108, 1.0280988, 0.004105961]\n",
            "Batch:94\n",
            "d_loss_real:0.9624676704406738\n",
            "d_loss_fake:0.03780899941921234\n",
            "d_loss_wrong:0.6764509677886963\n",
            "d_loss:0.6597988307476044\n",
            "g_loss:[0.92735076, 0.9198152, 0.0037677796]\n",
            "Batch:95\n",
            "d_loss_real:0.9744164943695068\n",
            "d_loss_fake:0.04210962727665901\n",
            "d_loss_wrong:0.677832841873169\n",
            "d_loss:0.6671938598155975\n",
            "g_loss:[0.7240229, 0.71694064, 0.0035411322]\n",
            "Batch:96\n",
            "d_loss_real:0.8652418851852417\n",
            "d_loss_fake:0.0552048534154892\n",
            "d_loss_wrong:0.6485457420349121\n",
            "d_loss:0.6085585951805115\n",
            "g_loss:[0.82573575, 0.81795204, 0.003891845]\n",
            "Batch:97\n",
            "d_loss_real:0.9596444368362427\n",
            "d_loss_fake:0.0732852965593338\n",
            "d_loss_wrong:0.6219955682754517\n",
            "d_loss:0.6536424309015274\n",
            "g_loss:[0.77483493, 0.7680216, 0.003406662]\n",
            "Batch:98\n",
            "d_loss_real:0.9328576326370239\n",
            "d_loss_fake:0.019470810890197754\n",
            "d_loss_wrong:0.7038408517837524\n",
            "d_loss:0.6472567319869995\n",
            "g_loss:[0.7975342, 0.7915422, 0.0029960372]\n",
            "Batch:99\n",
            "d_loss_real:0.8702381253242493\n",
            "d_loss_fake:0.01861046999692917\n",
            "d_loss_wrong:0.6412501335144043\n",
            "d_loss:0.6000842154026031\n",
            "g_loss:[0.79095894, 0.7848434, 0.0030577844]\n",
            "Batch:100\n",
            "d_loss_real:0.8943582773208618\n",
            "d_loss_fake:0.013982431963086128\n",
            "d_loss_wrong:0.615446925163269\n",
            "d_loss:0.6045364737510681\n",
            "g_loss:[0.7619675, 0.7552757, 0.0033458648]\n",
            "Batch:101\n",
            "d_loss_real:0.8650068044662476\n",
            "d_loss_fake:0.06211418658494949\n",
            "d_loss_wrong:0.6633307933807373\n",
            "d_loss:0.6138646453619003\n",
            "g_loss:[0.8254984, 0.8203279, 0.0025852653]\n",
            "Batch:102\n",
            "d_loss_real:0.9466861486434937\n",
            "d_loss_fake:0.0413411445915699\n",
            "d_loss_wrong:0.6342899799346924\n",
            "d_loss:0.6422508507966995\n",
            "g_loss:[0.7326708, 0.7266622, 0.0030042953]\n",
            "Batch:103\n",
            "d_loss_real:0.9424827098846436\n",
            "d_loss_fake:0.03813517838716507\n",
            "d_loss_wrong:0.6250630021095276\n",
            "d_loss:0.6370408982038498\n",
            "g_loss:[0.9146997, 0.90813076, 0.003284465]\n",
            "Batch:104\n",
            "d_loss_real:0.9650212526321411\n",
            "d_loss_fake:0.03017737716436386\n",
            "d_loss_wrong:0.6457731127738953\n",
            "d_loss:0.6514982432126999\n",
            "g_loss:[0.7511085, 0.74540764, 0.0028504576]\n",
            "Batch:105\n",
            "d_loss_real:0.9445100426673889\n",
            "d_loss_fake:0.022464849054813385\n",
            "d_loss_wrong:0.6970327496528625\n",
            "d_loss:0.6521294265985489\n",
            "g_loss:[0.7437492, 0.73939073, 0.002179224]\n",
            "Batch:106\n",
            "d_loss_real:0.8606966137886047\n",
            "d_loss_fake:0.01811317168176174\n",
            "d_loss_wrong:0.6705396771430969\n",
            "d_loss:0.6025115251541138\n",
            "g_loss:[0.7645182, 0.7597838, 0.0023671864]\n",
            "Batch:107\n",
            "d_loss_real:0.8633573055267334\n",
            "d_loss_fake:0.05615513026714325\n",
            "d_loss_wrong:0.6818063855171204\n",
            "d_loss:0.6161690354347229\n",
            "g_loss:[0.83032894, 0.82421786, 0.0030555553]\n",
            "Batch:108\n",
            "d_loss_real:0.9400420784950256\n",
            "d_loss_fake:0.03239187225699425\n",
            "d_loss_wrong:0.6776394248008728\n",
            "d_loss:0.6475288569927216\n",
            "g_loss:[0.9223541, 0.9171524, 0.0026008338]\n",
            "Batch:109\n",
            "d_loss_real:0.9447382688522339\n",
            "d_loss_fake:0.02282066084444523\n",
            "d_loss_wrong:0.6560965776443481\n",
            "d_loss:0.6420984417200089\n",
            "g_loss:[0.84240043, 0.8369975, 0.0027014567]\n",
            "Batch:110\n",
            "d_loss_real:0.9163719415664673\n",
            "d_loss_fake:0.021374858915805817\n",
            "d_loss_wrong:0.6582260131835938\n",
            "d_loss:0.628086194396019\n",
            "g_loss:[0.7613626, 0.75468963, 0.0033364948]\n",
            "Batch:111\n",
            "d_loss_real:0.8201077580451965\n",
            "d_loss_fake:0.016851268708705902\n",
            "d_loss_wrong:0.6839166283607483\n",
            "d_loss:0.5852458477020264\n",
            "g_loss:[0.75391245, 0.74778056, 0.0030659307]\n",
            "Batch:112\n",
            "d_loss_real:0.9472298622131348\n",
            "d_loss_fake:0.024397294968366623\n",
            "d_loss_wrong:0.6484420895576477\n",
            "d_loss:0.6418247818946838\n",
            "g_loss:[0.78634495, 0.7792975, 0.0035237344]\n",
            "Batch:113\n",
            "d_loss_real:0.8507810831069946\n",
            "d_loss_fake:0.036999642848968506\n",
            "d_loss_wrong:0.6766349077224731\n",
            "d_loss:0.6037991791963577\n",
            "g_loss:[0.90554976, 0.8990021, 0.0032738573]\n",
            "Batch:114\n",
            "d_loss_real:0.9113410711288452\n",
            "d_loss_fake:0.05443098768591881\n",
            "d_loss_wrong:0.6154078841209412\n",
            "d_loss:0.6231302469968796\n",
            "g_loss:[1.0006791, 0.99395794, 0.0033605814]\n",
            "Batch:115\n",
            "d_loss_real:0.9108372926712036\n",
            "d_loss_fake:0.08138110488653183\n",
            "d_loss_wrong:0.6139342188835144\n",
            "d_loss:0.6292474716901779\n",
            "g_loss:[1.0242838, 1.0183661, 0.0029588472]\n",
            "Batch:116\n",
            "d_loss_real:1.04997718334198\n",
            "d_loss_fake:0.018500393256545067\n",
            "d_loss_wrong:0.7084761261940002\n",
            "d_loss:0.7067327201366425\n",
            "g_loss:[0.854135, 0.8482231, 0.0029559468]\n",
            "Batch:117\n",
            "d_loss_real:0.9239185452461243\n",
            "d_loss_fake:0.1883125603199005\n",
            "d_loss_wrong:0.6069981455802917\n",
            "d_loss:0.6607869565486908\n",
            "g_loss:[1.1393892, 1.1342566, 0.0025662612]\n",
            "Batch:118\n",
            "d_loss_real:0.952911376953125\n",
            "d_loss_fake:0.05818307399749756\n",
            "d_loss_wrong:0.6255704164505005\n",
            "d_loss:0.647394061088562\n",
            "g_loss:[1.101866, 1.0970606, 0.0024026942]\n",
            "Batch:119\n",
            "d_loss_real:1.074965000152588\n",
            "d_loss_fake:0.1172776073217392\n",
            "d_loss_wrong:0.6020404100418091\n",
            "d_loss:0.7173120081424713\n",
            "g_loss:[0.95374626, 0.94944274, 0.0021517714]\n",
            "Batch:120\n",
            "d_loss_real:0.8942036628723145\n",
            "d_loss_fake:0.10965379327535629\n",
            "d_loss_wrong:0.6396604180335999\n",
            "d_loss:0.6344303786754608\n",
            "g_loss:[1.1018587, 1.0972416, 0.0023085303]\n",
            "Batch:121\n",
            "d_loss_real:1.0634430646896362\n",
            "d_loss_fake:0.185069739818573\n",
            "d_loss_wrong:0.6375977993011475\n",
            "d_loss:0.7373884171247482\n",
            "g_loss:[1.214167, 1.2092321, 0.00246746]\n",
            "Batch:122\n",
            "d_loss_real:1.0445952415466309\n",
            "d_loss_fake:0.2257535308599472\n",
            "d_loss_wrong:0.5715032815933228\n",
            "d_loss:0.7216118276119232\n",
            "g_loss:[1.3246962, 1.3198738, 0.002411197]\n",
            "Batch:123\n",
            "d_loss_real:1.1406253576278687\n",
            "d_loss_fake:0.11783379316329956\n",
            "d_loss_wrong:0.6545177698135376\n",
            "d_loss:0.7634005695581436\n",
            "g_loss:[1.0890274, 1.0846213, 0.002203037]\n",
            "Batch:124\n",
            "d_loss_real:1.0745075941085815\n",
            "d_loss_fake:0.20918625593185425\n",
            "d_loss_wrong:0.592004656791687\n",
            "d_loss:0.7375515252351761\n",
            "g_loss:[1.2949674, 1.2911193, 0.0019240121]\n",
            "Batch:125\n",
            "d_loss_real:0.97809898853302\n",
            "d_loss_fake:0.11809226870536804\n",
            "d_loss_wrong:0.5892217755317688\n",
            "d_loss:0.6658779978752136\n",
            "g_loss:[1.286035, 1.2813212, 0.002356876]\n",
            "Batch:126\n",
            "d_loss_real:1.078601598739624\n",
            "d_loss_fake:0.1207190752029419\n",
            "d_loss_wrong:0.682586669921875\n",
            "d_loss:0.7401272356510162\n",
            "g_loss:[1.2508701, 1.246594, 0.0021381048]\n",
            "Batch:127\n",
            "d_loss_real:1.005709171295166\n",
            "d_loss_fake:0.14263787865638733\n",
            "d_loss_wrong:0.5773468017578125\n",
            "d_loss:0.6828507483005524\n",
            "g_loss:[1.1156101, 1.1116581, 0.0019760053]\n",
            "Batch:128\n",
            "d_loss_real:1.3526489734649658\n",
            "d_loss_fake:0.1587304174900055\n",
            "d_loss_wrong:0.5325130820274353\n",
            "d_loss:0.8491353690624237\n",
            "g_loss:[1.0647881, 1.0573512, 0.0037184644]\n",
            "Batch:129\n",
            "d_loss_real:1.0132993459701538\n",
            "d_loss_fake:0.06982265412807465\n",
            "d_loss_wrong:0.589836835861206\n",
            "d_loss:0.6715645492076874\n",
            "g_loss:[0.868927, 0.8627792, 0.0030739137]\n",
            "Batch:130\n",
            "d_loss_real:1.02254056930542\n",
            "d_loss_fake:0.1170918196439743\n",
            "d_loss_wrong:0.6251891851425171\n",
            "d_loss:0.6968405395746231\n",
            "g_loss:[0.85970986, 0.85500175, 0.0023540622]\n",
            "Batch:131\n",
            "d_loss_real:1.0597281455993652\n",
            "d_loss_fake:0.07293085753917694\n",
            "d_loss_wrong:0.6775346994400024\n",
            "d_loss:0.7174804657697678\n",
            "g_loss:[0.95897895, 0.95242715, 0.0032758964]\n",
            "Batch:132\n",
            "d_loss_real:0.9172376990318298\n",
            "d_loss_fake:0.0790603831410408\n",
            "d_loss_wrong:0.6421611309051514\n",
            "d_loss:0.6389242261648178\n",
            "g_loss:[0.70422274, 0.6994054, 0.0024086933]\n",
            "Batch:133\n",
            "d_loss_real:0.8990969657897949\n",
            "d_loss_fake:0.01789563149213791\n",
            "d_loss_wrong:0.6549021601676941\n",
            "d_loss:0.6177479326725006\n",
            "g_loss:[0.9603576, 0.95671815, 0.0018197343]\n",
            "Batch:134\n",
            "d_loss_real:0.889754593372345\n",
            "d_loss_fake:0.020527899265289307\n",
            "d_loss_wrong:0.6641062498092651\n",
            "d_loss:0.6160358339548111\n",
            "g_loss:[0.99134004, 0.9870632, 0.0021384086]\n",
            "Batch:135\n",
            "d_loss_real:0.9922544956207275\n",
            "d_loss_fake:0.008445946499705315\n",
            "d_loss_wrong:0.7300887703895569\n",
            "d_loss:0.68076092004776\n",
            "g_loss:[0.99277985, 0.9892807, 0.0017495637]\n",
            "Batch:136\n",
            "d_loss_real:0.8286694288253784\n",
            "d_loss_fake:0.034109391272068024\n",
            "d_loss_wrong:0.6644709706306458\n",
            "d_loss:0.5889798104763031\n",
            "g_loss:[1.2544575, 1.2511593, 0.0016490768]\n",
            "Batch:137\n",
            "d_loss_real:0.9284083247184753\n",
            "d_loss_fake:0.062250006943941116\n",
            "d_loss_wrong:0.6168091893196106\n",
            "d_loss:0.6339689642190933\n",
            "g_loss:[0.8541089, 0.85043514, 0.0018368978]\n",
            "Batch:138\n",
            "d_loss_real:0.9186134934425354\n",
            "d_loss_fake:0.009272553026676178\n",
            "d_loss_wrong:0.6290346384048462\n",
            "d_loss:0.6188835501670837\n",
            "g_loss:[1.0867188, 1.0824685, 0.0021251328]\n",
            "========================================\n",
            "Epoch is: 4\n",
            "Number of batches 138\n",
            "Batch:1\n",
            "d_loss_real:0.9059931039810181\n",
            "d_loss_fake:0.017586681991815567\n",
            "d_loss_wrong:0.6708523631095886\n",
            "d_loss:0.6251063197851181\n",
            "g_loss:[0.79275525, 0.78891116, 0.0019220547]\n",
            "Batch:2\n",
            "d_loss_real:0.8457703590393066\n",
            "d_loss_fake:0.012903516180813313\n",
            "d_loss_wrong:0.6783614754676819\n",
            "d_loss:0.5957014262676239\n",
            "g_loss:[0.8334373, 0.82843417, 0.0025015816]\n",
            "Batch:3\n",
            "d_loss_real:1.022589921951294\n",
            "d_loss_fake:0.018584826961159706\n",
            "d_loss_wrong:0.6729055643081665\n",
            "d_loss:0.6841675639152527\n",
            "g_loss:[0.6610046, 0.655669, 0.0026678024]\n",
            "Batch:4\n",
            "d_loss_real:0.8704171776771545\n",
            "d_loss_fake:0.028330495581030846\n",
            "d_loss_wrong:0.6714572310447693\n",
            "d_loss:0.6101555228233337\n",
            "g_loss:[0.8236631, 0.8182757, 0.0026937171]\n",
            "Batch:5\n",
            "d_loss_real:0.8167617917060852\n",
            "d_loss_fake:0.015670660883188248\n",
            "d_loss_wrong:0.6704018115997314\n",
            "d_loss:0.57989901304245\n",
            "g_loss:[0.7735755, 0.7684814, 0.00254705]\n",
            "Batch:6\n",
            "d_loss_real:0.9035665988922119\n",
            "d_loss_fake:0.024470508098602295\n",
            "d_loss_wrong:0.6665120720863342\n",
            "d_loss:0.6245289444923401\n",
            "g_loss:[0.6686689, 0.66359043, 0.0025392603]\n",
            "Batch:7\n",
            "d_loss_real:0.8516359329223633\n",
            "d_loss_fake:0.010105234570801258\n",
            "d_loss_wrong:0.6775673031806946\n",
            "d_loss:0.5977361053228378\n",
            "g_loss:[0.6713845, 0.6665166, 0.0024339557]\n",
            "Batch:8\n",
            "d_loss_real:0.8932781219482422\n",
            "d_loss_fake:0.009974507614970207\n",
            "d_loss_wrong:0.6025649309158325\n",
            "d_loss:0.5997739136219025\n",
            "g_loss:[0.6443487, 0.6376413, 0.0033536789]\n",
            "Batch:9\n",
            "d_loss_real:1.082268238067627\n",
            "d_loss_fake:0.04712420701980591\n",
            "d_loss_wrong:0.6602678894996643\n",
            "d_loss:0.717982143163681\n",
            "g_loss:[0.798086, 0.7902489, 0.0039185593]\n",
            "Batch:10\n",
            "d_loss_real:0.9280688762664795\n",
            "d_loss_fake:0.07721121609210968\n",
            "d_loss_wrong:0.613211989402771\n",
            "d_loss:0.6366402357816696\n",
            "g_loss:[0.74583906, 0.7391103, 0.0033643814]\n",
            "Batch:11\n",
            "d_loss_real:1.0875614881515503\n",
            "d_loss_fake:0.038801562041044235\n",
            "d_loss_wrong:0.6678862571716309\n",
            "d_loss:0.7204526960849762\n",
            "g_loss:[0.7189559, 0.7145409, 0.002207486]\n",
            "Batch:12\n",
            "d_loss_real:0.8398423194885254\n",
            "d_loss_fake:0.053627438843250275\n",
            "d_loss_wrong:0.6381967663764954\n",
            "d_loss:0.592877209186554\n",
            "g_loss:[0.8614843, 0.8572774, 0.0021034623]\n",
            "Batch:13\n",
            "d_loss_real:0.8521207571029663\n",
            "d_loss_fake:0.02261456847190857\n",
            "d_loss_wrong:0.6362621784210205\n",
            "d_loss:0.590779572725296\n",
            "g_loss:[0.8296141, 0.8251098, 0.0022521496]\n",
            "Batch:14\n",
            "d_loss_real:0.9254088401794434\n",
            "d_loss_fake:0.014112809672951698\n",
            "d_loss_wrong:0.6355019211769104\n",
            "d_loss:0.6251081079244614\n",
            "g_loss:[0.754267, 0.7502492, 0.0020088907]\n",
            "Batch:15\n",
            "d_loss_real:0.8713712692260742\n",
            "d_loss_fake:0.00853913277387619\n",
            "d_loss_wrong:0.6875163912773132\n",
            "d_loss:0.6096995174884796\n",
            "g_loss:[0.6594235, 0.65329725, 0.0030631055]\n",
            "Batch:16\n",
            "d_loss_real:0.8187406659126282\n",
            "d_loss_fake:0.013718766160309315\n",
            "d_loss_wrong:0.6957852244377136\n",
            "d_loss:0.586746335029602\n",
            "g_loss:[0.82323253, 0.816556, 0.0033382631]\n",
            "Batch:17\n",
            "d_loss_real:0.86600661277771\n",
            "d_loss_fake:0.00465908320620656\n",
            "d_loss_wrong:0.6836196780204773\n",
            "d_loss:0.6050729900598526\n",
            "g_loss:[0.64674914, 0.6417591, 0.0024950309]\n",
            "Batch:18\n",
            "d_loss_real:0.8534051775932312\n",
            "d_loss_fake:0.011010900139808655\n",
            "d_loss_wrong:0.645986020565033\n",
            "d_loss:0.5909518152475357\n",
            "g_loss:[0.7565298, 0.75263536, 0.001947225]\n",
            "Batch:19\n",
            "d_loss_real:0.8662778735160828\n",
            "d_loss_fake:0.011011355556547642\n",
            "d_loss_wrong:0.6593787670135498\n",
            "d_loss:0.6007364690303802\n",
            "g_loss:[0.6881983, 0.68432546, 0.0019364334]\n",
            "Batch:20\n",
            "d_loss_real:0.8757683634757996\n",
            "d_loss_fake:0.014836209826171398\n",
            "d_loss_wrong:0.6806073188781738\n",
            "d_loss:0.6117450594902039\n",
            "g_loss:[0.6168131, 0.61228555, 0.0022637886]\n",
            "Batch:21\n",
            "d_loss_real:0.876549482345581\n",
            "d_loss_fake:0.011932969093322754\n",
            "d_loss_wrong:0.6071446537971497\n",
            "d_loss:0.5930441468954086\n",
            "g_loss:[0.68936265, 0.68556345, 0.001899599]\n",
            "Batch:22\n",
            "d_loss_real:0.8489695191383362\n",
            "d_loss_fake:0.014385029673576355\n",
            "d_loss_wrong:0.699351966381073\n",
            "d_loss:0.6029190123081207\n",
            "g_loss:[0.7187369, 0.7152481, 0.0017443816]\n",
            "Batch:23\n",
            "d_loss_real:0.8885449171066284\n",
            "d_loss_fake:0.026834171265363693\n",
            "d_loss_wrong:0.6383514404296875\n",
            "d_loss:0.6105688661336899\n",
            "g_loss:[1.0188016, 1.0149894, 0.0019060784]\n",
            "Batch:24\n",
            "d_loss_real:0.8644406795501709\n",
            "d_loss_fake:0.0008432301692664623\n",
            "d_loss_wrong:0.6348789930343628\n",
            "d_loss:0.5911508947610855\n",
            "g_loss:[1.2212876, 1.2162637, 0.002511972]\n",
            "Batch:25\n",
            "d_loss_real:0.9107706546783447\n",
            "d_loss_fake:0.02150968834757805\n",
            "d_loss_wrong:0.6712439656257629\n",
            "d_loss:0.6285737454891205\n",
            "g_loss:[1.2162762, 1.2109696, 0.0026532724]\n",
            "Batch:26\n",
            "d_loss_real:0.8284540176391602\n",
            "d_loss_fake:0.06229230761528015\n",
            "d_loss_wrong:0.636134922504425\n",
            "d_loss:0.5888338088989258\n",
            "g_loss:[1.292241, 1.2881281, 0.00205643]\n",
            "Batch:27\n",
            "d_loss_real:0.9405819177627563\n",
            "d_loss_fake:0.017694514244794846\n",
            "d_loss_wrong:0.7149856090545654\n",
            "d_loss:0.6534609943628311\n",
            "g_loss:[1.0938755, 1.089525, 0.0021752445]\n",
            "Batch:28\n",
            "d_loss_real:0.9223909378051758\n",
            "d_loss_fake:0.03127022087574005\n",
            "d_loss_wrong:0.5814239382743835\n",
            "d_loss:0.6143690049648285\n",
            "g_loss:[1.3222924, 1.3165798, 0.0028563042]\n",
            "Batch:29\n",
            "d_loss_real:0.8877822160720825\n",
            "d_loss_fake:0.03365597501397133\n",
            "d_loss_wrong:0.6289398074150085\n",
            "d_loss:0.6095400601625443\n",
            "g_loss:[1.04727, 1.0422428, 0.002513662]\n",
            "Batch:30\n",
            "d_loss_real:0.8821969032287598\n",
            "d_loss_fake:0.021798577159643173\n",
            "d_loss_wrong:0.7522226572036743\n",
            "d_loss:0.6346037536859512\n",
            "g_loss:[1.2208226, 1.2165802, 0.0021212064]\n",
            "Batch:31\n",
            "d_loss_real:0.9295328855514526\n",
            "d_loss_fake:0.019834280014038086\n",
            "d_loss_wrong:0.6315026879310608\n",
            "d_loss:0.627600684762001\n",
            "g_loss:[1.3591287, 1.353398, 0.0028653927]\n",
            "Batch:32\n",
            "d_loss_real:0.9873592257499695\n",
            "d_loss_fake:0.05930450186133385\n",
            "d_loss_wrong:0.5771217942237854\n",
            "d_loss:0.6527861803770065\n",
            "g_loss:[1.2106761, 1.2050198, 0.002828141]\n",
            "Batch:33\n",
            "d_loss_real:0.9254152774810791\n",
            "d_loss_fake:0.07941226661205292\n",
            "d_loss_wrong:0.591238796710968\n",
            "d_loss:0.6303704082965851\n",
            "g_loss:[1.8538383, 1.8472672, 0.0032856083]\n",
            "Batch:34\n",
            "d_loss_real:0.9710063934326172\n",
            "d_loss_fake:0.06228230893611908\n",
            "d_loss_wrong:0.5815609097480774\n",
            "d_loss:0.646464005112648\n",
            "g_loss:[1.2838525, 1.2777169, 0.0030678133]\n",
            "Batch:35\n",
            "d_loss_real:0.936781108379364\n",
            "d_loss_fake:0.039595458656549454\n",
            "d_loss_wrong:0.6264638304710388\n",
            "d_loss:0.6349053829908371\n",
            "g_loss:[1.1958709, 1.1899867, 0.0029420822]\n",
            "Batch:36\n",
            "d_loss_real:0.8674276471138\n",
            "d_loss_fake:0.021490680053830147\n",
            "d_loss_wrong:0.6419023871421814\n",
            "d_loss:0.5995620936155319\n",
            "g_loss:[1.283351, 1.2775348, 0.0029080403]\n",
            "Batch:37\n",
            "d_loss_real:0.8843445777893066\n",
            "d_loss_fake:0.01393640786409378\n",
            "d_loss_wrong:0.6766202449798584\n",
            "d_loss:0.6148114502429962\n",
            "g_loss:[1.0308516, 1.0256009, 0.0026253301]\n",
            "Batch:38\n",
            "d_loss_real:0.8755823373794556\n",
            "d_loss_fake:0.010732362046837807\n",
            "d_loss_wrong:0.6738585233688354\n",
            "d_loss:0.6089388877153397\n",
            "g_loss:[1.1062905, 1.101133, 0.0025787419]\n",
            "Batch:39\n",
            "d_loss_real:0.8387451171875\n",
            "d_loss_fake:0.020884819328784943\n",
            "d_loss_wrong:0.6673462986946106\n",
            "d_loss:0.5914303362369537\n",
            "g_loss:[1.1287318, 1.1233392, 0.0026963362]\n",
            "Batch:40\n",
            "d_loss_real:0.8868321180343628\n",
            "d_loss_fake:0.006865841336548328\n",
            "d_loss_wrong:0.7243199348449707\n",
            "d_loss:0.6262125074863434\n",
            "g_loss:[1.1466471, 1.1416211, 0.0025129619]\n",
            "Batch:41\n",
            "d_loss_real:0.8815420866012573\n",
            "d_loss_fake:0.010526634752750397\n",
            "d_loss_wrong:0.6511964201927185\n",
            "d_loss:0.6062018126249313\n",
            "g_loss:[1.0560889, 1.050239, 0.002924954]\n",
            "Batch:42\n",
            "d_loss_real:0.9233617782592773\n",
            "d_loss_fake:0.024431314319372177\n",
            "d_loss_wrong:0.6645867228507996\n",
            "d_loss:0.6339353919029236\n",
            "g_loss:[0.9924584, 0.98676586, 0.0028462824]\n",
            "Batch:43\n",
            "d_loss_real:1.1082895994186401\n",
            "d_loss_fake:0.18599069118499756\n",
            "d_loss_wrong:0.5407472848892212\n",
            "d_loss:0.7358292937278748\n",
            "g_loss:[1.4448764, 1.4389951, 0.0029406552]\n",
            "Batch:44\n",
            "d_loss_real:1.0787923336029053\n",
            "d_loss_fake:0.011571936309337616\n",
            "d_loss_wrong:0.6585912704467773\n",
            "d_loss:0.7069369703531265\n",
            "g_loss:[1.2503824, 1.2454784, 0.0024520268]\n",
            "Batch:45\n",
            "d_loss_real:0.9285237789154053\n",
            "d_loss_fake:0.03381718322634697\n",
            "d_loss_wrong:0.5902078747749329\n",
            "d_loss:0.6202681511640549\n",
            "g_loss:[1.2001318, 1.1946795, 0.0027261607]\n",
            "Batch:46\n",
            "d_loss_real:0.833419919013977\n",
            "d_loss_fake:0.023020725697278976\n",
            "d_loss_wrong:0.6681612730026245\n",
            "d_loss:0.5895054638385773\n",
            "g_loss:[1.238886, 1.2332692, 0.0028083785]\n",
            "Batch:47\n",
            "d_loss_real:0.8227360844612122\n",
            "d_loss_fake:0.020876329392194748\n",
            "d_loss_wrong:0.6787444949150085\n",
            "d_loss:0.5862732529640198\n",
            "g_loss:[1.4378839, 1.432405, 0.0027394146]\n",
            "Batch:48\n",
            "d_loss_real:0.9405065774917603\n",
            "d_loss_fake:0.03465483710169792\n",
            "d_loss_wrong:0.6393501162528992\n",
            "d_loss:0.6387545317411423\n",
            "g_loss:[1.1128922, 1.10713, 0.002881035]\n",
            "Batch:49\n",
            "d_loss_real:0.9125707149505615\n",
            "d_loss_fake:0.12140078097581863\n",
            "d_loss_wrong:0.5531717538833618\n",
            "d_loss:0.6249284893274307\n",
            "g_loss:[2.5427275, 2.5364423, 0.0031426281]\n",
            "Batch:50\n",
            "d_loss_real:1.0375041961669922\n",
            "d_loss_fake:0.10956520587205887\n",
            "d_loss_wrong:0.5871200561523438\n",
            "d_loss:0.6929234117269516\n",
            "g_loss:[2.0879405, 2.0817113, 0.0031145504]\n",
            "Batch:51\n",
            "d_loss_real:1.171074628829956\n",
            "d_loss_fake:0.256681889295578\n",
            "d_loss_wrong:0.7047398686408997\n",
            "d_loss:0.8258927464485168\n",
            "g_loss:[1.7375993, 1.7310714, 0.0032639662]\n",
            "Batch:52\n",
            "d_loss_real:0.8642711043357849\n",
            "d_loss_fake:0.28754064440727234\n",
            "d_loss_wrong:0.594551682472229\n",
            "d_loss:0.6526586413383484\n",
            "g_loss:[1.9550481, 1.9502857, 0.0023811846]\n",
            "Batch:53\n",
            "d_loss_real:1.0884087085723877\n",
            "d_loss_fake:0.21344690024852753\n",
            "d_loss_wrong:0.5511312484741211\n",
            "d_loss:0.7353488951921463\n",
            "g_loss:[1.8940707, 1.8874698, 0.00330046]\n",
            "Batch:54\n",
            "d_loss_real:1.0069235563278198\n",
            "d_loss_fake:0.11609841138124466\n",
            "d_loss_wrong:0.5334316492080688\n",
            "d_loss:0.6658442914485931\n",
            "g_loss:[1.9578301, 1.9524137, 0.0027081885]\n",
            "Batch:55\n",
            "d_loss_real:1.0087926387786865\n",
            "d_loss_fake:0.0652056559920311\n",
            "d_loss_wrong:0.5344966053962708\n",
            "d_loss:0.6543218791484833\n",
            "g_loss:[2.6870372, 2.6811588, 0.0029392508]\n",
            "Batch:56\n",
            "d_loss_real:1.3040781021118164\n",
            "d_loss_fake:0.1338478922843933\n",
            "d_loss_wrong:0.6320162415504456\n",
            "d_loss:0.8435050845146179\n",
            "g_loss:[1.594111, 1.5885751, 0.0027679447]\n",
            "Batch:57\n",
            "d_loss_real:0.9146947860717773\n",
            "d_loss_fake:0.1395387202501297\n",
            "d_loss_wrong:0.57427978515625\n",
            "d_loss:0.6358020156621933\n",
            "g_loss:[1.6592791, 1.6520903, 0.003594401]\n",
            "Batch:58\n",
            "d_loss_real:0.9389528632164001\n",
            "d_loss_fake:0.05789970979094505\n",
            "d_loss_wrong:0.5631321668624878\n",
            "d_loss:0.6247344017028809\n",
            "g_loss:[1.7815347, 1.7757438, 0.0028953857]\n",
            "Batch:59\n",
            "d_loss_real:1.0200603008270264\n",
            "d_loss_fake:0.03470933437347412\n",
            "d_loss_wrong:0.6062836647033691\n",
            "d_loss:0.670278400182724\n",
            "g_loss:[1.7859806, 1.7814176, 0.0022815054]\n",
            "Batch:60\n",
            "d_loss_real:0.9499104022979736\n",
            "d_loss_fake:0.05865801125764847\n",
            "d_loss_wrong:0.607370138168335\n",
            "d_loss:0.6414622366428375\n",
            "g_loss:[1.8054522, 1.8007474, 0.002352404]\n",
            "Batch:61\n",
            "d_loss_real:1.0444942712783813\n",
            "d_loss_fake:0.017391949892044067\n",
            "d_loss_wrong:0.5842919945716858\n",
            "d_loss:0.6726681292057037\n",
            "g_loss:[1.6646427, 1.6601555, 0.0022436078]\n",
            "Batch:62\n",
            "d_loss_real:0.9711366891860962\n",
            "d_loss_fake:0.040076665580272675\n",
            "d_loss_wrong:0.6152712106704712\n",
            "d_loss:0.6494053155183792\n",
            "g_loss:[1.3682145, 1.3642962, 0.00195916]\n",
            "Batch:63\n",
            "d_loss_real:0.970894455909729\n",
            "d_loss_fake:0.01803852617740631\n",
            "d_loss_wrong:0.6166561841964722\n",
            "d_loss:0.6441209018230438\n",
            "g_loss:[1.1815507, 1.1769496, 0.002300587]\n",
            "Batch:64\n",
            "d_loss_real:0.9309675097465515\n",
            "d_loss_fake:0.04070699214935303\n",
            "d_loss_wrong:0.5986454486846924\n",
            "d_loss:0.6253218650817871\n",
            "g_loss:[1.2042181, 1.1996572, 0.0022804877]\n",
            "Batch:65\n",
            "d_loss_real:0.9717560410499573\n",
            "d_loss_fake:0.05192328989505768\n",
            "d_loss_wrong:0.6969826221466064\n",
            "d_loss:0.6731044948101044\n",
            "g_loss:[1.1730379, 1.1679151, 0.0025614148]\n",
            "Batch:66\n",
            "d_loss_real:0.9010003805160522\n",
            "d_loss_fake:0.01624470204114914\n",
            "d_loss_wrong:0.6245325207710266\n",
            "d_loss:0.6106944978237152\n",
            "g_loss:[1.18621, 1.182091, 0.0020595426]\n",
            "Batch:67\n",
            "d_loss_real:1.0667724609375\n",
            "d_loss_fake:0.0647258535027504\n",
            "d_loss_wrong:0.61600261926651\n",
            "d_loss:0.7035683542490005\n",
            "g_loss:[1.0384398, 1.0346897, 0.0018750175]\n",
            "Batch:68\n",
            "d_loss_real:0.9388648271560669\n",
            "d_loss_fake:0.04033757001161575\n",
            "d_loss_wrong:0.6520724892616272\n",
            "d_loss:0.642534926533699\n",
            "g_loss:[1.0875989, 1.0832247, 0.0021871524]\n",
            "Batch:69\n",
            "d_loss_real:1.3236967325210571\n",
            "d_loss_fake:0.1163807213306427\n",
            "d_loss_wrong:0.8000510931015015\n",
            "d_loss:0.890956312417984\n",
            "g_loss:[0.92434376, 0.9197923, 0.0022757465]\n",
            "Batch:70\n",
            "d_loss_real:0.784213662147522\n",
            "d_loss_fake:0.19098177552223206\n",
            "d_loss_wrong:0.618025004863739\n",
            "d_loss:0.5943585336208344\n",
            "g_loss:[1.1988094, 1.1941898, 0.0023098078]\n",
            "Batch:71\n",
            "d_loss_real:1.0451935529708862\n",
            "d_loss_fake:0.09186086803674698\n",
            "d_loss_wrong:0.5366355180740356\n",
            "d_loss:0.6797208786010742\n",
            "g_loss:[1.2188568, 1.2144282, 0.002214312]\n",
            "Batch:72\n",
            "d_loss_real:0.9581687450408936\n",
            "d_loss_fake:0.06585919857025146\n",
            "d_loss_wrong:0.6115485429763794\n",
            "d_loss:0.6484363079071045\n",
            "g_loss:[1.2959677, 1.2921605, 0.0019035742]\n",
            "Batch:73\n",
            "d_loss_real:0.9003453254699707\n",
            "d_loss_fake:0.09969137609004974\n",
            "d_loss_wrong:0.5816132426261902\n",
            "d_loss:0.6204988211393356\n",
            "g_loss:[1.3467902, 1.3432279, 0.0017811407]\n",
            "Batch:74\n",
            "d_loss_real:1.0385442972183228\n",
            "d_loss_fake:0.008662363514304161\n",
            "d_loss_wrong:0.6111814975738525\n",
            "d_loss:0.6742331087589264\n",
            "g_loss:[1.3093376, 1.305701, 0.0018183112]\n",
            "Batch:75\n",
            "d_loss_real:0.9624543190002441\n",
            "d_loss_fake:0.029482247307896614\n",
            "d_loss_wrong:0.6217676401138306\n",
            "d_loss:0.6440396308898926\n",
            "g_loss:[1.1331973, 1.1297195, 0.0017388831]\n",
            "Batch:76\n",
            "d_loss_real:1.5927255153656006\n",
            "d_loss_fake:0.16379928588867188\n",
            "d_loss_wrong:0.6731012463569641\n",
            "d_loss:1.0055878907442093\n",
            "g_loss:[1.0268173, 1.0233352, 0.0017410801]\n",
            "Batch:77\n",
            "d_loss_real:0.8484389781951904\n",
            "d_loss_fake:0.14158210158348083\n",
            "d_loss_wrong:0.626147449016571\n",
            "d_loss:0.6161518692970276\n",
            "g_loss:[1.1593503, 1.1560822, 0.00163404]\n",
            "Batch:78\n",
            "d_loss_real:1.054650902748108\n",
            "d_loss_fake:0.05030784755945206\n",
            "d_loss_wrong:0.5807151794433594\n",
            "d_loss:0.6850812137126923\n",
            "g_loss:[1.0232077, 1.0198154, 0.0016960809]\n",
            "Batch:79\n",
            "d_loss_real:0.9156140089035034\n",
            "d_loss_fake:0.042769644409418106\n",
            "d_loss_wrong:0.624622642993927\n",
            "d_loss:0.624655082821846\n",
            "g_loss:[1.0207642, 1.0176395, 0.0015623386]\n",
            "Batch:80\n",
            "d_loss_real:0.8820791244506836\n",
            "d_loss_fake:0.035551078617572784\n",
            "d_loss_wrong:0.6536211371421814\n",
            "d_loss:0.6133326143026352\n",
            "g_loss:[1.068709, 1.0650955, 0.0018067238]\n",
            "Batch:81\n",
            "d_loss_real:0.8629770278930664\n",
            "d_loss_fake:0.036856163293123245\n",
            "d_loss_wrong:0.615654468536377\n",
            "d_loss:0.594616174697876\n",
            "g_loss:[1.0952063, 1.0916178, 0.0017942137]\n",
            "Batch:82\n",
            "d_loss_real:0.9928938150405884\n",
            "d_loss_fake:0.035427484661340714\n",
            "d_loss_wrong:0.60948246717453\n",
            "d_loss:0.6576744019985199\n",
            "g_loss:[1.0674987, 1.0641615, 0.0016685757]\n",
            "Batch:83\n",
            "d_loss_real:0.8726625442504883\n",
            "d_loss_fake:0.006258396431803703\n",
            "d_loss_wrong:0.6294873952865601\n",
            "d_loss:0.5952677130699158\n",
            "g_loss:[0.9911655, 0.98744154, 0.0018619845]\n",
            "Batch:84\n",
            "d_loss_real:0.8910399675369263\n",
            "d_loss_fake:0.05261601507663727\n",
            "d_loss_wrong:0.6449220180511475\n",
            "d_loss:0.619904488325119\n",
            "g_loss:[1.1185774, 1.115279, 0.0016492016]\n",
            "Batch:85\n",
            "d_loss_real:0.8924612998962402\n",
            "d_loss_fake:0.007745259907096624\n",
            "d_loss_wrong:0.6384700536727905\n",
            "d_loss:0.6077844798564911\n",
            "g_loss:[0.989961, 0.9866623, 0.0016493521]\n",
            "Batch:86\n",
            "d_loss_real:0.8800785541534424\n",
            "d_loss_fake:0.0192143302410841\n",
            "d_loss_wrong:0.6581434011459351\n",
            "d_loss:0.6093787103891373\n",
            "g_loss:[0.9552209, 0.9523854, 0.0014177395]\n",
            "Batch:87\n",
            "d_loss_real:0.8689107298851013\n",
            "d_loss_fake:0.012413712218403816\n",
            "d_loss_wrong:0.650557279586792\n",
            "d_loss:0.6001981198787689\n",
            "g_loss:[0.933239, 0.9299018, 0.0016686015]\n",
            "Batch:88\n",
            "d_loss_real:1.035660982131958\n",
            "d_loss_fake:0.04579709470272064\n",
            "d_loss_wrong:0.6695985794067383\n",
            "d_loss:0.696679413318634\n",
            "g_loss:[0.90558636, 0.90215075, 0.0017178196]\n",
            "Batch:89\n",
            "d_loss_real:0.9075733423233032\n",
            "d_loss_fake:0.035192087292671204\n",
            "d_loss_wrong:0.7866939306259155\n",
            "d_loss:0.659258171916008\n",
            "g_loss:[0.9624548, 0.9569838, 0.0027355086]\n",
            "Batch:90\n",
            "d_loss_real:0.8886878490447998\n",
            "d_loss_fake:0.0594891756772995\n",
            "d_loss_wrong:0.6530303359031677\n",
            "d_loss:0.622473806142807\n",
            "g_loss:[0.9648004, 0.9609044, 0.0019480089]\n",
            "Batch:91\n",
            "d_loss_real:0.9624297618865967\n",
            "d_loss_fake:0.03912916034460068\n",
            "d_loss_wrong:0.6605062484741211\n",
            "d_loss:0.6561237275600433\n",
            "g_loss:[1.0194858, 1.0160954, 0.001695219]\n",
            "Batch:92\n",
            "d_loss_real:1.000110387802124\n",
            "d_loss_fake:0.016937004402279854\n",
            "d_loss_wrong:0.770091712474823\n",
            "d_loss:0.6968123763799667\n",
            "g_loss:[0.96666205, 0.9610795, 0.002791297]\n",
            "Batch:93\n",
            "d_loss_real:0.8667503595352173\n",
            "d_loss_fake:0.011285293847322464\n",
            "d_loss_wrong:0.6590599417686462\n",
            "d_loss:0.6009614914655685\n",
            "g_loss:[0.9774388, 0.972877, 0.0022808951]\n",
            "Batch:94\n",
            "d_loss_real:0.9509008526802063\n",
            "d_loss_fake:0.011309817433357239\n",
            "d_loss_wrong:0.6769142746925354\n",
            "d_loss:0.647506445646286\n",
            "g_loss:[0.89927864, 0.8949405, 0.0021690847]\n",
            "Batch:95\n",
            "d_loss_real:0.8959398865699768\n",
            "d_loss_fake:0.06875000894069672\n",
            "d_loss_wrong:0.6297038197517395\n",
            "d_loss:0.6225834041833878\n",
            "g_loss:[0.9625457, 0.9588865, 0.0018296032]\n",
            "Batch:96\n",
            "d_loss_real:0.923495888710022\n",
            "d_loss_fake:0.03815815970301628\n",
            "d_loss_wrong:0.6227877736091614\n",
            "d_loss:0.6269844323396683\n",
            "g_loss:[0.9719715, 0.9673133, 0.0023291013]\n",
            "Batch:97\n",
            "d_loss_real:0.9258585572242737\n",
            "d_loss_fake:0.00824066810309887\n",
            "d_loss_wrong:0.645004153251648\n",
            "d_loss:0.6262404769659042\n",
            "g_loss:[0.9351442, 0.9309342, 0.0021050056]\n",
            "Batch:98\n",
            "d_loss_real:0.9491362571716309\n",
            "d_loss_fake:0.011757301166653633\n",
            "d_loss_wrong:0.6866635680198669\n",
            "d_loss:0.6491733491420746\n",
            "g_loss:[0.825045, 0.82155085, 0.0017470701]\n",
            "Batch:99\n",
            "d_loss_real:0.8242567181587219\n",
            "d_loss_fake:0.011184233240783215\n",
            "d_loss_wrong:0.671183168888092\n",
            "d_loss:0.5827202051877975\n",
            "g_loss:[0.83587193, 0.83180714, 0.0020323968]\n",
            "Batch:100\n",
            "d_loss_real:0.8352209329605103\n",
            "d_loss_fake:0.04718947783112526\n",
            "d_loss_wrong:0.6408693790435791\n",
            "d_loss:0.5896251797676086\n",
            "g_loss:[1.012602, 1.008646, 0.0019780025]\n",
            "Batch:101\n",
            "d_loss_real:0.9784246683120728\n",
            "d_loss_fake:0.005892360582947731\n",
            "d_loss_wrong:0.675022304058075\n",
            "d_loss:0.6594409942626953\n",
            "g_loss:[0.8172956, 0.8144935, 0.0014010537]\n",
            "Batch:102\n",
            "d_loss_real:0.8227386474609375\n",
            "d_loss_fake:0.051296573132276535\n",
            "d_loss_wrong:0.6751706600189209\n",
            "d_loss:0.592986136674881\n",
            "g_loss:[0.9044616, 0.90123725, 0.001612172]\n",
            "Batch:103\n",
            "d_loss_real:0.914635419845581\n",
            "d_loss_fake:0.02163766138255596\n",
            "d_loss_wrong:0.6491809487342834\n",
            "d_loss:0.625022366642952\n",
            "g_loss:[0.9386952, 0.93448085, 0.0021071802]\n",
            "Batch:104\n",
            "d_loss_real:0.9110317826271057\n",
            "d_loss_fake:0.004858117550611496\n",
            "d_loss_wrong:0.6700899600982666\n",
            "d_loss:0.6242529153823853\n",
            "g_loss:[0.83637315, 0.8331258, 0.0016236516]\n",
            "Batch:105\n",
            "d_loss_real:0.936363935470581\n",
            "d_loss_fake:0.03890760987997055\n",
            "d_loss_wrong:0.6717644929885864\n",
            "d_loss:0.6458499878644943\n",
            "g_loss:[0.78943425, 0.7867742, 0.0013300066]\n",
            "Batch:106\n",
            "d_loss_real:0.900762677192688\n",
            "d_loss_fake:0.0030985521152615547\n",
            "d_loss_wrong:0.6532138586044312\n",
            "d_loss:0.614459440112114\n",
            "g_loss:[0.832705, 0.829847, 0.0014290141]\n",
            "Batch:107\n",
            "d_loss_real:0.8989487290382385\n",
            "d_loss_fake:0.015049772337079048\n",
            "d_loss_wrong:0.7048958539962769\n",
            "d_loss:0.6294607669115067\n",
            "g_loss:[0.8425818, 0.83900386, 0.0017889756]\n",
            "Batch:108\n",
            "d_loss_real:0.8050113320350647\n",
            "d_loss_fake:0.005117642693221569\n",
            "d_loss_wrong:0.747333824634552\n",
            "d_loss:0.5906185358762741\n",
            "g_loss:[0.82671314, 0.8239435, 0.0013848135]\n",
            "Batch:109\n",
            "d_loss_real:0.8869165182113647\n",
            "d_loss_fake:0.020231951028108597\n",
            "d_loss_wrong:0.6654661297798157\n",
            "d_loss:0.6148827821016312\n",
            "g_loss:[0.8400436, 0.8370842, 0.0014797259]\n",
            "Batch:110\n",
            "d_loss_real:0.9058091044425964\n",
            "d_loss_fake:0.0076882848516106606\n",
            "d_loss_wrong:0.6494966745376587\n",
            "d_loss:0.6172007918357849\n",
            "g_loss:[0.84207404, 0.8382716, 0.0019012096]\n",
            "Batch:111\n",
            "d_loss_real:0.8575634360313416\n",
            "d_loss_fake:0.007574697025120258\n",
            "d_loss_wrong:0.6759604811668396\n",
            "d_loss:0.5996655076742172\n",
            "g_loss:[0.80084366, 0.7971747, 0.0018344952]\n",
            "Batch:112\n",
            "d_loss_real:0.8285986185073853\n",
            "d_loss_fake:0.007689593359827995\n",
            "d_loss_wrong:0.6738354563713074\n",
            "d_loss:0.5846805721521378\n",
            "g_loss:[0.7900138, 0.7852099, 0.0024019459]\n",
            "Batch:113\n",
            "d_loss_real:0.8521271347999573\n",
            "d_loss_fake:0.005019078496843576\n",
            "d_loss_wrong:0.6659696698188782\n",
            "d_loss:0.5938107520341873\n",
            "g_loss:[0.84999037, 0.8458139, 0.0020882576]\n",
            "Batch:114\n",
            "d_loss_real:0.8277015686035156\n",
            "d_loss_fake:0.0031510507687926292\n",
            "d_loss_wrong:0.6469594836235046\n",
            "d_loss:0.5763784199953079\n",
            "g_loss:[0.7690714, 0.7650491, 0.0020111506]\n",
            "Batch:115\n",
            "d_loss_real:0.8845583200454712\n",
            "d_loss_fake:0.007757825311273336\n",
            "d_loss_wrong:0.6353172659873962\n",
            "d_loss:0.6030479371547699\n",
            "g_loss:[0.76521415, 0.76136845, 0.0019228557]\n",
            "Batch:116\n",
            "d_loss_real:0.8769978880882263\n",
            "d_loss_fake:0.003908622544258833\n",
            "d_loss_wrong:0.6714808940887451\n",
            "d_loss:0.6073463261127472\n",
            "g_loss:[0.8082484, 0.8045226, 0.0018629087]\n",
            "Batch:117\n",
            "d_loss_real:0.876465916633606\n",
            "d_loss_fake:0.01113461796194315\n",
            "d_loss_wrong:0.672507643699646\n",
            "d_loss:0.6091435253620148\n",
            "g_loss:[0.7279602, 0.72459245, 0.0016838992]\n",
            "Batch:118\n",
            "d_loss_real:0.8684879541397095\n",
            "d_loss_fake:0.00941358134150505\n",
            "d_loss_wrong:0.6426162719726562\n",
            "d_loss:0.5972514450550079\n",
            "g_loss:[0.72839624, 0.7250998, 0.0016482045]\n",
            "Batch:119\n",
            "d_loss_real:0.8615777492523193\n",
            "d_loss_fake:0.0052053844556212425\n",
            "d_loss_wrong:0.672174870967865\n",
            "d_loss:0.600133940577507\n",
            "g_loss:[0.7695396, 0.7665434, 0.001498108]\n",
            "Batch:120\n",
            "d_loss_real:0.8893747329711914\n",
            "d_loss_fake:0.013609090819954872\n",
            "d_loss_wrong:0.6379928588867188\n",
            "d_loss:0.6075878590345383\n",
            "g_loss:[0.728895, 0.72564954, 0.0016227376]\n",
            "Batch:121\n",
            "d_loss_real:0.8824796676635742\n",
            "d_loss_fake:0.010538500733673573\n",
            "d_loss_wrong:0.6879210472106934\n",
            "d_loss:0.6158547252416611\n",
            "g_loss:[0.6987734, 0.69527054, 0.001751435]\n",
            "Batch:122\n",
            "d_loss_real:0.8203207850456238\n",
            "d_loss_fake:0.003912180662155151\n",
            "d_loss_wrong:0.6754046082496643\n",
            "d_loss:0.5799895823001862\n",
            "g_loss:[0.70027775, 0.6968727, 0.0017025231]\n",
            "Batch:123\n",
            "d_loss_real:0.8874155282974243\n",
            "d_loss_fake:0.007424758281558752\n",
            "d_loss_wrong:0.7109095454216003\n",
            "d_loss:0.6232913434505463\n",
            "g_loss:[0.7797966, 0.7766105, 0.0015930496]\n",
            "Batch:124\n",
            "d_loss_real:0.8878344893455505\n",
            "d_loss_fake:0.0033813079353421926\n",
            "d_loss_wrong:0.6233653426170349\n",
            "d_loss:0.6006039083003998\n",
            "g_loss:[0.7162684, 0.71344674, 0.0014108338]\n",
            "Batch:125\n",
            "d_loss_real:0.8594202995300293\n",
            "d_loss_fake:0.014116750098764896\n",
            "d_loss_wrong:0.6458882689476013\n",
            "d_loss:0.5947114080190659\n",
            "g_loss:[0.63924515, 0.6355423, 0.0018514269]\n",
            "Batch:126\n",
            "d_loss_real:0.8691070079803467\n",
            "d_loss_fake:0.007979911752045155\n",
            "d_loss_wrong:0.6481491327285767\n",
            "d_loss:0.598585769534111\n",
            "g_loss:[0.7350468, 0.73155046, 0.0017481813]\n",
            "Batch:127\n",
            "d_loss_real:0.8545098304748535\n",
            "d_loss_fake:0.005633213557302952\n",
            "d_loss_wrong:0.6659455299377441\n",
            "d_loss:0.5951496064662933\n",
            "g_loss:[0.6696166, 0.6665753, 0.001520627]\n",
            "Batch:128\n",
            "d_loss_real:0.9116396903991699\n",
            "d_loss_fake:0.0030164350755512714\n",
            "d_loss_wrong:0.7067571878433228\n",
            "d_loss:0.6332632452249527\n",
            "g_loss:[0.62069535, 0.6164537, 0.0021208108]\n",
            "Batch:129\n",
            "d_loss_real:0.8519608974456787\n",
            "d_loss_fake:0.014139040373265743\n",
            "d_loss_wrong:0.657978355884552\n",
            "d_loss:0.5940098017454147\n",
            "g_loss:[0.63140225, 0.6272807, 0.0020607738]\n",
            "Batch:130\n",
            "d_loss_real:0.8629131317138672\n",
            "d_loss_fake:0.008426493033766747\n",
            "d_loss_wrong:0.6654231548309326\n",
            "d_loss:0.5999189764261246\n",
            "g_loss:[0.62009823, 0.617121, 0.0014886195]\n",
            "Batch:131\n",
            "d_loss_real:0.8908757567405701\n",
            "d_loss_fake:0.007812118157744408\n",
            "d_loss_wrong:0.6950064897537231\n",
            "d_loss:0.6211425364017487\n",
            "g_loss:[0.6709202, 0.6669727, 0.001973757]\n",
            "Batch:132\n",
            "d_loss_real:0.8378177881240845\n",
            "d_loss_fake:0.00715268449857831\n",
            "d_loss_wrong:0.6610337495803833\n",
            "d_loss:0.5859555006027222\n",
            "g_loss:[0.61700666, 0.6134987, 0.0017539951]\n",
            "Batch:133\n",
            "d_loss_real:0.8618988394737244\n",
            "d_loss_fake:0.0070620314218103886\n",
            "d_loss_wrong:0.6557910442352295\n",
            "d_loss:0.5966626852750778\n",
            "g_loss:[0.6534153, 0.65057945, 0.0014179482]\n",
            "Batch:134\n",
            "d_loss_real:0.8627040386199951\n",
            "d_loss_fake:0.004937114659696817\n",
            "d_loss_wrong:0.6401490569114685\n",
            "d_loss:0.5926235616207123\n",
            "g_loss:[0.6208099, 0.61773515, 0.0015373878]\n",
            "Batch:135\n",
            "d_loss_real:0.8354454040527344\n",
            "d_loss_fake:0.007265996187925339\n",
            "d_loss_wrong:0.6736772656440735\n",
            "d_loss:0.5879585146903992\n",
            "g_loss:[0.616832, 0.61424506, 0.001293473]\n",
            "Batch:136\n",
            "d_loss_real:0.8527191877365112\n",
            "d_loss_fake:0.007533098105341196\n",
            "d_loss_wrong:0.6586688160896301\n",
            "d_loss:0.5929100662469864\n",
            "g_loss:[0.63460934, 0.63207257, 0.0012683957]\n",
            "Batch:137\n",
            "d_loss_real:0.8749335408210754\n",
            "d_loss_fake:0.005213962867856026\n",
            "d_loss_wrong:0.6407694220542908\n",
            "d_loss:0.5989626199007034\n",
            "g_loss:[0.568772, 0.5661146, 0.0013286949]\n",
            "Batch:138\n",
            "d_loss_real:0.821735143661499\n",
            "d_loss_fake:0.011696672067046165\n",
            "d_loss_wrong:0.6878284215927124\n",
            "d_loss:0.5857488512992859\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "g_loss:[0.60858697, 0.6053959, 0.0015955318]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4dvOLeCxRhc"
      },
      "source": [
        "import os\r\n",
        "import pickle\r\n",
        "import random\r\n",
        "import time\r\n",
        "\r\n",
        "import PIL\r\n",
        "import numpy as np\r\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfrsiozOxROr",
        "outputId": "ed6abdfa-6a7c-419d-b88a-dd11ade7ffdb"
      },
      "source": [
        "print(tf.version)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<module 'tensorflow._api.v1.version' from '/tensorflow-1.15.2/python3.6/tensorflow_core/_api/v1/version/__init__.py'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79dtjcQExQ8b"
      },
      "source": [
        "%tensorflow_version 1.x\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jn5F7h-ZxaBi"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from PIL import Image\r\n",
        "from keras import Input, Model\r\n",
        "from keras import backend as K\r\n",
        "from keras.callbacks import TensorBoard\r\n",
        "from keras.layers import Dense, LeakyReLU, BatchNormalization, ReLU, Reshape, UpSampling2D, Conv2D, Activation, \\\r\n",
        "    concatenate, Flatten, Lambda, Concatenate, ZeroPadding2D\r\n",
        "from keras.layers import add\r\n",
        "from keras.optimizers import Adam\r\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdaLLilGxfy9",
        "outputId": "4a8f38ef-19fd-4507-b587-a4ab0f0b7af4"
      },
      "source": [
        "print(tf.version)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<module 'tensorflow._api.v1.version' from '/tensorflow-1.15.2/python3.6/tensorflow_core/_api/v1/version/__init__.py'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdGpNlFoGkRK"
      },
      "source": [
        "def build_ca_model():\n",
        "    \"\"\"\n",
        "    Get conditioning augmentation model.\n",
        "    Takes an embedding of shape (1024,) and returns a tensor of shape (256,)\n",
        "    \"\"\"\n",
        "    input_layer = Input(shape=(1024,))\n",
        "    x = Dense(256)(input_layer)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    model = Model(inputs=[input_layer], outputs=[x])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXufneCKGqAP"
      },
      "source": [
        "def build_embedding_compressor_model():\n",
        "    \"\"\"\n",
        "    Build embedding compressor model\n",
        "    \"\"\"\n",
        "    input_layer = Input(shape=(1024,))\n",
        "    x = Dense(128)(input_layer)\n",
        "    x = ReLU()(x)\n",
        "    model = Model(inputs=[input_layer], outputs=[x])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CMnFMqpQzBc2",
        "outputId": "ac05bdb5-ac58-4274-b774-2c48ba63465d"
      },
      "source": [
        "pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/project'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_XGxN2SGp6e"
      },
      "source": [
        "def generate_c(x):\n",
        "    mean = x[:, :128]\n",
        "    log_sigma = x[:, 128:]\n",
        "\n",
        "    stddev = K.exp(log_sigma)\n",
        "    epsilon = K.random_normal(shape=K.constant((mean.shape[1],), dtype='int32'))\n",
        "    c = stddev * epsilon + mean\n",
        "\n",
        "    return c"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPhoM1-SGvnI"
      },
      "source": [
        "def build_stage1_generator():\n",
        "    \"\"\"\n",
        "    Builds a generator model used in Stage-I\n",
        "    \"\"\"\n",
        "    input_layer = Input(shape=(1024,))\n",
        "    x = Dense(256)(input_layer)\n",
        "    mean_logsigma = LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    c = Lambda(generate_c)(mean_logsigma)\n",
        "\n",
        "    input_layer2 = Input(shape=(100,))\n",
        "\n",
        "    gen_input = Concatenate(axis=1)([c, input_layer2])\n",
        "\n",
        "    x = Dense(128 * 8 * 4 * 4, use_bias=False)(gen_input)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    x = Reshape((4, 4, 128 * 8), input_shape=(128 * 8 * 4 * 4,))(x)\n",
        "\n",
        "    x = UpSampling2D(size=(2, 2))(x)\n",
        "    x = Conv2D(512, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    x = UpSampling2D(size=(2, 2))(x)\n",
        "    x = Conv2D(256, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    x = UpSampling2D(size=(2, 2))(x)\n",
        "    x = Conv2D(128, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    x = UpSampling2D(size=(2, 2))(x)\n",
        "    x = Conv2D(64, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    x = Conv2D(3, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n",
        "    x = Activation(activation='tanh')(x)\n",
        "\n",
        "    stage1_gen = Model(inputs=[input_layer, input_layer2], outputs=[x, mean_logsigma])\n",
        "    return stage1_gen\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rVtHb8HGwEw"
      },
      "source": [
        "def residual_block(input):\n",
        "    \"\"\"\n",
        "    Residual block in the generator network\n",
        "    \"\"\"\n",
        "    x = Conv2D(128 * 4, kernel_size=(3, 3), padding='same', strides=1)(input)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    x = Conv2D(128 * 4, kernel_size=(3, 3), strides=1, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = add([x, input])\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBOvO3mWGvZv"
      },
      "source": [
        "def joint_block(inputs):\n",
        "    c = inputs[0]\n",
        "    x = inputs[1]\n",
        "\n",
        "    c = K.expand_dims(c, axis=1)\n",
        "    c = K.expand_dims(c, axis=1)\n",
        "    c = K.tile(c, [1, 16, 16, 1])\n",
        "    return K.concatenate([c, x], axis=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PplKJFfnHBh4"
      },
      "source": [
        "def build_stage2_generator():\n",
        "    \"\"\"\n",
        "    Create Stage-II generator containing the CA Augmentation Network,\n",
        "    the image encoder and the generator network\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. CA Augmentation Network\n",
        "    input_layer = Input(shape=(1024,))\n",
        "    input_lr_images = Input(shape=(64, 64, 3))\n",
        "\n",
        "    ca = Dense(256)(input_layer)\n",
        "    mean_logsigma = LeakyReLU(alpha=0.2)(ca)\n",
        "    c = Lambda(generate_c)(mean_logsigma)\n",
        "\n",
        "    # 2. Image Encoder\n",
        "    x = ZeroPadding2D(padding=(1, 1))(input_lr_images)\n",
        "    x = Conv2D(128, kernel_size=(3, 3), strides=1, use_bias=False)(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    x = ZeroPadding2D(padding=(1, 1))(x)\n",
        "    x = Conv2D(256, kernel_size=(4, 4), strides=2, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    x = ZeroPadding2D(padding=(1, 1))(x)\n",
        "    x = Conv2D(512, kernel_size=(4, 4), strides=2, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    # 3. Joint\n",
        "    c_code = Lambda(joint_block)([c, x])\n",
        "\n",
        "    x = ZeroPadding2D(padding=(1, 1))(c_code)\n",
        "    x = Conv2D(512, kernel_size=(3, 3), strides=1, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    # 4. Residual blocks\n",
        "    x = residual_block(x)\n",
        "    x = residual_block(x)\n",
        "    x = residual_block(x)\n",
        "    x = residual_block(x)\n",
        "\n",
        "    # 5. Upsampling blocks\n",
        "    x = UpSampling2D(size=(2, 2))(x)\n",
        "    x = Conv2D(512, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    x = UpSampling2D(size=(2, 2))(x)\n",
        "    x = Conv2D(256, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    x = UpSampling2D(size=(2, 2))(x)\n",
        "    x = Conv2D(128, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    x = UpSampling2D(size=(2, 2))(x)\n",
        "    x = Conv2D(64, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    x = Conv2D(3, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n",
        "    x = Activation('tanh')(x)\n",
        "\n",
        "    model = Model(inputs=[input_layer, input_lr_images], outputs=[x, mean_logsigma])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fh1H9qfeHBoX"
      },
      "source": [
        "def build_stage2_discriminator():\n",
        "    \"\"\"\n",
        "    Create Stage-II discriminator network\n",
        "    \"\"\"\n",
        "    input_layer = Input(shape=(256, 256, 3))\n",
        "\n",
        "    x = Conv2D(64, (4, 4), padding='same', strides=2, input_shape=(256, 256, 3), use_bias=False)(input_layer)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    x = Conv2D(128, (4, 4), padding='same', strides=2, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    x = Conv2D(256, (4, 4), padding='same', strides=2, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    x = Conv2D(512, (4, 4), padding='same', strides=2, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    x = Conv2D(1024, (4, 4), padding='same', strides=2, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    x = Conv2D(2048, (4, 4), padding='same', strides=2, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    x = Conv2D(1024, (1, 1), padding='same', strides=1, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    x = Conv2D(512, (1, 1), padding='same', strides=1, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x2 = Conv2D(128, (1, 1), padding='same', strides=1, use_bias=False)(x)\n",
        "    x2 = BatchNormalization()(x2)\n",
        "    x2 = LeakyReLU(alpha=0.2)(x2)\n",
        "\n",
        "    x2 = Conv2D(128, (3, 3), padding='same', strides=1, use_bias=False)(x2)\n",
        "    x2 = BatchNormalization()(x2)\n",
        "    x2 = LeakyReLU(alpha=0.2)(x2)\n",
        "\n",
        "    x2 = Conv2D(512, (3, 3), padding='same', strides=1, use_bias=False)(x2)\n",
        "    x2 = BatchNormalization()(x2)\n",
        "\n",
        "    \n",
        "    \n",
        "    added_x = add([x, x2])\n",
        "    added_x = LeakyReLU(alpha=0.2)(added_x)\n",
        "\n",
        "    input_layer2 = Input(shape=(4, 4, 128))\n",
        "\n",
        "    merged_input = concatenate([added_x, input_layer2])\n",
        "\n",
        "\n",
        "    x3 = Conv2D(64 * 8, kernel_size=1, padding=\"same\", strides=1)(merged_input)\n",
        "    x3 = BatchNormalization()(x3)\n",
        "    x3 = LeakyReLU(alpha=0.2)(x3)\n",
        "    x3 = Flatten()(x3)\n",
        "    x3 = Dense(1)(x3)\n",
        "    x3 = Activation('sigmoid')(x3)\n",
        "\n",
        "    stage2_dis = Model(inputs=[input_layer, input_layer2], outputs=[x3])\n",
        "    return stage2_dis\n",
        "                "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8aEaoqXHBlV"
      },
      "source": [
        "def build_adversarial_model(gen_model2, dis_model, gen_model1):\n",
        "    \"\"\"\n",
        "    Create adversarial model\n",
        "    \"\"\"\n",
        "    embeddings_input_layer = Input(shape=(1024, ))\n",
        "    noise_input_layer = Input(shape=(100, ))\n",
        "    compressed_embedding_input_layer = Input(shape=(4, 4, 128))\n",
        "\n",
        "    gen_model1.trainable = False\n",
        "    dis_model.trainable = False\n",
        "\n",
        "    lr_images, mean_logsigma1 = gen_model1([embeddings_input_layer, noise_input_layer])\n",
        "    hr_images, mean_logsigma2 = gen_model2([embeddings_input_layer, lr_images])\n",
        "    valid = dis_model([hr_images, compressed_embedding_input_layer])\n",
        "\n",
        "    model = Model(inputs=[embeddings_input_layer, noise_input_layer, compressed_embedding_input_layer], outputs=[valid, mean_logsigma2])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddxNPxt1Hqkf"
      },
      "source": [
        "\"\"\"\n",
        "Dataset loading related methods\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def load_class_ids(class_info_file_path):\n",
        "    \"\"\"\n",
        "    Load class ids from class_info.pickle file\n",
        "    \"\"\"\n",
        "    with open(class_info_file_path, 'rb') as f:\n",
        "        class_ids = pickle.load(f, encoding='latin1')\n",
        "        return class_ids\n",
        "\n",
        "\n",
        "def load_embeddings(embeddings_file_path):\n",
        "    \"\"\"\n",
        "    Function to load embeddings\n",
        "    \"\"\"\n",
        "    with open(embeddings_file_path, 'rb') as f:\n",
        "        embeddings = pickle.load(f, encoding='latin1')\n",
        "        embeddings = np.array(embeddings)\n",
        "        print('embeddings: ', embeddings.shape)\n",
        "    return embeddings\n",
        "\n",
        "\n",
        "def load_filenames(filenames_file_path):\n",
        "    \"\"\"\n",
        "    Load filenames.pickle file and return a list of all file names\n",
        "    \"\"\"\n",
        "    with open(filenames_file_path, 'rb') as f:\n",
        "        filenames = pickle.load(f, encoding='latin1')\n",
        "    return filenames\n",
        "def load_bounding_boxes(dataset_dir):\n",
        "    \"\"\"\n",
        "    Load bounding boxes and return a dictionary of file names and corresponding bounding boxes\n",
        "    \"\"\"\n",
        "    # Paths\n",
        "    bounding_boxes_path = os.path.join(dataset_dir, 'bounding_boxes.txt')\n",
        "    file_paths_path = os.path.join(dataset_dir, 'images.txt')\n",
        "\n",
        "    # Read bounding_boxes.txt and images.txt file\n",
        "    df_bounding_boxes = pd.read_csv(bounding_boxes_path,\n",
        "                                    delim_whitespace=True, header=None).astype(int)\n",
        "    df_file_names = pd.read_csv(file_paths_path, delim_whitespace=True, header=None)\n",
        "\n",
        "    # Create a list of file names\n",
        "    file_names = df_file_names[1].tolist()\n",
        "\n",
        "    # Create a dictionary of file_names and bounding boxes\n",
        "    filename_boundingbox_dict = {img_file[:-4]: [] for img_file in file_names[:2]}\n",
        "\n",
        "    # Assign a bounding box to the corresponding image\n",
        "    for i in range(0, len(file_names)):\n",
        "        # Get the bounding box\n",
        "        bounding_box = df_bounding_boxes.iloc[i][1:].tolist()\n",
        "        key = file_names[i][:-4]\n",
        "        filename_boundingbox_dict[key] = bounding_box\n",
        "\n",
        "    return filename_boundingbox_dict\n",
        "\n",
        "def get_img(img_path, bbox, image_size):\n",
        "    \"\"\"\n",
        "    Load and resize images\n",
        "    \"\"\"\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    width, height = img.size\n",
        "    if bbox is not None:\n",
        "        R = int(np.maximum(bbox[2], bbox[3]) * 0.75)\n",
        "        center_x = int((2 * bbox[0] + bbox[2]) / 2)\n",
        "        center_y = int((2 * bbox[1] + bbox[3]) / 2)\n",
        "        y1 = np.maximum(0, center_y - R)\n",
        "        y2 = np.minimum(height, center_y + R)\n",
        "        x1 = np.maximum(0, center_x - R)\n",
        "        x2 = np.minimum(width, center_x + R)\n",
        "        img = img.crop([x1, y1, x2, y2])\n",
        "    img = img.resize(image_size, PIL.Image.BILINEAR)\n",
        "    return img\n",
        "\n",
        "\n",
        "def load_dataset(filenames_file_path, class_info_file_path, cub_dataset_dir, embeddings_file_path, image_size):\n",
        "    filenames = load_filenames(filenames_file_path)\n",
        "    class_ids = load_class_ids(class_info_file_path)\n",
        "    bounding_boxes = load_bounding_boxes(cub_dataset_dir)\n",
        "    all_embeddings = load_embeddings(embeddings_file_path)\n",
        "\n",
        "    X, y, embeddings = [], [], []\n",
        "\n",
        "    print(\"All embeddings shape:\", all_embeddings.shape)\n",
        "\n",
        "    for index, filename in enumerate(filenames):\n",
        "        bounding_box = bounding_boxes[filename]\n",
        "\n",
        "        try:\n",
        "            # Load images\n",
        "            img_name = '{}/images/{}.jpg'.format(cub_dataset_dir, filename)\n",
        "            img = get_img(img_name, bounding_box, image_size)\n",
        "\n",
        "            all_embeddings1 = all_embeddings[index, :, :]\n",
        "\n",
        "            embedding_ix = random.randint(0, all_embeddings1.shape[0] - 1)\n",
        "            embedding = all_embeddings1[embedding_ix, :]\n",
        "\n",
        "            X.append(np.array(img))\n",
        "            y.append(class_ids[index])\n",
        "            embeddings.append(embedding)\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    embeddings = np.array(embeddings)\n",
        "\n",
        "    return X, y, embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwtDQbb6HqSq"
      },
      "source": [
        "\"\"\"\n",
        "Loss functions\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def KL_loss(y_true, y_pred):\n",
        "    mean = y_pred[:, :128]\n",
        "    logsigma = y_pred[:, :128]\n",
        "    loss = -logsigma + .5 * (-1 + K.exp(2. * logsigma) + K.square(mean))\n",
        "    loss = K.mean(loss)\n",
        "    return loss\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3B5UH_olyLcU"
      },
      "source": [
        "def custom_generator_loss(y_true, y_pred):\r\n",
        "    # Calculate binary cross entropy loss\r\n",
        "    return K.binary_crossentropy(y_true, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLZGWu-SyN54"
      },
      "source": [
        "def write_log(callback, name, loss, batch_no):\r\n",
        "    \"\"\"\r\n",
        "    Write training summary to TensorBoard\r\n",
        "    \"\"\"\r\n",
        "    summary = tf.Summary()\r\n",
        "    summary_value = summary.value.add()\r\n",
        "    summary_value.simple_value = loss\r\n",
        "    summary_value.tag = name\r\n",
        "    callback.writer.add_summary(summary, batch_no)\r\n",
        "    callback.writer.flush()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yx9muDRoyLG2"
      },
      "source": [
        "def save_rgb_img(img, path):\r\n",
        "    \"\"\"\r\n",
        "    Save an rgb image\r\n",
        "    \"\"\"\r\n",
        "    fig = plt.figure()\r\n",
        "    ax = fig.add_subplot(1, 1, 1)\r\n",
        "    ax.imshow(img)\r\n",
        "    ax.axis(\"off\")\r\n",
        "    ax.set_title(\"Image\")\r\n",
        "    plt.savefig(path)\r\n",
        "    plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dr68gpdgI_R6",
        "outputId": "3124b50c-e113-41ea-bd1c-93fdd4dfa432"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    data_dir = \"/content/drive/MyDrive/project/birds\"\n",
        "    train_dir = data_dir + \"/train\"\n",
        "    test_dir = data_dir + \"/test\"\n",
        "    hr_image_size = (256, 256)\n",
        "    lr_image_size = (64, 64)\n",
        "    batch_size = 32\n",
        "    z_dim = 100\n",
        "    stage1_generator_lr = 0.0002\n",
        "    stage1_discriminator_lr = 0.0002\n",
        "    stage1_lr_decay_step = 600\n",
        "    epochs = 2\n",
        "    condition_dim = 128\n",
        "\n",
        "    embeddings_file_path_train = train_dir + \"/char-CNN-RNN-embeddings.pickle\"\n",
        "    embeddings_file_path_test = test_dir + \"/char-CNN-RNN-embeddings.pickle\"\n",
        "\n",
        "    filenames_file_path_train = train_dir + \"/filenames.pickle\"\n",
        "    filenames_file_path_test = test_dir + \"/filenames.pickle\"\n",
        "\n",
        "    class_info_file_path_train = train_dir + \"/class_info.pickle\"\n",
        "    class_info_file_path_test = test_dir + \"/class_info.pickle\"\n",
        "\n",
        "    cub_dataset_dir = \"/content/drive/MyDrive/project/CUB_200_2011\"\n",
        "\n",
        "    # Define optimizers\n",
        "    dis_optimizer = Adam(lr=stage1_discriminator_lr, beta_1=0.5, beta_2=0.999)\n",
        "    gen_optimizer = Adam(lr=stage1_generator_lr, beta_1=0.5, beta_2=0.999)\n",
        "\n",
        "    \"\"\"\n",
        "    Load datasets\n",
        "    \"\"\"\n",
        "    X_hr_train, y_hr_train, embeddings_train = load_dataset(filenames_file_path=filenames_file_path_train,\n",
        "                                                            class_info_file_path=class_info_file_path_train,\n",
        "                                                            cub_dataset_dir=cub_dataset_dir,\n",
        "                                                            embeddings_file_path=embeddings_file_path_train,\n",
        "                                                            image_size=(256, 256))\n",
        "\n",
        "    X_hr_test, y_hr_test, embeddings_test = load_dataset(filenames_file_path=filenames_file_path_test,\n",
        "                                                         class_info_file_path=class_info_file_path_test,\n",
        "                                                         cub_dataset_dir=cub_dataset_dir,\n",
        "                                                         embeddings_file_path=embeddings_file_path_test,\n",
        "                                                         image_size=(256, 256))\n",
        "\n",
        "    X_lr_train, y_lr_train, _ = load_dataset(filenames_file_path=filenames_file_path_train,\n",
        "                                             class_info_file_path=class_info_file_path_train,\n",
        "                                             cub_dataset_dir=cub_dataset_dir,\n",
        "                                             embeddings_file_path=embeddings_file_path_train,\n",
        "                                             image_size=(64, 64))\n",
        "\n",
        "    X_lr_test, y_lr_test, _ = load_dataset(filenames_file_path=filenames_file_path_test,\n",
        "                                           class_info_file_path=class_info_file_path_test,\n",
        "                                           cub_dataset_dir=cub_dataset_dir,\n",
        "                                           embeddings_file_path=embeddings_file_path_test,\n",
        "                                           image_size=(64, 64))\n",
        "\n",
        "    \"\"\"\n",
        "    Build and compile models\n",
        "    \"\"\"\n",
        "    stage2_dis = build_stage2_discriminator()\n",
        "    stage2_dis.compile(loss='binary_crossentropy', optimizer=dis_optimizer)\n",
        "\n",
        "    stage1_gen = build_stage1_generator()\n",
        "    stage1_gen.compile(loss=\"binary_crossentropy\", optimizer=gen_optimizer)\n",
        "\n",
        "    stage1_gen.load_weights(\"stage1_gen.h5\")\n",
        "\n",
        "    stage2_gen = build_stage2_generator()\n",
        "    stage2_gen.compile(loss=\"binary_crossentropy\", optimizer=gen_optimizer)\n",
        "\n",
        "    embedding_compressor_model = build_embedding_compressor_model()\n",
        "    embedding_compressor_model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "\n",
        "    adversarial_model = build_adversarial_model(stage2_gen, stage2_dis, stage1_gen)\n",
        "    adversarial_model.compile(loss=['binary_crossentropy', KL_loss], loss_weights=[1.0, 2.0],\n",
        "                              optimizer=gen_optimizer, metrics=None)\n",
        "\n",
        "    tensorboard = TensorBoard(log_dir=\"logs/\".format(time.time()))\n",
        "    tensorboard.set_model(stage2_gen)\n",
        "    tensorboard.set_model(stage2_dis)\n",
        "\n",
        "    # Generate an array containing real and fake values\n",
        "    # Apply label smoothing\n",
        "    real_labels = np.ones((batch_size, 1), dtype=float) * 0.9\n",
        "    fake_labels = np.zeros((batch_size, 1), dtype=float) * 0.1\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(\"========================================\")\n",
        "        print(\"Epoch is:\", epoch)\n",
        "\n",
        "        gen_losses = []\n",
        "        dis_losses = []\n",
        "\n",
        "        # Load data and train model\n",
        "        number_of_batches = int(X_hr_train.shape[0] / batch_size)\n",
        "        print(\"Number of batches:{}\".format(number_of_batches))\n",
        "        for index in range(number_of_batches):\n",
        "            print(\"Batch:{}\".format(index+1))\n",
        "\n",
        "            # Create a noise vector\n",
        "            z_noise = np.random.normal(0, 1, size=(batch_size, z_dim))\n",
        "            X_hr_train_batch = X_hr_train[index * batch_size:(index + 1) * batch_size]\n",
        "            embedding_batch = embeddings_train[index * batch_size:(index + 1) * batch_size]\n",
        "            X_hr_train_batch = (X_hr_train_batch - 127.5) / 127.5\n",
        "\n",
        "            # Generate fake images\n",
        "            lr_fake_images, _ = stage1_gen.predict([embedding_batch, z_noise], verbose=3)\n",
        "            hr_fake_images, _ = stage2_gen.predict([embedding_batch, lr_fake_images], verbose=3)\n",
        "\n",
        "            \"\"\"\n",
        "            4. Generate compressed embeddings\n",
        "            \"\"\"\n",
        "            compressed_embedding = embedding_compressor_model.predict_on_batch(embedding_batch)\n",
        "            compressed_embedding = np.reshape(compressed_embedding, (-1, 1, 1, condition_dim))\n",
        "            compressed_embedding = np.tile(compressed_embedding, (1, 4, 4, 1))\n",
        "\n",
        "            \"\"\"\n",
        "            5. Train the discriminator model\n",
        "            \"\"\"\n",
        "            dis_loss_real = stage2_dis.train_on_batch([X_hr_train_batch, compressed_embedding],\n",
        "                                                      np.reshape(real_labels, (batch_size, 1)))\n",
        "            dis_loss_fake = stage2_dis.train_on_batch([hr_fake_images, compressed_embedding],\n",
        "                                                      np.reshape(fake_labels, (batch_size, 1)))\n",
        "            dis_loss_wrong = stage2_dis.train_on_batch([X_hr_train_batch[:(batch_size - 1)], compressed_embedding[1:]],\n",
        "                                                       np.reshape(fake_labels[1:], (batch_size-1, 1)))\n",
        "            d_loss = 0.5 * np.add(dis_loss_real, 0.5 * np.add(dis_loss_wrong,  dis_loss_fake))\n",
        "            print(\"d_loss:{}\".format(d_loss))\n",
        "            \n",
        "            \"\"\"\n",
        "            Train the adversarial model\n",
        "            \"\"\"\n",
        "            g_loss = adversarial_model.train_on_batch([embedding_batch, z_noise, compressed_embedding],\n",
        "                                                                [K.ones((batch_size, 1)) * 0.9, K.ones((batch_size, 256)) * 0.9])\n",
        "\n",
        "            print(\"g_loss:{}\".format(g_loss))\n",
        "\n",
        "            dis_losses.append(d_loss)\n",
        "            gen_losses.append(g_loss)\n",
        "\n",
        "        \"\"\"\n",
        "        Save losses to Tensorboard after each epoch\n",
        "        \"\"\"\n",
        "        write_log(tensorboard, 'discriminator_loss', np.mean(dis_losses), epoch)\n",
        "        write_log(tensorboard, 'generator_loss', np.mean(gen_losses[0]), epoch)\n",
        "\n",
        "        # Generate and save images after every 2nd epoch\n",
        "        if epoch % 2 == 0:\n",
        "            # z_noise2 = np.random.uniform(-1, 1, size=(batch_size, z_dim))\n",
        "            z_noise2 = np.random.normal(0, 1, size=(batch_size, z_dim))\n",
        "            embedding_batch = embeddings_test[0:batch_size]\n",
        "\n",
        "            lr_fake_images, _ = stage1_gen.predict([embedding_batch, z_noise2], verbose=3)\n",
        "            hr_fake_images, _ = stage2_gen.predict([embedding_batch, lr_fake_images], verbose=3)\n",
        "\n",
        "            # Save images\n",
        "            for i, img in enumerate(hr_fake_images[:10]):\n",
        "                save_rgb_img(img, \"results2/gen_{}_{}.png\".format(epoch, i))\n",
        "\n",
        "    # Saving the models\n",
        "    stage2_gen.save_weights(\"stage2_gen.h5\")\n",
        "    stage2_dis.save_weights(\"stage2_dis.h5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "embeddings:  (8855, 10, 1024)\n",
            "All embeddings shape: (8855, 10, 1024)\n",
            "embeddings:  (2933, 10, 1024)\n",
            "All embeddings shape: (2933, 10, 1024)\n",
            "embeddings:  (8855, 10, 1024)\n",
            "All embeddings shape: (8855, 10, 1024)\n",
            "embeddings:  (2933, 10, 1024)\n",
            "All embeddings shape: (2933, 10, 1024)\n",
            "========================================\n",
            "Epoch is: 0\n",
            "Number of batches:276\n",
            "Batch:1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/tensorflow-1.15.2/python3.6/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n",
            "/tensorflow-1.15.2/python3.6/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "d_loss:5.588882565498352\n",
            "g_loss:[0.61164457, 0.5643287, 0.023657957]\n",
            "Batch:2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/tensorflow-1.15.2/python3.6/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "d_loss:1.4127824902534485\n",
            "g_loss:[5.322043, 5.285161, 0.018440858]\n",
            "Batch:3\n",
            "d_loss:2.7378381490707397\n",
            "g_loss:[0.60869783, 0.57175463, 0.018471587]\n",
            "Batch:4\n",
            "d_loss:1.585126280784607\n",
            "g_loss:[0.6702418, 0.6291865, 0.020527627]\n",
            "Batch:5\n",
            "d_loss:1.3032847046852112\n",
            "g_loss:[0.52519286, 0.48467982, 0.020256517]\n",
            "Batch:6\n",
            "d_loss:1.4556050300598145\n",
            "g_loss:[0.63919747, 0.6030866, 0.018055433]\n",
            "Batch:7\n",
            "d_loss:1.3771581947803497\n",
            "g_loss:[0.81499076, 0.77422607, 0.020382345]\n",
            "Batch:8\n",
            "d_loss:1.2867646217346191\n",
            "g_loss:[0.80544376, 0.7581645, 0.023639627]\n",
            "Batch:9\n",
            "d_loss:0.9438888430595398\n",
            "g_loss:[0.65581656, 0.6189825, 0.018417021]\n",
            "Batch:10\n",
            "d_loss:1.0904521197080612\n",
            "g_loss:[0.55553913, 0.53367954, 0.010929787]\n",
            "Batch:11\n",
            "d_loss:1.131147861480713\n",
            "g_loss:[0.42113727, 0.39275134, 0.014192971]\n",
            "Batch:12\n",
            "d_loss:0.9022780656814575\n",
            "g_loss:[0.4575357, 0.4272883, 0.015123714]\n",
            "Batch:13\n",
            "d_loss:0.9795585870742798\n",
            "g_loss:[0.40791342, 0.38641953, 0.010746942]\n",
            "Batch:14\n",
            "d_loss:0.8858652263879776\n",
            "g_loss:[0.45076123, 0.42545307, 0.012654076]\n",
            "Batch:15\n",
            "d_loss:0.9616212397813797\n",
            "g_loss:[0.41821143, 0.37318438, 0.022513527]\n",
            "Batch:16\n",
            "d_loss:0.811429813504219\n",
            "g_loss:[0.3994691, 0.35296196, 0.023253573]\n",
            "Batch:17\n",
            "d_loss:0.8881825506687164\n",
            "g_loss:[0.39996666, 0.35957986, 0.02019339]\n",
            "Batch:18\n",
            "d_loss:0.7709174454212189\n",
            "g_loss:[0.39816296, 0.35999656, 0.019083202]\n",
            "Batch:19\n",
            "d_loss:0.8454218357801437\n",
            "g_loss:[0.45334965, 0.41802996, 0.017659843]\n",
            "Batch:20\n",
            "d_loss:0.9101616591215134\n",
            "g_loss:[0.42733407, 0.4009697, 0.013182176]\n",
            "Batch:21\n",
            "d_loss:0.7917953431606293\n",
            "g_loss:[0.39394307, 0.37423933, 0.009851875]\n",
            "Batch:22\n",
            "d_loss:0.8169490098953247\n",
            "g_loss:[0.3742164, 0.35686672, 0.008674845]\n",
            "Batch:23\n",
            "d_loss:0.7326329052448273\n",
            "g_loss:[0.38442564, 0.36512107, 0.009652284]\n",
            "Batch:24\n",
            "d_loss:0.8239113837480545\n",
            "g_loss:[0.3680388, 0.34967995, 0.009179423]\n",
            "Batch:25\n",
            "d_loss:0.7697865664958954\n",
            "g_loss:[0.51124436, 0.4892522, 0.010996065]\n",
            "Batch:26\n",
            "d_loss:0.8129902929067612\n",
            "g_loss:[0.5644363, 0.5504496, 0.006993345]\n",
            "Batch:27\n",
            "d_loss:0.8553743809461594\n",
            "g_loss:[0.45179522, 0.4372185, 0.00728836]\n",
            "Batch:28\n",
            "d_loss:0.8247449994087219\n",
            "g_loss:[0.5884725, 0.5753502, 0.006561138]\n",
            "Batch:29\n",
            "d_loss:0.777206763625145\n",
            "g_loss:[0.47114557, 0.44721717, 0.011964206]\n",
            "Batch:30\n",
            "d_loss:0.7192371189594269\n",
            "g_loss:[0.5408562, 0.51339406, 0.013731057]\n",
            "Batch:31\n",
            "d_loss:0.7839423567056656\n",
            "g_loss:[0.51273966, 0.4970624, 0.007838624]\n",
            "Batch:32\n",
            "d_loss:0.7355841547250748\n",
            "g_loss:[0.7378341, 0.72196084, 0.007936633]\n",
            "Batch:33\n",
            "d_loss:0.8522796630859375\n",
            "g_loss:[0.5563915, 0.54688585, 0.004752802]\n",
            "Batch:34\n",
            "d_loss:0.781461626291275\n",
            "g_loss:[0.4350144, 0.42475253, 0.0051309336]\n",
            "Batch:35\n",
            "d_loss:0.7540854513645172\n",
            "g_loss:[0.41744992, 0.4065448, 0.00545256]\n",
            "Batch:36\n",
            "d_loss:0.810169592499733\n",
            "g_loss:[0.40584183, 0.39568937, 0.0050762356]\n",
            "Batch:37\n",
            "d_loss:0.7933867126703262\n",
            "g_loss:[0.39502057, 0.38375938, 0.005630592]\n",
            "Batch:38\n",
            "d_loss:0.7831404209136963\n",
            "g_loss:[0.40073732, 0.39077702, 0.0049801455]\n",
            "Batch:39\n",
            "d_loss:0.7232375293970108\n",
            "g_loss:[0.37965024, 0.3711043, 0.004272965]\n",
            "Batch:40\n",
            "d_loss:0.7958337366580963\n",
            "g_loss:[0.35441083, 0.34358186, 0.0054144827]\n",
            "Batch:41\n",
            "d_loss:0.7012372612953186\n",
            "g_loss:[0.46587914, 0.4541682, 0.0058554756]\n",
            "Batch:42\n",
            "d_loss:0.8699653893709183\n",
            "g_loss:[1.5670578, 1.5566378, 0.0052100364]\n",
            "Batch:43\n",
            "d_loss:0.8083275258541107\n",
            "g_loss:[0.8136455, 0.8035234, 0.005061036]\n",
            "Batch:44\n",
            "d_loss:0.7647834718227386\n",
            "g_loss:[0.7656965, 0.75413156, 0.005782498]\n",
            "Batch:45\n",
            "d_loss:0.7298793792724609\n",
            "g_loss:[0.5924519, 0.58382505, 0.004313412]\n",
            "Batch:46\n",
            "d_loss:0.6988931596279144\n",
            "g_loss:[0.58961415, 0.5774665, 0.0060738297]\n",
            "Batch:47\n",
            "d_loss:0.6517760008573532\n",
            "g_loss:[0.6405783, 0.6257821, 0.0073981187]\n",
            "Batch:48\n",
            "d_loss:0.7084266841411591\n",
            "g_loss:[0.59520143, 0.5807219, 0.007239763]\n",
            "Batch:49\n",
            "d_loss:0.7293980568647385\n",
            "g_loss:[0.5169219, 0.50545985, 0.00573102]\n",
            "Batch:50\n",
            "d_loss:0.6503317505121231\n",
            "g_loss:[0.56742364, 0.5531807, 0.007121483]\n",
            "Batch:51\n",
            "d_loss:0.7182374149560928\n",
            "g_loss:[0.58296055, 0.56717503, 0.007892764]\n",
            "Batch:52\n",
            "d_loss:0.7160177975893021\n",
            "g_loss:[0.4770716, 0.46596044, 0.005555581]\n",
            "Batch:53\n",
            "d_loss:0.7573978304862976\n",
            "g_loss:[0.7489965, 0.73669326, 0.0061516175]\n",
            "Batch:54\n",
            "d_loss:0.7369498610496521\n",
            "g_loss:[1.0505953, 1.0423797, 0.0041077547]\n",
            "Batch:55\n",
            "d_loss:0.7078528851270676\n",
            "g_loss:[0.831194, 0.81606185, 0.00756606]\n",
            "Batch:56\n",
            "d_loss:0.8308410048484802\n",
            "g_loss:[0.7197157, 0.7037921, 0.007961806]\n",
            "Batch:57\n",
            "d_loss:0.6649764478206635\n",
            "g_loss:[0.6825448, 0.6717211, 0.0054118666]\n",
            "Batch:58\n",
            "d_loss:0.6586778163909912\n",
            "g_loss:[0.5977925, 0.5872703, 0.0052610817]\n",
            "Batch:59\n",
            "d_loss:0.6924630552530289\n",
            "g_loss:[0.56532, 0.5584588, 0.0034305917]\n",
            "Batch:60\n",
            "d_loss:0.6334586888551712\n",
            "g_loss:[0.576047, 0.56968653, 0.0031802263]\n",
            "Batch:61\n",
            "d_loss:0.6607075482606888\n",
            "g_loss:[0.55806375, 0.5510288, 0.0035174878]\n",
            "Batch:62\n",
            "d_loss:0.6555207073688507\n",
            "g_loss:[0.53675216, 0.52639025, 0.005180962]\n",
            "Batch:63\n",
            "d_loss:0.6582343876361847\n",
            "g_loss:[0.6014851, 0.5907246, 0.005380256]\n",
            "Batch:64\n",
            "d_loss:0.6952517330646515\n",
            "g_loss:[0.6070831, 0.5954498, 0.005816632]\n",
            "Batch:65\n",
            "d_loss:0.6742947995662689\n",
            "g_loss:[0.57061696, 0.5582486, 0.0061841947]\n",
            "Batch:66\n",
            "d_loss:0.6683773547410965\n",
            "g_loss:[0.678533, 0.67032206, 0.0041054697]\n",
            "Batch:67\n",
            "d_loss:0.7676494717597961\n",
            "g_loss:[0.60227656, 0.5959766, 0.0031499725]\n",
            "Batch:68\n",
            "d_loss:0.6337447613477707\n",
            "g_loss:[0.62453973, 0.6153167, 0.0046115275]\n",
            "Batch:69\n",
            "d_loss:0.6762997806072235\n",
            "g_loss:[0.5920222, 0.5847623, 0.0036299636]\n",
            "Batch:70\n",
            "d_loss:0.6640131026506424\n",
            "g_loss:[0.56588805, 0.55799186, 0.003948105]\n",
            "Batch:71\n",
            "d_loss:0.6185383349657059\n",
            "g_loss:[0.57049537, 0.55980384, 0.0053457594]\n",
            "Batch:72\n",
            "d_loss:0.7215777635574341\n",
            "g_loss:[0.49594885, 0.48343918, 0.006254837]\n",
            "Batch:73\n",
            "d_loss:0.6506952494382858\n",
            "g_loss:[0.5996196, 0.5899613, 0.004829171]\n",
            "Batch:74\n",
            "d_loss:0.7242601215839386\n",
            "g_loss:[0.48199368, 0.47196376, 0.0050149565]\n",
            "Batch:75\n",
            "d_loss:0.5956791937351227\n",
            "g_loss:[0.52226555, 0.51180935, 0.00522811]\n",
            "Batch:76\n",
            "d_loss:0.7423189431428909\n",
            "g_loss:[0.57401276, 0.5664594, 0.003776674]\n",
            "Batch:77\n",
            "d_loss:0.6568334400653839\n",
            "g_loss:[0.5888804, 0.5787591, 0.0050606728]\n",
            "Batch:78\n",
            "d_loss:0.6687016785144806\n",
            "g_loss:[0.548022, 0.53813696, 0.0049424954]\n",
            "Batch:79\n",
            "d_loss:0.6693563461303711\n",
            "g_loss:[0.510478, 0.5009049, 0.0047865445]\n",
            "Batch:80\n",
            "d_loss:0.7022794783115387\n",
            "g_loss:[0.52771896, 0.52108514, 0.003316916]\n",
            "Batch:81\n",
            "d_loss:0.6263487786054611\n",
            "g_loss:[0.58777714, 0.5797478, 0.004014666]\n",
            "Batch:82\n",
            "d_loss:0.6462070643901825\n",
            "g_loss:[0.4555992, 0.4493307, 0.0031342541]\n",
            "Batch:83\n",
            "d_loss:0.669818565249443\n",
            "g_loss:[0.42801726, 0.42255995, 0.0027286576]\n",
            "Batch:84\n",
            "d_loss:0.6865363419055939\n",
            "g_loss:[0.40526873, 0.39881098, 0.0032288765]\n",
            "Batch:85\n",
            "d_loss:0.5838153064250946\n",
            "g_loss:[0.41190326, 0.4038744, 0.0040144254]\n",
            "Batch:86\n",
            "d_loss:0.6087887734174728\n",
            "g_loss:[0.39144576, 0.38296062, 0.00424257]\n",
            "Batch:87\n",
            "d_loss:0.6540684700012207\n",
            "g_loss:[0.3894768, 0.3810526, 0.0042121024]\n",
            "Batch:88\n",
            "d_loss:0.6234349012374878\n",
            "g_loss:[0.38807932, 0.38160282, 0.0032382465]\n",
            "Batch:89\n",
            "d_loss:0.6149378418922424\n",
            "g_loss:[0.6083378, 0.60177356, 0.0032821419]\n",
            "Batch:90\n",
            "d_loss:0.6673769503831863\n",
            "g_loss:[0.5580449, 0.55137503, 0.003334941]\n",
            "Batch:91\n",
            "d_loss:0.6537765860557556\n",
            "g_loss:[0.6665934, 0.65901387, 0.0037897425]\n",
            "Batch:92\n",
            "d_loss:0.674021378159523\n",
            "g_loss:[0.63572115, 0.62903106, 0.003345045]\n",
            "Batch:93\n",
            "d_loss:0.6233749687671661\n",
            "g_loss:[0.57834363, 0.5725118, 0.0029159307]\n",
            "Batch:94\n",
            "d_loss:0.6351282447576523\n",
            "g_loss:[0.64011776, 0.6318821, 0.0041178446]\n",
            "Batch:95\n",
            "d_loss:0.6909311413764954\n",
            "g_loss:[0.5664328, 0.5609474, 0.0027426868]\n",
            "Batch:96\n",
            "d_loss:0.6756084859371185\n",
            "g_loss:[0.54144317, 0.536662, 0.0023906012]\n",
            "Batch:97\n",
            "d_loss:0.6150380969047546\n",
            "g_loss:[0.5595111, 0.5533445, 0.003083318]\n",
            "Batch:98\n",
            "d_loss:0.7625699192285538\n",
            "g_loss:[0.52295697, 0.5161483, 0.0034043274]\n",
            "Batch:99\n",
            "d_loss:0.6040845662355423\n",
            "g_loss:[0.56128526, 0.5554391, 0.0029230623]\n",
            "Batch:100\n",
            "d_loss:0.6325138360261917\n",
            "g_loss:[0.5353312, 0.53096545, 0.0021828583]\n",
            "Batch:101\n",
            "d_loss:0.6071245223283768\n",
            "g_loss:[0.5448362, 0.53971577, 0.0025602323]\n",
            "Batch:102\n",
            "d_loss:0.6229579448699951\n",
            "g_loss:[0.4848712, 0.47832757, 0.003271811]\n",
            "Batch:103\n",
            "d_loss:0.6270167082548141\n",
            "g_loss:[0.45222273, 0.44808504, 0.0020688416]\n",
            "Batch:104\n",
            "d_loss:0.6234920620918274\n",
            "g_loss:[0.45180288, 0.44922298, 0.0012899511]\n",
            "Batch:105\n",
            "d_loss:0.6135691255331039\n",
            "g_loss:[0.47352895, 0.46888512, 0.002321908]\n",
            "Batch:106\n",
            "d_loss:0.6305082738399506\n",
            "g_loss:[0.40707687, 0.40017557, 0.0034506517]\n",
            "Batch:107\n",
            "d_loss:0.6143565773963928\n",
            "g_loss:[0.44251174, 0.43838316, 0.0020642863]\n",
            "Batch:108\n",
            "d_loss:0.5999192446470261\n",
            "g_loss:[0.47302297, 0.46764767, 0.002687648]\n",
            "Batch:109\n",
            "d_loss:0.6029378026723862\n",
            "g_loss:[0.43460932, 0.4303252, 0.0021420568]\n",
            "Batch:110\n",
            "d_loss:0.6255276501178741\n",
            "g_loss:[0.39148754, 0.3869682, 0.002259666]\n",
            "Batch:111\n",
            "d_loss:0.6368422359228134\n",
            "g_loss:[0.38294712, 0.37857646, 0.0021853324]\n",
            "Batch:112\n",
            "d_loss:0.6432464420795441\n",
            "g_loss:[0.3819415, 0.37600058, 0.0029704636]\n",
            "Batch:113\n",
            "d_loss:0.6204429864883423\n",
            "g_loss:[0.38665482, 0.38089752, 0.0028786494]\n",
            "Batch:114\n",
            "d_loss:0.604811429977417\n",
            "g_loss:[0.4374924, 0.4305998, 0.0034462889]\n",
            "Batch:115\n",
            "d_loss:0.6263784170150757\n",
            "g_loss:[0.45863408, 0.45364085, 0.002496614]\n",
            "Batch:116\n",
            "d_loss:0.6085711121559143\n",
            "g_loss:[0.38971713, 0.38578743, 0.0019648483]\n",
            "Batch:117\n",
            "d_loss:0.6594619303941727\n",
            "g_loss:[0.38983634, 0.38538802, 0.0022241601]\n",
            "Batch:118\n",
            "d_loss:0.6098862737417221\n",
            "g_loss:[0.3770655, 0.3716734, 0.0026960457]\n",
            "Batch:119\n",
            "d_loss:0.6206689476966858\n",
            "g_loss:[0.3910733, 0.38459432, 0.00323949]\n",
            "Batch:120\n",
            "d_loss:0.6645189523696899\n",
            "g_loss:[0.37617293, 0.37168768, 0.0022426273]\n",
            "Batch:121\n",
            "d_loss:0.6202304512262344\n",
            "g_loss:[0.37971467, 0.37569746, 0.002008609]\n",
            "Batch:122\n",
            "d_loss:0.6231254041194916\n",
            "g_loss:[0.38288242, 0.37967044, 0.0016059922]\n",
            "Batch:123\n",
            "d_loss:0.6456028968095779\n",
            "g_loss:[0.35506254, 0.35207957, 0.0014914831]\n",
            "Batch:124\n",
            "d_loss:0.6258837431669235\n",
            "g_loss:[0.34460878, 0.34143132, 0.0015887258]\n",
            "Batch:125\n",
            "d_loss:0.6457215845584869\n",
            "g_loss:[0.33639646, 0.3327273, 0.001834565]\n",
            "Batch:126\n",
            "d_loss:0.6049669533967972\n",
            "g_loss:[0.3365249, 0.33244038, 0.002042265]\n",
            "Batch:127\n",
            "d_loss:0.6013691872358322\n",
            "g_loss:[0.3363328, 0.33223355, 0.0020496312]\n",
            "Batch:128\n",
            "d_loss:0.6160838603973389\n",
            "g_loss:[0.3558818, 0.35105205, 0.0024148803]\n",
            "Batch:129\n",
            "d_loss:0.6538052707910538\n",
            "g_loss:[0.3638334, 0.35909927, 0.0023670616]\n",
            "Batch:130\n",
            "d_loss:0.6156667172908783\n",
            "g_loss:[0.47567004, 0.47044423, 0.0026129084]\n",
            "Batch:131\n",
            "d_loss:0.6404920518398285\n",
            "g_loss:[0.36906415, 0.36588407, 0.0015900397]\n",
            "Batch:132\n",
            "d_loss:0.6334968358278275\n",
            "g_loss:[0.34580788, 0.34294713, 0.0014303805]\n",
            "Batch:133\n",
            "d_loss:0.6313620954751968\n",
            "g_loss:[0.33550096, 0.3325418, 0.0014795853]\n",
            "Batch:134\n",
            "d_loss:0.6152357161045074\n",
            "g_loss:[0.33540326, 0.3323924, 0.0015054324]\n",
            "Batch:135\n",
            "d_loss:0.5993174910545349\n",
            "g_loss:[0.339206, 0.33540803, 0.0018989876]\n",
            "Batch:136\n",
            "d_loss:0.6191533505916595\n",
            "g_loss:[0.34255967, 0.33869317, 0.0019332401]\n",
            "Batch:137\n",
            "d_loss:0.6307685375213623\n",
            "g_loss:[0.35393757, 0.35124618, 0.0013456903]\n",
            "Batch:138\n",
            "d_loss:0.6072788387537003\n",
            "g_loss:[0.35047778, 0.34783337, 0.0013222073]\n",
            "Batch:139\n",
            "d_loss:0.6184050589799881\n",
            "g_loss:[0.34380648, 0.3411507, 0.0013278852]\n",
            "Batch:140\n",
            "d_loss:0.6505626738071442\n",
            "g_loss:[0.35377517, 0.34821272, 0.0027812326]\n",
            "Batch:141\n",
            "d_loss:0.5930213183164597\n",
            "g_loss:[0.33515552, 0.33013737, 0.0025090666]\n",
            "Batch:142\n",
            "d_loss:0.6169378459453583\n",
            "g_loss:[0.33869117, 0.3356995, 0.001495838]\n",
            "Batch:143\n",
            "d_loss:0.6365608870983124\n",
            "g_loss:[0.34096605, 0.33799738, 0.0014843374]\n",
            "Batch:144\n",
            "d_loss:0.6006267815828323\n",
            "g_loss:[0.3643036, 0.3608378, 0.0017328997]\n",
            "Batch:145\n",
            "d_loss:0.6141800135374069\n",
            "g_loss:[0.39242792, 0.38891065, 0.0017586383]\n",
            "Batch:146\n",
            "d_loss:0.6392558068037033\n",
            "g_loss:[0.35475615, 0.3515226, 0.0016167698]\n",
            "Batch:147\n",
            "d_loss:0.6421413868665695\n",
            "g_loss:[0.3768904, 0.37361613, 0.0016371371]\n",
            "Batch:148\n",
            "d_loss:0.6005493998527527\n",
            "g_loss:[0.3513997, 0.34868836, 0.0013556578]\n",
            "Batch:149\n",
            "d_loss:0.639461949467659\n",
            "g_loss:[0.3425053, 0.33906507, 0.0017201193]\n",
            "Batch:150\n",
            "d_loss:0.6198483109474182\n",
            "g_loss:[0.33365, 0.33020633, 0.0017218275]\n",
            "Batch:151\n",
            "d_loss:0.6127486228942871\n",
            "g_loss:[0.38368732, 0.3804807, 0.0016032981]\n",
            "Batch:152\n",
            "d_loss:0.6223382949829102\n",
            "g_loss:[0.4569393, 0.45352012, 0.0017095899]\n",
            "Batch:153\n",
            "d_loss:0.7755265980958939\n",
            "g_loss:[0.40787187, 0.4049654, 0.0014532318]\n",
            "Batch:154\n",
            "d_loss:0.6053073406219482\n",
            "g_loss:[0.3809431, 0.37804008, 0.0014515098]\n",
            "Batch:155\n",
            "d_loss:0.6062090992927551\n",
            "g_loss:[0.39091784, 0.38795558, 0.0014811325]\n",
            "Batch:156\n",
            "d_loss:0.6178731471300125\n",
            "g_loss:[0.36075637, 0.35737166, 0.0016923577]\n",
            "Batch:157\n",
            "d_loss:0.6369672119617462\n",
            "g_loss:[0.3482824, 0.34469223, 0.0017950868]\n",
            "Batch:158\n",
            "d_loss:0.5924322158098221\n",
            "g_loss:[0.3329508, 0.33036053, 0.0012951407]\n",
            "Batch:159\n",
            "d_loss:0.5910111963748932\n",
            "g_loss:[0.33427042, 0.3313648, 0.0014528094]\n",
            "Batch:160\n",
            "d_loss:0.6303002685308456\n",
            "g_loss:[0.35455748, 0.35103846, 0.00175952]\n",
            "Batch:161\n",
            "d_loss:0.5990246087312698\n",
            "g_loss:[0.33878285, 0.3352161, 0.0017833764]\n",
            "Batch:162\n",
            "d_loss:0.6220618486404419\n",
            "g_loss:[0.3297161, 0.3269465, 0.0013847899]\n",
            "Batch:163\n",
            "d_loss:0.6300254166126251\n",
            "g_loss:[0.3394535, 0.33720306, 0.0011252124]\n",
            "Batch:164\n",
            "d_loss:0.6315260082483292\n",
            "g_loss:[0.3356898, 0.33285353, 0.0014181404]\n",
            "Batch:165\n",
            "d_loss:0.6113166064023972\n",
            "g_loss:[0.33075434, 0.32764322, 0.0015555685]\n",
            "Batch:166\n",
            "d_loss:0.6026542782783508\n",
            "g_loss:[0.34030375, 0.33737224, 0.0014657587]\n",
            "Batch:167\n",
            "d_loss:0.6015175580978394\n",
            "g_loss:[0.33088943, 0.3275894, 0.0016500221]\n",
            "Batch:168\n",
            "d_loss:0.6264955401420593\n",
            "g_loss:[0.3331542, 0.33015662, 0.0014987887]\n",
            "Batch:169\n",
            "d_loss:0.6105373054742813\n",
            "g_loss:[0.3354035, 0.33183783, 0.001782839]\n",
            "Batch:170\n",
            "d_loss:0.6182602047920227\n",
            "g_loss:[0.33295447, 0.33068642, 0.0011340186]\n",
            "Batch:171\n",
            "d_loss:0.6202711313962936\n",
            "g_loss:[0.32931575, 0.3263404, 0.0014876766]\n",
            "Batch:172\n",
            "d_loss:0.6040263772010803\n",
            "g_loss:[0.3291283, 0.32697716, 0.0010755627]\n",
            "Batch:173\n",
            "d_loss:0.5857396274805069\n",
            "g_loss:[0.3312632, 0.32762957, 0.0018168201]\n",
            "Batch:174\n",
            "d_loss:0.5854499638080597\n",
            "g_loss:[0.32930565, 0.32592946, 0.0016880913]\n",
            "Batch:175\n",
            "d_loss:0.6585087180137634\n",
            "g_loss:[0.33203125, 0.3280496, 0.001990822]\n",
            "Batch:176\n",
            "d_loss:0.609030157327652\n",
            "g_loss:[0.3305933, 0.3267619, 0.0019156935]\n",
            "Batch:177\n",
            "d_loss:0.6039984971284866\n",
            "g_loss:[0.33642226, 0.33124602, 0.0025881159]\n",
            "Batch:178\n",
            "d_loss:0.5938295125961304\n",
            "g_loss:[0.33692005, 0.3316498, 0.002635124]\n",
            "Batch:179\n",
            "d_loss:0.6106901615858078\n",
            "g_loss:[0.3565585, 0.35296515, 0.0017966717]\n",
            "Batch:180\n",
            "d_loss:0.6039622873067856\n",
            "g_loss:[0.39783013, 0.39355353, 0.002138301]\n",
            "Batch:181\n",
            "d_loss:0.6195118278264999\n",
            "g_loss:[0.40838185, 0.4054271, 0.0014773797]\n",
            "Batch:182\n",
            "d_loss:0.6046208888292313\n",
            "g_loss:[0.35803413, 0.35455525, 0.0017394497]\n",
            "Batch:183\n",
            "d_loss:0.6312779188156128\n",
            "g_loss:[0.3476432, 0.34244493, 0.0025991309]\n",
            "Batch:184\n",
            "d_loss:0.6420761793851852\n",
            "g_loss:[0.34507844, 0.34061974, 0.002229352]\n",
            "Batch:185\n",
            "d_loss:0.5987048298120499\n",
            "g_loss:[0.351564, 0.3463816, 0.0025911906]\n",
            "Batch:186\n",
            "d_loss:0.6290359497070312\n",
            "g_loss:[0.3611001, 0.35664156, 0.0022292694]\n",
            "Batch:187\n",
            "d_loss:0.5943856835365295\n",
            "g_loss:[0.34561518, 0.33792967, 0.0038427508]\n",
            "Batch:188\n",
            "d_loss:0.6069383174180984\n",
            "g_loss:[0.33288574, 0.3282712, 0.0023072727]\n",
            "Batch:189\n",
            "d_loss:0.611077755689621\n",
            "g_loss:[0.49880695, 0.4956663, 0.0015703321]\n",
            "Batch:190\n",
            "d_loss:0.5915758162736893\n",
            "g_loss:[0.4617692, 0.4571782, 0.0022954901]\n",
            "Batch:191\n",
            "d_loss:0.6393035501241684\n",
            "g_loss:[0.45029083, 0.44624364, 0.0020235907]\n",
            "Batch:192\n",
            "d_loss:0.6413648128509521\n",
            "g_loss:[0.6801794, 0.6757204, 0.002229498]\n",
            "Batch:193\n",
            "d_loss:0.6465440541505814\n",
            "g_loss:[0.67135054, 0.6663228, 0.002513866]\n",
            "Batch:194\n",
            "d_loss:0.6119316518306732\n",
            "g_loss:[0.49428526, 0.49044377, 0.0019207495]\n",
            "Batch:195\n",
            "d_loss:0.6425643861293793\n",
            "g_loss:[0.7331214, 0.7289746, 0.0020734048]\n",
            "Batch:196\n",
            "d_loss:0.612474262714386\n",
            "g_loss:[0.78419745, 0.7805444, 0.0018265262]\n",
            "Batch:197\n",
            "d_loss:0.7031121551990509\n",
            "g_loss:[0.5859155, 0.58229345, 0.0018110224]\n",
            "Batch:198\n",
            "d_loss:0.632934033870697\n",
            "g_loss:[0.9047818, 0.90159655, 0.0015926244]\n",
            "Batch:199\n",
            "d_loss:0.6429560035467148\n",
            "g_loss:[0.8036941, 0.7997591, 0.0019675284]\n",
            "Batch:200\n",
            "d_loss:0.6692040413618088\n",
            "g_loss:[1.2364578, 1.234004, 0.0012269075]\n",
            "Batch:201\n",
            "d_loss:0.6242858469486237\n",
            "g_loss:[0.84945464, 0.84648955, 0.0014825573]\n",
            "Batch:202\n",
            "d_loss:0.6141190379858017\n",
            "g_loss:[0.9182049, 0.91576505, 0.0012199273]\n",
            "Batch:203\n",
            "d_loss:0.6307142376899719\n",
            "g_loss:[0.8189317, 0.8155227, 0.0017045271]\n",
            "Batch:204\n",
            "d_loss:0.647367388010025\n",
            "g_loss:[0.65962034, 0.65689063, 0.0013648558]\n",
            "Batch:205\n",
            "d_loss:0.6404654681682587\n",
            "g_loss:[0.64830095, 0.6450331, 0.0016339114]\n",
            "Batch:206\n",
            "d_loss:0.6152595728635788\n",
            "g_loss:[0.52193314, 0.5191436, 0.0013947753]\n",
            "Batch:207\n",
            "d_loss:0.6353857815265656\n",
            "g_loss:[0.4905247, 0.48774993, 0.0013873936]\n",
            "Batch:208\n",
            "d_loss:0.6304576992988586\n",
            "g_loss:[0.47790787, 0.474667, 0.001620428]\n",
            "Batch:209\n",
            "d_loss:0.6051209270954132\n",
            "g_loss:[0.5388724, 0.5365167, 0.0011778458]\n",
            "Batch:210\n",
            "d_loss:0.6252522319555283\n",
            "g_loss:[0.51085997, 0.5084849, 0.001187543]\n",
            "Batch:211\n",
            "d_loss:0.5959907174110413\n",
            "g_loss:[0.53354466, 0.5313515, 0.0010965714]\n",
            "Batch:212\n",
            "d_loss:0.5913136154413223\n",
            "g_loss:[0.4614097, 0.45852807, 0.0014408033]\n",
            "Batch:213\n",
            "d_loss:0.6184211373329163\n",
            "g_loss:[0.48071408, 0.47731078, 0.0017016521]\n",
            "Batch:214\n",
            "d_loss:0.5940921157598495\n",
            "g_loss:[0.38539243, 0.38260046, 0.001395987]\n",
            "Batch:215\n",
            "d_loss:0.6097522974014282\n",
            "g_loss:[0.4292224, 0.426068, 0.0015771944]\n",
            "Batch:216\n",
            "d_loss:0.6016617119312286\n",
            "g_loss:[0.39413083, 0.39172608, 0.001202373]\n",
            "Batch:217\n",
            "d_loss:0.5824712365865707\n",
            "g_loss:[0.4140879, 0.41190562, 0.0010911354]\n",
            "Batch:218\n",
            "d_loss:0.6380689591169357\n",
            "g_loss:[0.38891935, 0.38582796, 0.001545699]\n",
            "Batch:219\n",
            "d_loss:0.5917455703020096\n",
            "g_loss:[0.40756902, 0.4049096, 0.00132971]\n",
            "Batch:220\n",
            "d_loss:0.5937585234642029\n",
            "g_loss:[0.37996548, 0.37652272, 0.0017213781]\n",
            "Batch:221\n",
            "d_loss:0.5737262964248657\n",
            "g_loss:[0.38488227, 0.38139457, 0.0017438509]\n",
            "Batch:222\n",
            "d_loss:0.6012253761291504\n",
            "g_loss:[0.37461635, 0.3715907, 0.0015128292]\n",
            "Batch:223\n",
            "d_loss:0.585334837436676\n",
            "g_loss:[0.35523888, 0.35158366, 0.0018276087]\n",
            "Batch:224\n",
            "d_loss:0.7187744975090027\n",
            "g_loss:[0.34206465, 0.33872733, 0.0016686546]\n",
            "Batch:225\n",
            "d_loss:0.5990401655435562\n",
            "g_loss:[0.37991357, 0.37635005, 0.0017817636]\n",
            "Batch:226\n",
            "d_loss:0.6281531453132629\n",
            "g_loss:[0.38439235, 0.3811354, 0.0016284666]\n",
            "Batch:227\n",
            "d_loss:0.589640736579895\n",
            "g_loss:[0.35843956, 0.35439855, 0.0020205032]\n",
            "Batch:228\n",
            "d_loss:0.5680235028266907\n",
            "g_loss:[0.35033152, 0.3469657, 0.0016829096]\n",
            "Batch:229\n",
            "d_loss:0.6293950229883194\n",
            "g_loss:[0.3353093, 0.33241835, 0.0014454661]\n",
            "Batch:230\n",
            "d_loss:0.5730745643377304\n",
            "g_loss:[0.3446385, 0.34119397, 0.0017222601]\n",
            "Batch:231\n",
            "d_loss:0.6031711250543594\n",
            "g_loss:[0.3395398, 0.33549148, 0.0020241588]\n",
            "Batch:232\n",
            "d_loss:0.6065140515565872\n",
            "g_loss:[0.33488712, 0.33142653, 0.0017302965]\n",
            "Batch:233\n",
            "d_loss:0.6087338030338287\n",
            "g_loss:[0.3451584, 0.34256002, 0.0012991838]\n",
            "Batch:234\n",
            "d_loss:0.5996450036764145\n",
            "g_loss:[0.3369744, 0.33369923, 0.0016375938]\n",
            "Batch:235\n",
            "d_loss:0.5909432768821716\n",
            "g_loss:[0.3502711, 0.34739506, 0.0014380242]\n",
            "Batch:236\n",
            "d_loss:0.5891359746456146\n",
            "g_loss:[0.3342572, 0.33124417, 0.0015065128]\n",
            "Batch:237\n",
            "d_loss:0.6136922687292099\n",
            "g_loss:[0.34554747, 0.3429516, 0.0012979377]\n",
            "Batch:238\n",
            "d_loss:0.5726594924926758\n",
            "g_loss:[0.3395505, 0.33704093, 0.0012547885]\n",
            "Batch:239\n",
            "d_loss:0.5968254655599594\n",
            "g_loss:[0.33530962, 0.33250564, 0.0014019842]\n",
            "Batch:240\n",
            "d_loss:0.5901997238397598\n",
            "g_loss:[0.33669075, 0.33393908, 0.0013758449]\n",
            "Batch:241\n",
            "d_loss:0.6085619032382965\n",
            "g_loss:[0.32995373, 0.32748836, 0.0012326885]\n",
            "Batch:242\n",
            "d_loss:0.595977395772934\n",
            "g_loss:[0.3332009, 0.3296385, 0.0017811891]\n",
            "Batch:243\n",
            "d_loss:0.5944641828536987\n",
            "g_loss:[0.33777988, 0.33454362, 0.0016181257]\n",
            "Batch:244\n",
            "d_loss:0.6094560325145721\n",
            "g_loss:[0.36297354, 0.35995072, 0.001511413]\n",
            "Batch:245\n",
            "d_loss:0.6443389356136322\n",
            "g_loss:[0.3450205, 0.34229112, 0.0013646991]\n",
            "Batch:246\n",
            "d_loss:0.6376479715108871\n",
            "g_loss:[0.3571109, 0.35505748, 0.0010267022]\n",
            "Batch:247\n",
            "d_loss:0.6267772316932678\n",
            "g_loss:[0.35373598, 0.3514462, 0.0011448839]\n",
            "Batch:248\n",
            "d_loss:0.5916282534599304\n",
            "g_loss:[0.3471728, 0.34478945, 0.0011916735]\n",
            "Batch:249\n",
            "d_loss:0.5893491357564926\n",
            "g_loss:[0.34516114, 0.34256995, 0.001295591]\n",
            "Batch:250\n",
            "d_loss:0.5882767736911774\n",
            "g_loss:[0.33877775, 0.3363912, 0.0011932743]\n",
            "Batch:251\n",
            "d_loss:0.6080081462860107\n",
            "g_loss:[0.33154148, 0.32871825, 0.0014116121]\n",
            "Batch:252\n",
            "d_loss:0.5867236107587814\n",
            "g_loss:[0.33040774, 0.32775185, 0.001327947]\n",
            "Batch:253\n",
            "d_loss:0.5826613456010818\n",
            "g_loss:[0.33114648, 0.32827508, 0.0014356994]\n",
            "Batch:254\n",
            "d_loss:0.6520354300737381\n",
            "g_loss:[0.33184698, 0.3297876, 0.0010296886]\n",
            "Batch:255\n",
            "d_loss:0.6063739061355591\n",
            "g_loss:[0.33033383, 0.3269385, 0.0016976654]\n",
            "Batch:256\n",
            "d_loss:0.6191097497940063\n",
            "g_loss:[0.3329562, 0.32867447, 0.0021408664]\n",
            "Batch:257\n",
            "d_loss:0.630834311246872\n",
            "g_loss:[0.3368019, 0.33230472, 0.0022485878]\n",
            "Batch:258\n",
            "d_loss:0.6405643671751022\n",
            "g_loss:[0.34560955, 0.34147096, 0.0020692905]\n",
            "Batch:259\n",
            "d_loss:0.5903940051794052\n",
            "g_loss:[0.3322265, 0.3297575, 0.0012345074]\n",
            "Batch:260\n",
            "d_loss:0.5897558331489563\n",
            "g_loss:[0.34319872, 0.33986157, 0.0016685689]\n",
            "Batch:261\n",
            "d_loss:0.6165061146020889\n",
            "g_loss:[0.430707, 0.42715222, 0.0017773937]\n",
            "Batch:262\n",
            "d_loss:0.5969124436378479\n",
            "g_loss:[0.34639364, 0.34329137, 0.0015511306]\n",
            "Batch:263\n",
            "d_loss:0.5921466946601868\n",
            "g_loss:[0.34470966, 0.34168753, 0.0015110743]\n",
            "Batch:264\n",
            "d_loss:0.5957178175449371\n",
            "g_loss:[0.3464083, 0.34405756, 0.0011753753]\n",
            "Batch:265\n",
            "d_loss:0.5871971100568771\n",
            "g_loss:[0.33070976, 0.32878393, 0.0009629077]\n",
            "Batch:266\n",
            "d_loss:0.5871783047914505\n",
            "g_loss:[0.3284076, 0.3263022, 0.0010526889]\n",
            "Batch:267\n",
            "d_loss:0.5867632925510406\n",
            "g_loss:[0.3299436, 0.32747614, 0.0012337222]\n",
            "Batch:268\n",
            "d_loss:0.5825837403535843\n",
            "g_loss:[0.32899487, 0.32683778, 0.0010785434]\n",
            "Batch:269\n",
            "d_loss:0.6119127720594406\n",
            "g_loss:[0.3300648, 0.32787332, 0.0010957477]\n",
            "Batch:270\n",
            "d_loss:0.585106760263443\n",
            "g_loss:[0.33185133, 0.32993343, 0.0009589551]\n",
            "Batch:271\n",
            "d_loss:0.5759534984827042\n",
            "g_loss:[0.3279704, 0.32606065, 0.0009548721]\n",
            "Batch:272\n",
            "d_loss:0.5950865894556046\n",
            "g_loss:[0.32855085, 0.32672182, 0.0009145143]\n",
            "Batch:273\n",
            "d_loss:0.5900502055883408\n",
            "g_loss:[0.32935604, 0.3273435, 0.0010062768]\n",
            "Batch:274\n",
            "d_loss:0.5875927656888962\n",
            "g_loss:[0.32791662, 0.3260103, 0.000953167]\n",
            "Batch:275\n",
            "d_loss:0.582330510020256\n",
            "g_loss:[0.3277057, 0.32549447, 0.0011056141]\n",
            "Batch:276\n",
            "d_loss:0.5796951502561569\n",
            "g_loss:[0.33013326, 0.32727405, 0.0014296018]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "========================================\n",
            "Epoch is: 1\n",
            "Number of batches:276\n",
            "Batch:1\n",
            "d_loss:0.5857464224100113\n",
            "g_loss:[0.33313194, 0.32995334, 0.0015892922]\n",
            "Batch:2\n",
            "d_loss:0.5991552025079727\n",
            "g_loss:[0.32787913, 0.3256362, 0.0011214617]\n",
            "Batch:3\n",
            "d_loss:0.5844319015741348\n",
            "g_loss:[0.3290196, 0.32645437, 0.0012826247]\n",
            "Batch:4\n",
            "d_loss:0.5953895747661591\n",
            "g_loss:[0.32953164, 0.326504, 0.0015138244]\n",
            "Batch:5\n",
            "d_loss:0.5970718711614609\n",
            "g_loss:[0.33330005, 0.33022374, 0.001538152]\n",
            "Batch:6\n",
            "d_loss:0.5911290049552917\n",
            "g_loss:[0.32987183, 0.32710856, 0.00138163]\n",
            "Batch:7\n",
            "d_loss:0.6412150710821152\n",
            "g_loss:[0.32959652, 0.3267892, 0.0014036527]\n",
            "Batch:8\n",
            "d_loss:0.5905336141586304\n",
            "g_loss:[0.34190226, 0.33812863, 0.0018868203]\n",
            "Batch:9\n",
            "d_loss:0.6040646433830261\n",
            "g_loss:[0.40566212, 0.40276772, 0.0014472064]\n",
            "Batch:10\n",
            "d_loss:0.5777639299631119\n",
            "g_loss:[0.67356324, 0.6716739, 0.00094467064]\n",
            "Batch:11\n",
            "d_loss:0.6062196642160416\n",
            "g_loss:[0.6849493, 0.6824944, 0.0012274435]\n",
            "Batch:12\n",
            "d_loss:0.5920801609754562\n",
            "g_loss:[0.690121, 0.6874913, 0.0013148424]\n",
            "Batch:13\n",
            "d_loss:0.5809767842292786\n",
            "g_loss:[0.5979264, 0.5959946, 0.00096590805]\n",
            "Batch:14\n",
            "d_loss:0.5706145614385605\n",
            "g_loss:[0.5561289, 0.5540106, 0.0010591504]\n",
            "Batch:15\n",
            "d_loss:0.599314495921135\n",
            "g_loss:[0.5229738, 0.51926184, 0.0018559827]\n",
            "Batch:16\n",
            "d_loss:0.5904791206121445\n",
            "g_loss:[0.4858732, 0.48202026, 0.0019264641]\n",
            "Batch:17\n",
            "d_loss:0.6029286980628967\n",
            "g_loss:[0.4488385, 0.4444074, 0.0022155514]\n",
            "Batch:18\n",
            "d_loss:0.5824273377656937\n",
            "g_loss:[0.42495042, 0.4206627, 0.002143857]\n",
            "Batch:19\n",
            "d_loss:0.583972841501236\n",
            "g_loss:[0.49380922, 0.48997912, 0.0019150518]\n",
            "Batch:20\n",
            "d_loss:0.6030850410461426\n",
            "g_loss:[0.6020205, 0.5991355, 0.0014425036]\n",
            "Batch:21\n",
            "d_loss:0.5954274982213974\n",
            "g_loss:[0.42679498, 0.42444482, 0.001175077]\n",
            "Batch:22\n",
            "d_loss:0.6144679188728333\n",
            "g_loss:[0.41672948, 0.41496098, 0.0008842512]\n",
            "Batch:23\n",
            "d_loss:0.5696900933980942\n",
            "g_loss:[0.49935913, 0.49720764, 0.0010757458]\n",
            "Batch:24\n",
            "d_loss:0.593508243560791\n",
            "g_loss:[0.47194314, 0.47003645, 0.000953353]\n",
            "Batch:25\n",
            "d_loss:0.5745018869638443\n",
            "g_loss:[0.44210017, 0.43961862, 0.0012407699]\n",
            "Batch:26\n",
            "d_loss:0.5827004015445709\n",
            "g_loss:[0.40047765, 0.39874443, 0.00086661207]\n",
            "Batch:27\n",
            "d_loss:0.5852150321006775\n",
            "g_loss:[0.420135, 0.418099, 0.0010180013]\n",
            "Batch:28\n",
            "d_loss:0.5867442041635513\n",
            "g_loss:[0.39036903, 0.38841712, 0.0009759461]\n",
            "Batch:29\n",
            "d_loss:0.5879899263381958\n",
            "g_loss:[0.33417055, 0.33123028, 0.001470138]\n",
            "Batch:30\n",
            "d_loss:0.6081076711416245\n",
            "g_loss:[0.3536389, 0.3502919, 0.001673483]\n",
            "Batch:31\n",
            "d_loss:0.5938181132078171\n",
            "g_loss:[0.34801415, 0.34573603, 0.001139063]\n",
            "Batch:32\n",
            "d_loss:0.6049054712057114\n",
            "g_loss:[0.34737626, 0.3448745, 0.0012508724]\n",
            "Batch:33\n",
            "d_loss:0.583150178194046\n",
            "g_loss:[0.3359982, 0.33374718, 0.0011255138]\n",
            "Batch:34\n",
            "d_loss:0.6080190986394882\n",
            "g_loss:[0.33251998, 0.33043295, 0.0010435065]\n",
            "Batch:35\n",
            "d_loss:0.5916852653026581\n",
            "g_loss:[0.3273869, 0.3255933, 0.000896795]\n",
            "Batch:36\n",
            "d_loss:0.6026368886232376\n",
            "g_loss:[0.3276535, 0.3259461, 0.0008537004]\n",
            "Batch:37\n",
            "d_loss:0.6024542152881622\n",
            "g_loss:[0.32786122, 0.326248, 0.0008066207]\n",
            "Batch:38\n",
            "d_loss:0.6208081245422363\n",
            "g_loss:[0.33157098, 0.32985634, 0.0008573303]\n",
            "Batch:39\n",
            "d_loss:0.5742974430322647\n",
            "g_loss:[0.34009758, 0.33842045, 0.0008385652]\n",
            "Batch:40\n",
            "d_loss:0.6158878207206726\n",
            "g_loss:[0.3272726, 0.32564896, 0.0008118176]\n",
            "Batch:41\n",
            "d_loss:0.5956402570009232\n",
            "g_loss:[0.3334541, 0.33158463, 0.00093473593]\n",
            "Batch:42\n",
            "d_loss:0.6034152656793594\n",
            "g_loss:[0.3275934, 0.3257895, 0.00090193585]\n",
            "Batch:43\n",
            "d_loss:0.5845014899969101\n",
            "g_loss:[0.32885593, 0.32720503, 0.0008254439]\n",
            "Batch:44\n",
            "d_loss:0.6071304678916931\n",
            "g_loss:[0.3783571, 0.37656236, 0.0008973743]\n",
            "Batch:45\n",
            "d_loss:0.5875598043203354\n",
            "g_loss:[0.35653707, 0.3550034, 0.00076684356]\n",
            "Batch:46\n",
            "d_loss:0.5834978669881821\n",
            "g_loss:[0.37495437, 0.37265283, 0.0011507756]\n",
            "Batch:47\n",
            "d_loss:0.5778006166219711\n",
            "g_loss:[0.4534725, 0.45047826, 0.0014971169]\n",
            "Batch:48\n",
            "d_loss:0.5872159898281097\n",
            "g_loss:[0.3559287, 0.35314727, 0.0013907058]\n",
            "Batch:49\n",
            "d_loss:0.5933799594640732\n",
            "g_loss:[0.34718755, 0.34491587, 0.0011358452]\n",
            "Batch:50\n",
            "d_loss:0.5814033448696136\n",
            "g_loss:[0.3507514, 0.34810066, 0.0013253759]\n",
            "Batch:51\n",
            "d_loss:0.5866713225841522\n",
            "g_loss:[0.38486502, 0.3825058, 0.0011796084]\n",
            "Batch:52\n",
            "d_loss:0.5761092603206635\n",
            "g_loss:[0.33999637, 0.3382185, 0.0008889332]\n",
            "Batch:53\n",
            "d_loss:0.582284227013588\n",
            "g_loss:[0.33477122, 0.33231315, 0.0012290275]\n",
            "Batch:54\n",
            "d_loss:0.5912208706140518\n",
            "g_loss:[0.32790026, 0.32619727, 0.00085149985]\n",
            "Batch:55\n",
            "d_loss:0.5750588476657867\n",
            "g_loss:[0.32980615, 0.3271541, 0.0013260187]\n",
            "Batch:56\n",
            "d_loss:0.5803671628236771\n",
            "g_loss:[0.33032873, 0.32720307, 0.0015628397]\n",
            "Batch:57\n",
            "d_loss:0.5818556696176529\n",
            "g_loss:[0.32937974, 0.32715893, 0.0011103989]\n",
            "Batch:58\n",
            "d_loss:0.5743353962898254\n",
            "g_loss:[0.3412803, 0.3388862, 0.0011970587]\n",
            "Batch:59\n",
            "d_loss:0.5876244604587555\n",
            "g_loss:[0.35580954, 0.3539176, 0.0009459711]\n",
            "Batch:60\n",
            "d_loss:0.5758063644170761\n",
            "g_loss:[0.33097342, 0.3292277, 0.0008728659]\n",
            "Batch:61\n",
            "d_loss:0.5734233260154724\n",
            "g_loss:[0.36569655, 0.3638981, 0.0008992296]\n",
            "Batch:62\n",
            "d_loss:0.5784372687339783\n",
            "g_loss:[0.32994255, 0.3272651, 0.0013387158]\n",
            "Batch:63\n",
            "d_loss:0.5820334255695343\n",
            "g_loss:[0.32944658, 0.32676572, 0.0013404303]\n",
            "Batch:64\n",
            "d_loss:0.600327268242836\n",
            "g_loss:[0.3336629, 0.33091235, 0.001375272]\n",
            "Batch:65\n",
            "d_loss:0.5846446454524994\n",
            "g_loss:[0.3319722, 0.32928315, 0.0013445358]\n",
            "Batch:66\n",
            "d_loss:0.5734514445066452\n",
            "g_loss:[0.32908207, 0.32671142, 0.0011853232]\n",
            "Batch:67\n",
            "d_loss:0.6119412034749985\n",
            "g_loss:[0.3284527, 0.3263987, 0.0010270094]\n",
            "Batch:68\n",
            "d_loss:0.5645377188920975\n",
            "g_loss:[0.3287906, 0.32616633, 0.0013121349]\n",
            "Batch:69\n",
            "d_loss:0.5936318933963776\n",
            "g_loss:[0.32789484, 0.3258502, 0.0010223304]\n",
            "Batch:70\n",
            "d_loss:0.5981849879026413\n",
            "g_loss:[0.33007142, 0.3280779, 0.0009967483]\n",
            "Batch:71\n",
            "d_loss:0.5707068592309952\n",
            "g_loss:[0.327906, 0.32563943, 0.0011332867]\n",
            "Batch:72\n",
            "d_loss:0.6065706759691238\n",
            "g_loss:[0.33087164, 0.3285952, 0.0011382177]\n",
            "Batch:73\n",
            "d_loss:0.5814073085784912\n",
            "g_loss:[0.32886627, 0.3269054, 0.0009804344]\n",
            "Batch:74\n",
            "d_loss:0.6211565881967545\n",
            "g_loss:[0.3284588, 0.3262829, 0.0010879461]\n",
            "Batch:75\n",
            "d_loss:0.5610871911048889\n",
            "g_loss:[0.3300037, 0.32776868, 0.00111751]\n",
            "Batch:76\n",
            "d_loss:0.6170403510332108\n",
            "g_loss:[0.34600353, 0.3441844, 0.0009095694]\n",
            "Batch:77\n",
            "d_loss:0.5947912335395813\n",
            "g_loss:[0.33765584, 0.33534205, 0.0011568896]\n",
            "Batch:78\n",
            "d_loss:0.5935561805963516\n",
            "g_loss:[0.32786426, 0.32551476, 0.0011747489]\n",
            "Batch:79\n",
            "d_loss:0.5712838917970657\n",
            "g_loss:[0.33115268, 0.32872218, 0.001215243]\n",
            "Batch:80\n",
            "d_loss:0.6231223195791245\n",
            "g_loss:[0.3296146, 0.3278232, 0.00089570927]\n",
            "Batch:81\n",
            "d_loss:0.5612062811851501\n",
            "g_loss:[0.33408272, 0.3319318, 0.0010754666]\n",
            "Batch:82\n",
            "d_loss:0.5968820601701736\n",
            "g_loss:[0.3285703, 0.32674068, 0.0009148189]\n",
            "Batch:83\n",
            "d_loss:0.5938344150781631\n",
            "g_loss:[0.32948157, 0.327707, 0.0008872824]\n",
            "Batch:84\n",
            "d_loss:0.5903159826993942\n",
            "g_loss:[0.3871577, 0.38498548, 0.0010861086]\n",
            "Batch:85\n",
            "d_loss:0.5658164769411087\n",
            "g_loss:[0.46870726, 0.46597868, 0.0013642957]\n",
            "Batch:86\n",
            "d_loss:0.5764648169279099\n",
            "g_loss:[0.4517803, 0.4489898, 0.0013952365]\n",
            "Batch:87\n",
            "d_loss:0.605876088142395\n",
            "g_loss:[0.49104258, 0.48834184, 0.0013503728]\n",
            "Batch:88\n",
            "d_loss:0.5797759890556335\n",
            "g_loss:[0.5803359, 0.5780734, 0.0011312751]\n",
            "Batch:89\n",
            "d_loss:0.58600714802742\n",
            "g_loss:[1.1372465, 1.1349705, 0.0011379736]\n",
            "Batch:90\n",
            "d_loss:0.6439665257930756\n",
            "g_loss:[2.4956062, 2.4929852, 0.001310471]\n",
            "Batch:91\n",
            "d_loss:0.6716829389333725\n",
            "g_loss:[0.97256106, 0.9695426, 0.0015092117]\n",
            "Batch:92\n",
            "d_loss:0.6766513288021088\n",
            "g_loss:[3.9168088, 3.9148376, 0.0009856541]\n",
            "Batch:93\n",
            "d_loss:1.133963331580162\n",
            "g_loss:[1.5271925, 1.5253768, 0.0009078481]\n",
            "Batch:94\n",
            "d_loss:0.7133868038654327\n",
            "g_loss:[0.9631057, 0.96064156, 0.0012320592]\n",
            "Batch:95\n",
            "d_loss:0.8039490878582001\n",
            "g_loss:[0.89149183, 0.8894159, 0.0010379571]\n",
            "Batch:96\n",
            "d_loss:0.7507903277873993\n",
            "g_loss:[0.905533, 0.9036609, 0.00093605765]\n",
            "Batch:97\n",
            "d_loss:0.7087683379650116\n",
            "g_loss:[0.9235911, 0.921314, 0.0011385432]\n",
            "Batch:98\n",
            "d_loss:0.7444217503070831\n",
            "g_loss:[0.8545712, 0.8518975, 0.0013368677]\n",
            "Batch:99\n",
            "d_loss:0.659039631485939\n",
            "g_loss:[0.8617439, 0.85957915, 0.0010823827]\n",
            "Batch:100\n",
            "d_loss:0.6910039484500885\n",
            "g_loss:[0.81404185, 0.81247973, 0.000781065]\n",
            "Batch:101\n",
            "d_loss:0.6429853588342667\n",
            "g_loss:[0.75492996, 0.7531146, 0.000907696]\n",
            "Batch:102\n",
            "d_loss:0.6074114590883255\n",
            "g_loss:[0.67585945, 0.6735672, 0.0011461327]\n",
            "Batch:103\n",
            "d_loss:0.6298716217279434\n",
            "g_loss:[0.7342692, 0.7326506, 0.0008093085]\n",
            "Batch:104\n",
            "d_loss:0.6386365294456482\n",
            "g_loss:[0.7365222, 0.7353393, 0.0005914542]\n",
            "Batch:105\n",
            "d_loss:0.6362477838993073\n",
            "g_loss:[0.7227816, 0.72067666, 0.0010524807]\n",
            "Batch:106\n",
            "d_loss:0.6891250014305115\n",
            "g_loss:[0.74043936, 0.73797417, 0.0012325826]\n",
            "Batch:107\n",
            "d_loss:0.6535209119319916\n",
            "g_loss:[0.64760095, 0.64588726, 0.00085683394]\n",
            "Batch:108\n",
            "d_loss:0.6246932595968246\n",
            "g_loss:[0.6249143, 0.62281513, 0.001049575]\n",
            "Batch:109\n",
            "d_loss:0.6573902815580368\n",
            "g_loss:[0.94760484, 0.94575316, 0.0009258283]\n",
            "Batch:110\n",
            "d_loss:0.7057738304138184\n",
            "g_loss:[0.78837144, 0.7865528, 0.00090932974]\n",
            "Batch:111\n",
            "d_loss:0.7384251058101654\n",
            "g_loss:[0.9351394, 0.9334389, 0.0008502602]\n",
            "Batch:112\n",
            "d_loss:0.7512481212615967\n",
            "g_loss:[1.062202, 1.0599442, 0.0011289086]\n",
            "Batch:113\n",
            "d_loss:0.7200603187084198\n",
            "g_loss:[1.1081904, 1.1058505, 0.0011699654]\n",
            "Batch:114\n",
            "d_loss:0.7373340874910355\n",
            "g_loss:[1.0548451, 1.0522738, 0.0012856763]\n",
            "Batch:115\n",
            "d_loss:0.6680219173431396\n",
            "g_loss:[0.79204696, 0.7899864, 0.0010302842]\n",
            "Batch:116\n",
            "d_loss:0.8071356415748596\n",
            "g_loss:[0.8211361, 0.819465, 0.0008355656]\n",
            "Batch:117\n",
            "d_loss:0.6706929951906204\n",
            "g_loss:[0.70119524, 0.6994194, 0.00088793726]\n",
            "Batch:118\n",
            "d_loss:0.798252671957016\n",
            "g_loss:[0.8254693, 0.8235721, 0.00094862154]\n",
            "Batch:119\n",
            "d_loss:0.7084680050611496\n",
            "g_loss:[0.83430994, 0.8320582, 0.0011258607]\n",
            "Batch:120\n",
            "d_loss:0.7783876061439514\n",
            "g_loss:[0.6627381, 0.66093683, 0.00090062123]\n",
            "Batch:121\n",
            "d_loss:0.7128290086984634\n",
            "g_loss:[0.5845463, 0.58288753, 0.0008293904]\n",
            "Batch:122\n",
            "d_loss:0.6970373094081879\n",
            "g_loss:[0.5270121, 0.52549434, 0.00075887435]\n",
            "Batch:123\n",
            "d_loss:0.6968248933553696\n",
            "g_loss:[0.5733199, 0.57189727, 0.00071130897]\n",
            "Batch:124\n",
            "d_loss:0.6956197917461395\n",
            "g_loss:[0.6842442, 0.68289745, 0.00067338627]\n",
            "Batch:125\n",
            "d_loss:0.7171373814344406\n",
            "g_loss:[0.82293063, 0.8213398, 0.0007954329]\n",
            "Batch:126\n",
            "d_loss:0.7037301510572433\n",
            "g_loss:[1.0388148, 1.0370679, 0.0008734308]\n",
            "Batch:127\n",
            "d_loss:0.6429746747016907\n",
            "g_loss:[0.9006942, 0.8988589, 0.0009176424]\n",
            "Batch:128\n",
            "d_loss:0.6764901578426361\n",
            "g_loss:[0.7744114, 0.77243465, 0.0009883788]\n",
            "Batch:129\n",
            "d_loss:0.6365806460380554\n",
            "g_loss:[0.8001581, 0.798293, 0.0009325475]\n",
            "Batch:130\n",
            "d_loss:0.7733306884765625\n",
            "g_loss:[0.79532087, 0.7931826, 0.0010691306]\n",
            "Batch:131\n",
            "d_loss:0.5990900695323944\n",
            "g_loss:[0.76755387, 0.76609564, 0.00072911277]\n",
            "Batch:132\n",
            "d_loss:0.7004892826080322\n",
            "g_loss:[0.74653447, 0.7452339, 0.0006502893]\n",
            "Batch:133\n",
            "d_loss:0.6916700303554535\n",
            "g_loss:[0.6353931, 0.63406146, 0.00066581386]\n",
            "Batch:134\n",
            "d_loss:0.6489890515804291\n",
            "g_loss:[0.7846018, 0.78329617, 0.00065282185]\n",
            "Batch:135\n",
            "d_loss:0.7640075981616974\n",
            "g_loss:[0.595887, 0.5943408, 0.00077308994]\n",
            "Batch:136\n",
            "d_loss:0.649457186460495\n",
            "g_loss:[0.5521386, 0.55048287, 0.0008278914]\n",
            "Batch:137\n",
            "d_loss:0.7233235388994217\n",
            "g_loss:[0.60389364, 0.6026244, 0.0006346131]\n",
            "Batch:138\n",
            "d_loss:0.6340187191963196\n",
            "g_loss:[0.7065675, 0.7053158, 0.0006258469]\n",
            "Batch:139\n",
            "d_loss:0.680499792098999\n",
            "g_loss:[0.6080111, 0.606851, 0.0005800809]\n",
            "Batch:140\n",
            "d_loss:0.6707541942596436\n",
            "g_loss:[0.7933347, 0.79111046, 0.001112137]\n",
            "Batch:141\n",
            "d_loss:0.788934975862503\n",
            "g_loss:[1.0948024, 1.0928016, 0.001000417]\n",
            "Batch:142\n",
            "d_loss:0.7749965637922287\n",
            "g_loss:[0.9400746, 0.93856364, 0.00075550005]\n",
            "Batch:143\n",
            "d_loss:0.6792868822813034\n",
            "g_loss:[0.79354775, 0.79209083, 0.00072845945]\n",
            "Batch:144\n",
            "d_loss:0.6765837669372559\n",
            "g_loss:[0.74391025, 0.74241316, 0.0007485383]\n",
            "Batch:145\n",
            "d_loss:0.6394812166690826\n",
            "g_loss:[0.6749303, 0.67338586, 0.0007721962]\n",
            "Batch:146\n",
            "d_loss:0.708366185426712\n",
            "g_loss:[0.5626446, 0.5612139, 0.00071535696]\n",
            "Batch:147\n",
            "d_loss:0.6456488519906998\n",
            "g_loss:[0.5634478, 0.56200874, 0.00071951945]\n",
            "Batch:148\n",
            "d_loss:0.6454773843288422\n",
            "g_loss:[0.64830095, 0.6470165, 0.00064221007]\n",
            "Batch:149\n",
            "d_loss:0.7699792981147766\n",
            "g_loss:[2.3562686, 2.3546617, 0.000803426]\n",
            "Batch:150\n",
            "d_loss:0.6871587187051773\n",
            "g_loss:[1.9669262, 1.9653581, 0.00078402355]\n",
            "Batch:151\n",
            "d_loss:0.872631773352623\n",
            "g_loss:[0.70503974, 0.70367485, 0.0006824308]\n",
            "Batch:152\n",
            "d_loss:0.7224354296922684\n",
            "g_loss:[0.7113348, 0.70987046, 0.00073219166]\n",
            "Batch:153\n",
            "d_loss:0.709436684846878\n",
            "g_loss:[0.60909265, 0.6076767, 0.00070797396]\n",
            "Batch:154\n",
            "d_loss:0.59276382625103\n",
            "g_loss:[0.81098413, 0.8096284, 0.0006778397]\n",
            "Batch:155\n",
            "d_loss:0.6429554671049118\n",
            "g_loss:[0.7053604, 0.7039722, 0.0006940982]\n",
            "Batch:156\n",
            "d_loss:0.6028083264827728\n",
            "g_loss:[0.66916907, 0.66767764, 0.0007457231]\n",
            "Batch:157\n",
            "d_loss:0.5970606207847595\n",
            "g_loss:[0.5811541, 0.57969475, 0.0007296881]\n",
            "Batch:158\n",
            "d_loss:0.6359407603740692\n",
            "g_loss:[0.6249825, 0.62379867, 0.0005919134]\n",
            "Batch:159\n",
            "d_loss:0.5798833072185516\n",
            "g_loss:[0.68142796, 0.6801141, 0.00065693504]\n",
            "Batch:160\n",
            "d_loss:0.6097222715616226\n",
            "g_loss:[0.66982263, 0.6683325, 0.0007450724]\n",
            "Batch:161\n",
            "d_loss:0.5748878121376038\n",
            "g_loss:[0.6073763, 0.6057936, 0.0007913384]\n",
            "Batch:162\n",
            "d_loss:0.6298023015260696\n",
            "g_loss:[0.69368577, 0.69233453, 0.0006756055]\n",
            "Batch:163\n",
            "d_loss:0.6656655222177505\n",
            "g_loss:[0.62416893, 0.6230632, 0.00055287284]\n",
            "Batch:164\n",
            "d_loss:0.5942852199077606\n",
            "g_loss:[0.66510713, 0.66376907, 0.00066902535]\n",
            "Batch:165\n",
            "d_loss:0.6111383736133575\n",
            "g_loss:[0.5100985, 0.5086952, 0.0007016657]\n",
            "Batch:166\n",
            "d_loss:0.6012762635946274\n",
            "g_loss:[0.5136084, 0.5122942, 0.00065710535]\n",
            "Batch:167\n",
            "d_loss:0.6207223236560822\n",
            "g_loss:[0.49702778, 0.49550313, 0.000762327]\n",
            "Batch:168\n",
            "d_loss:0.5731528699398041\n",
            "g_loss:[0.55113864, 0.5497434, 0.0006975995]\n",
            "Batch:169\n",
            "d_loss:0.6127338111400604\n",
            "g_loss:[0.49752828, 0.49595022, 0.00078903086]\n",
            "Batch:170\n",
            "d_loss:0.6009680777788162\n",
            "g_loss:[0.46675035, 0.4656105, 0.00056992343]\n",
            "Batch:171\n",
            "d_loss:0.5948572605848312\n",
            "g_loss:[0.451981, 0.45055994, 0.00071053946]\n",
            "Batch:172\n",
            "d_loss:0.5862306356430054\n",
            "g_loss:[0.46494818, 0.46388102, 0.0005335775]\n",
            "Batch:173\n",
            "d_loss:0.6040389686822891\n",
            "g_loss:[0.7674366, 0.765809, 0.0008138253]\n",
            "Batch:174\n",
            "d_loss:0.6453463286161423\n",
            "g_loss:[0.6648696, 0.6634741, 0.0006977492]\n",
            "Batch:175\n",
            "d_loss:0.7587835639715195\n",
            "g_loss:[0.707607, 0.7059289, 0.00083901134]\n",
            "Batch:176\n",
            "d_loss:0.5886849462985992\n",
            "g_loss:[0.6323233, 0.6305555, 0.000883904]\n",
            "Batch:177\n",
            "d_loss:0.6365892291069031\n",
            "g_loss:[0.62293595, 0.6205706, 0.0011826784]\n",
            "Batch:178\n",
            "d_loss:0.5957723557949066\n",
            "g_loss:[0.6803387, 0.6779907, 0.0011740096]\n",
            "Batch:179\n",
            "d_loss:0.61247318983078\n",
            "g_loss:[0.6118125, 0.61018705, 0.00081272237]\n",
            "Batch:180\n",
            "d_loss:0.5767702460289001\n",
            "g_loss:[0.63897234, 0.6371206, 0.0009258684]\n",
            "Batch:181\n",
            "d_loss:0.5921219736337662\n",
            "g_loss:[0.68640584, 0.6850084, 0.0006987046]\n",
            "Batch:182\n",
            "d_loss:0.5872678756713867\n",
            "g_loss:[0.620904, 0.6193082, 0.00079794053]\n",
            "Batch:183\n",
            "d_loss:0.5989957749843597\n",
            "g_loss:[0.6374534, 0.635154, 0.0011496751]\n",
            "Batch:184\n",
            "d_loss:0.5904392153024673\n",
            "g_loss:[0.68444335, 0.68244624, 0.0009985508]\n",
            "Batch:185\n",
            "d_loss:0.5725541710853577\n",
            "g_loss:[0.66749567, 0.66532135, 0.0010871682]\n",
            "Batch:186\n",
            "d_loss:0.5912076681852341\n",
            "g_loss:[0.68939817, 0.6876024, 0.00089788507]\n",
            "Batch:187\n",
            "d_loss:0.5901110619306564\n",
            "g_loss:[0.67932665, 0.6767918, 0.0012674419]\n",
            "Batch:188\n",
            "d_loss:0.5953641384840012\n",
            "g_loss:[0.6547143, 0.65294254, 0.0008858723]\n",
            "Batch:189\n",
            "d_loss:0.5858187824487686\n",
            "g_loss:[0.64478874, 0.643299, 0.0007448658]\n",
            "Batch:190\n",
            "d_loss:0.5812266170978546\n",
            "g_loss:[0.66372615, 0.66180575, 0.0009602138]\n",
            "Batch:191\n",
            "d_loss:0.594425618648529\n",
            "g_loss:[0.6277218, 0.62600815, 0.000856812]\n",
            "Batch:192\n",
            "d_loss:0.5677229911088943\n",
            "g_loss:[0.62027913, 0.6184497, 0.0009147095]\n",
            "Batch:193\n",
            "d_loss:0.586980015039444\n",
            "g_loss:[0.75215715, 0.7501387, 0.0010092336]\n",
            "Batch:194\n",
            "d_loss:0.5876343101263046\n",
            "g_loss:[0.6339231, 0.6322683, 0.00082738826]\n",
            "Batch:195\n",
            "d_loss:0.5829487144947052\n",
            "g_loss:[0.60254264, 0.60062784, 0.0009574039]\n",
            "Batch:196\n",
            "d_loss:0.5870854556560516\n",
            "g_loss:[0.58388156, 0.5821878, 0.00084689696]\n",
            "Batch:197\n",
            "d_loss:0.5780515521764755\n",
            "g_loss:[0.5977752, 0.5961149, 0.0008301765]\n",
            "Batch:198\n",
            "d_loss:0.5615714192390442\n",
            "g_loss:[0.56739664, 0.56587577, 0.0007604303]\n",
            "Batch:199\n",
            "d_loss:0.5686574280261993\n",
            "g_loss:[0.54445213, 0.54257894, 0.00093660766]\n",
            "Batch:200\n",
            "d_loss:0.5959418267011642\n",
            "g_loss:[0.5383784, 0.53716457, 0.00060691126]\n",
            "Batch:201\n",
            "d_loss:0.5766567140817642\n",
            "g_loss:[0.5467995, 0.5453313, 0.00073410274]\n",
            "Batch:202\n",
            "d_loss:0.5692966282367706\n",
            "g_loss:[0.61772263, 0.6165083, 0.0006071677]\n",
            "Batch:203\n",
            "d_loss:0.5621362775564194\n",
            "g_loss:[0.5499181, 0.54840016, 0.00075898477]\n",
            "Batch:204\n",
            "d_loss:0.5929108709096909\n",
            "g_loss:[0.53428906, 0.5330188, 0.00063512736]\n",
            "Batch:205\n",
            "d_loss:0.6183539628982544\n",
            "g_loss:[0.5843787, 0.5827955, 0.00079159957]\n",
            "Batch:206\n",
            "d_loss:0.5844919085502625\n",
            "g_loss:[0.621366, 0.62009364, 0.00063619314]\n",
            "Batch:207\n",
            "d_loss:0.6054878234863281\n",
            "g_loss:[0.6162538, 0.6149242, 0.0006647919]\n",
            "Batch:208\n",
            "d_loss:0.6201475560665131\n",
            "g_loss:[0.7747575, 0.77327275, 0.00074239075]\n",
            "Batch:209\n",
            "d_loss:0.7899439334869385\n",
            "g_loss:[0.6562169, 0.6550375, 0.000589686]\n",
            "Batch:210\n",
            "d_loss:0.6323334723711014\n",
            "g_loss:[0.62670153, 0.6255606, 0.00057046826]\n",
            "Batch:211\n",
            "d_loss:0.5991778075695038\n",
            "g_loss:[0.6375759, 0.6364611, 0.00055741396]\n",
            "Batch:212\n",
            "d_loss:0.5881496220827103\n",
            "g_loss:[0.9220896, 0.9206865, 0.00070154545]\n",
            "Batch:213\n",
            "d_loss:0.6435786634683609\n",
            "g_loss:[0.7314761, 0.72990394, 0.0007861018]\n",
            "Batch:214\n",
            "d_loss:0.6006326675415039\n",
            "g_loss:[0.64892685, 0.6476047, 0.00066108524]\n",
            "Batch:215\n",
            "d_loss:0.5863495469093323\n",
            "g_loss:[0.6049981, 0.60348356, 0.0007572798]\n",
            "Batch:216\n",
            "d_loss:0.6232322752475739\n",
            "g_loss:[0.5562412, 0.5551032, 0.00056902843]\n",
            "Batch:217\n",
            "d_loss:0.5849941670894623\n",
            "g_loss:[0.5723834, 0.57128596, 0.00054872653]\n",
            "Batch:218\n",
            "d_loss:0.6394553482532501\n",
            "g_loss:[0.5444967, 0.5430125, 0.00074210984]\n",
            "Batch:219\n",
            "d_loss:0.5831778347492218\n",
            "g_loss:[0.50699013, 0.50573194, 0.0006290875]\n",
            "Batch:220\n",
            "d_loss:0.584040954709053\n",
            "g_loss:[0.5040194, 0.50243664, 0.0007913579]\n",
            "Batch:221\n",
            "d_loss:0.5732935965061188\n",
            "g_loss:[1.2468132, 1.2452192, 0.00079696573]\n",
            "Batch:222\n",
            "d_loss:0.6794133931398392\n",
            "g_loss:[0.6176991, 0.61617434, 0.0007623863]\n",
            "Batch:223\n",
            "d_loss:0.6305274218320847\n",
            "g_loss:[0.5732026, 0.57145226, 0.00087516697]\n",
            "Batch:224\n",
            "d_loss:0.7127763330936432\n",
            "g_loss:[0.66335076, 0.66168207, 0.00083433307]\n",
            "Batch:225\n",
            "d_loss:0.5780159384012222\n",
            "g_loss:[0.6540832, 0.6523205, 0.0008813387]\n",
            "Batch:226\n",
            "d_loss:0.6043126434087753\n",
            "g_loss:[0.5733998, 0.5718845, 0.0007576486]\n",
            "Batch:227\n",
            "d_loss:0.5977713316679001\n",
            "g_loss:[0.51898134, 0.51710576, 0.00093778805]\n",
            "Batch:228\n",
            "d_loss:0.5808860808610916\n",
            "g_loss:[0.58059716, 0.57897896, 0.00080911175]\n",
            "Batch:229\n",
            "d_loss:0.6180399656295776\n",
            "g_loss:[1.4792986, 1.4778497, 0.0007244646]\n",
            "Batch:230\n",
            "d_loss:0.6493421345949173\n",
            "g_loss:[1.8824095, 1.8806164, 0.00089652545]\n",
            "Batch:231\n",
            "d_loss:0.6316962689161301\n",
            "g_loss:[1.2512969, 1.2492961, 0.001000382]\n",
            "Batch:232\n",
            "d_loss:0.5958789736032486\n",
            "g_loss:[0.95376027, 0.952004, 0.00087812915]\n",
            "Batch:233\n",
            "d_loss:0.5875518769025803\n",
            "g_loss:[0.8389429, 0.8376862, 0.0006283407]\n",
            "Batch:234\n",
            "d_loss:0.5902865082025528\n",
            "g_loss:[0.84629714, 0.84469306, 0.0008020413]\n",
            "Batch:235\n",
            "d_loss:0.5655323266983032\n",
            "g_loss:[0.7661864, 0.7646409, 0.0007727384]\n",
            "Batch:236\n",
            "d_loss:0.5844528377056122\n",
            "g_loss:[1.0097018, 1.0081242, 0.00078883674]\n",
            "Batch:237\n",
            "d_loss:0.7334937155246735\n",
            "g_loss:[0.9279461, 0.92661154, 0.0006672781]\n",
            "Batch:238\n",
            "d_loss:0.6006250977516174\n",
            "g_loss:[1.5669093, 1.5655866, 0.00066138915]\n",
            "Batch:239\n",
            "d_loss:0.6349826902151108\n",
            "g_loss:[0.8303032, 0.82885814, 0.0007225219]\n",
            "Batch:240\n",
            "d_loss:0.6214549392461777\n",
            "g_loss:[0.8965546, 0.8951211, 0.0007167371]\n",
            "Batch:241\n",
            "d_loss:0.6003142148256302\n",
            "g_loss:[0.92075163, 0.9194018, 0.000674914]\n",
            "Batch:242\n",
            "d_loss:0.5705308616161346\n",
            "g_loss:[0.86599773, 0.86408, 0.0009588655]\n",
            "Batch:243\n",
            "d_loss:0.5975076854228973\n",
            "g_loss:[0.7932991, 0.79165727, 0.0008209058]\n",
            "Batch:244\n",
            "d_loss:0.5767262428998947\n",
            "g_loss:[0.76990473, 0.76838315, 0.00076078926]\n",
            "Batch:245\n",
            "d_loss:0.6011930555105209\n",
            "g_loss:[0.7641099, 0.76271284, 0.00069853076]\n",
            "Batch:246\n",
            "d_loss:0.6255204081535339\n",
            "g_loss:[0.81537175, 0.8143451, 0.0005133264]\n",
            "Batch:247\n",
            "d_loss:0.6099146157503128\n",
            "g_loss:[0.7945746, 0.7934364, 0.0005691133]\n",
            "Batch:248\n",
            "d_loss:0.5564568191766739\n",
            "g_loss:[0.7673094, 0.7660471, 0.00063114974]\n",
            "Batch:249\n",
            "d_loss:0.5649335533380508\n",
            "g_loss:[0.7727506, 0.77145034, 0.00065015]\n",
            "Batch:250\n",
            "d_loss:0.5622609406709671\n",
            "g_loss:[0.7971474, 0.79594487, 0.0006012661]\n",
            "Batch:251\n",
            "d_loss:0.5695075690746307\n",
            "g_loss:[0.8011511, 0.79973376, 0.0007086745]\n",
            "Batch:252\n",
            "d_loss:0.5853250920772552\n",
            "g_loss:[0.75823087, 0.7568778, 0.0006765458]\n",
            "Batch:253\n",
            "d_loss:0.5665156096220016\n",
            "g_loss:[0.76330507, 0.7617783, 0.0007633785]\n",
            "Batch:254\n",
            "d_loss:0.6343579292297363\n",
            "g_loss:[0.7339995, 0.73289967, 0.0005499046]\n",
            "Batch:255\n",
            "d_loss:0.5773078203201294\n",
            "g_loss:[0.8243378, 0.8226967, 0.00082055933]\n",
            "Batch:256\n",
            "d_loss:0.5689815729856491\n",
            "g_loss:[0.80813825, 0.8061062, 0.0010160182]\n",
            "Batch:257\n",
            "d_loss:0.5867383480072021\n",
            "g_loss:[0.80578643, 0.80379856, 0.0009939246]\n",
            "Batch:258\n",
            "d_loss:0.5987366586923599\n",
            "g_loss:[0.8413102, 0.8393537, 0.0009782652]\n",
            "Batch:259\n",
            "d_loss:0.6030355989933014\n",
            "g_loss:[0.76741153, 0.76620805, 0.0006017328]\n",
            "Batch:260\n",
            "d_loss:0.5659003108739853\n",
            "g_loss:[0.9100034, 0.9083979, 0.0008027646]\n",
            "Batch:261\n",
            "d_loss:0.6212555319070816\n",
            "g_loss:[0.7511189, 0.7494608, 0.000829029]\n",
            "Batch:262\n",
            "d_loss:0.5715308040380478\n",
            "g_loss:[0.77062315, 0.76915777, 0.0007326835]\n",
            "Batch:263\n",
            "d_loss:0.5842286497354507\n",
            "g_loss:[0.79678327, 0.7953589, 0.00071217905]\n",
            "Batch:264\n",
            "d_loss:0.5778694301843643\n",
            "g_loss:[0.78664947, 0.78552526, 0.0005621131]\n",
            "Batch:265\n",
            "d_loss:0.5597880035638809\n",
            "g_loss:[0.7836881, 0.7826749, 0.0005066188]\n",
            "Batch:266\n",
            "d_loss:0.5804844498634338\n",
            "g_loss:[0.8174462, 0.8163619, 0.0005421238]\n",
            "Batch:267\n",
            "d_loss:0.581238403916359\n",
            "g_loss:[0.7605499, 0.75925815, 0.00064587867]\n",
            "Batch:268\n",
            "d_loss:0.564418226480484\n",
            "g_loss:[0.83112323, 0.83003104, 0.00054609467]\n",
            "Batch:269\n",
            "d_loss:0.6268178969621658\n",
            "g_loss:[0.766416, 0.76528406, 0.0005659629]\n",
            "Batch:270\n",
            "d_loss:0.5662099421024323\n",
            "g_loss:[0.76621485, 0.7652612, 0.00047682697]\n",
            "Batch:271\n",
            "d_loss:0.566546157002449\n",
            "g_loss:[0.734743, 0.7337211, 0.00051095354]\n",
            "Batch:272\n",
            "d_loss:0.574523076415062\n",
            "g_loss:[0.75088876, 0.7499387, 0.00047502777]\n",
            "Batch:273\n",
            "d_loss:0.5750456154346466\n",
            "g_loss:[0.72882074, 0.7278312, 0.00049477443]\n",
            "Batch:274\n",
            "d_loss:0.5720211714506149\n",
            "g_loss:[0.7303989, 0.7294453, 0.00047679653]\n",
            "Batch:275\n",
            "d_loss:0.5581905543804169\n",
            "g_loss:[0.7494362, 0.7483199, 0.0005581507]\n",
            "Batch:276\n",
            "d_loss:0.5744134783744812\n",
            "g_loss:[0.7122509, 0.71072286, 0.00076401944]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}